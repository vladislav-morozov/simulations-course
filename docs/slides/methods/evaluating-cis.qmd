---
title: "Evaluating Confidence Intervals"
subtitle: "Understanding Actual Coverage and Length Properties"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="Learn how to evaluate confidence intervals in data science using Monte Carlo simulations; with worked maximum likelihood example(lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Evaluating Confidence Intervals"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: "Learn how to evaluate confidence intervals in data science using Monte Carlo simulations; with worked maximum likelihood example(lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  
```{python}
import matplotlib.pyplot as plt
import numpy as np

from scipy.stats import chi2
BG_COLOR = "whitesmoke"
THEME_COLOR = "#3c165c"
```

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is about evaluating confidence intervals

<br>

By the end, you should be able to

 
- Describe key properties of confidence intervals 
- Write a small simulation organized in several functions
- Use simulations to evaluate confidence interval coverage and length
 
 


#### References 
 
<br>

::: {.nonincremental}

 
- [My slides from undergraduate econometrics](https://vladislav-morozov.github.io/econometrics-2/slides/vector/ols-inference.html) for some basic theory on univariate confidence intervals
- Chapters 7 and 9 in @Hansen2022Econometrics

:::

 
  


### Motivating Example {background="#43464B"}

#### Setting and Estimators

Suppose that observe an IID sample $X_1, \dots, X_N$ where
$$
X_i \sim N(\mu, \sigma^2)
$$
We estimate $\mu$ and $\sigma^2$ with their maximum likelihood estimators

$$
\hat{\mu} = \dfrac{1}{N}\sum_{i=1}^N X_i, \quad \hat{\sigma}^2 = \dfrac{1}{N}\sum_{i=1}^{N}(X_i -\bar{X})^2
$$

#### Towards Confidence Intervals

::: {.left-color}

In practice usually want to quantify uncertainty using confidence intervals

:::

- Can construct based on exact sampling distribution of $\hat{\mu}$ and $\hat{\sigma}^2$
- More often and generally, use asymptotic normality approximations, e.g. that if $\mu=0$, then under any distribution of data

$$
\sqrt{N}(\hat{\sigma}^2 - \sigma^2) \xrightarrow{d} N(0, \E[X_i^4]-\sigma^4)
$$

::: {.footer}

Can prove the above asymptotic result using the [delta method](https://vladislav-morozov.github.io/econometrics-2/slides/vector/ols-delta-method.html)

:::

#### Motivating Issue: Errors in Mean vs. in Variance

- Suppose that true values are $\mu=0$ and $\sigma^2 = 0.1$
- For the mean: you can make the same mistake in both directions: e.g. you can underestimate or overestimate the mean by $0.5$
- You can also overestimate $\sigma^2$ by 0.5
 
::: {.left-color}

But you cannot underestimate $\sigma^2$ by $0.5$ (which would mean an estimate of $-0.4$)

$\Rightarrow$ Distribution of estimation errors $(\sigma^2-\sigma^2)$ cannot be symmetric
:::

#### Simulation Question of Today

- Asymmetry of error distribution $\Rightarrow$ maybe asymptotic normal (symmetric) approximation is not good?
- This effect may be more pronounced when $\sigma$ is small (less space for underestimation)
- Does it affect he "quality" of asymptotic confidence intervals for $\sigma^2$?


. . . 

::: {.left-color}

Today: check numerically whether confidence intervals properties change as $\sigma^2\to 0$

:::

::: {.footer}

There is a lot of theory work about things going wrong when parameters may be on the boundary. See [@Andrews1999EstimationWhenParameter; @Andrews2000InconsistencyBootstrapWhen; @Andrews2001TestingWhenParameter]

:::

## Theory Essentials for Confidence Intervals {background="#00100F"}

### Definitions and Examples {background="#43464B"}


#### Confidence Sets: Definition


<div class="rounded-box">

::: {#def-vector-inference-conf-set}

1. A <span class="highlight"> $(1-\alpha)\times 100\%$ confidence set for $\theta$ </span> ($\theta\in\R^p$) is a random set $S(X_1, \dots, X_N)\subseteq \R^p$  
$$ \scriptsize
P(\theta \in S(X_1, \dots, X_N)) = 1-\alpha
$$

1. $S(\cdot, \cdots)$ is an <span class="highlight">asymptotic $(1-\alpha)\times 100\%$ confidence set for $\theta$ </span>  if $\lim_{N\to\infty} P(\theta \in S(X_1, \dots, X_N)) = 1-\alpha$

2. $P(\theta \in S(X_1, \dots, X_N))$ is the <span class="highlight">coverage</span> of $S$

:::


</div>


::: footer

:::

#### Confidence Sets as Set Estimators

Suppose that we are interested in some $\btheta\in \R^p$

::: {.left-color}

Confidence sets can be viewed as <span class="highlight">set estimators</span> —  return a whole set of values in $\R^p$ as a collection of guesses for $\btheta$


:::
  
<br>

- Confidence sets — intervals estimators with coverage guarantees
- Should contrast with <span class="highlight">point estimators</span> (e.g. OLS, IV, etc.) — give only one point

#### Familiar Example: CI Based on Asymptotic Normality
  
<div class="rounded-box">

::: {#prp-vector-inference-ci}

Suppose that $\sqrt{N}(\hat{\theta} - \theta) \xrightarrow{d} N(0, \avar(\hat{\theta}))$. Let $\widehat{\avar}(\hat{\theta})\xrightarrow{p} \avar(\hat{\theta})$.


The confidence interval
$$ \small \hspace{-1.6cm}
S = \left[  \hat{\theta} -  z_{1-\alpha/2} \sqrt{ \frac{\widehat{\avar}(\hat{\theta})}{N} }, \hat{\theta}+  z_{1-\alpha/2} \sqrt{ \frac{\widehat{\avar}(\hat{\theta})}{N} }  \right]
$$ {#eq-vector-inference-basic-ci}
has asymptotic coverage $(1-\alpha)\times 100\%$

:::

</div>


#### Connection to Tests: Test Inversion

There is an equivalent way to construct the confidence interval ([-@eq-vector-inference-basic-ci])

- Recall $t$-statistic for $H_0: \theta = c$
- $S$ is the set of all $c$ for which the $t$-test does not reject

. . . 

<br>

::: {.left-color}


An example of <span class="highlight">test inversion</span> and equivalence between testing and confidence intervals

:::

 

#### Interpetation of Confidence Interval

Recall interpretation of $95\%$ coverage:

::: {.left-color}

- Suppose we draw many samples of the same size 
- In ~95% of the samples the CI will fall "on top" of the true parameter value



:::

<br>

Notice: interpretation aligns with what we actually do in Monte Carlo
 
### Motivating Example {background="#43464B"}

#### Metrics for Evaluating Confidence Intervals

<br>

There are two key metrics for looking at confidence intervals in simulations

1. Coverage: what the actual coverage is 
2. (Average) length

::: {.left-color}

Generally prefer shorter intervals with coverage that is close to the nominal level

:::

#### Metric: Actual Coverage

How do you evaluate coverage? 

<br>

::: {.left-color}

Draw many datasets and in each sample $s$

- Compute CI = $[\hat{a}_s, \hat{b}_s]$
- Record $\text{Length}_s = \hat{b}_s - \hat{a}_s$

Report estimated average of $\text{Length}_s$ over $s$
:::


#### Metric: Average Length


Similar story for length:

::: {.left-color}

Draw many datasets and in each sample $s$

- Compute CI
- Check if CI include the true parameter and record inclusion as variable $D_s = 0, 1$ (1 for included)

Report estimated coverage as average of $D_s$ over $s$
:::

Can also report other characteristics of length distribution (e.g. quantiles)

::: {.footer}

For multivariate sets the metric is usually called *volume*

::: 
 

## Simulation Implementation {background="#00100F"}

### Completing Simulation Design {background="#43464B"}


#### Recap: Simulation Setting

Let's come back to our example of today. Already have

- DGP of variables (normal with different variances)
- Estimators: maximum likelihood
- Confidence interval to evaluate: based on asympotic normality of $\hat{\sigma}^2$

::: {.left-color}

Still need to choose remaining DGP parameters: mean $\mu$, sample size $N$, and values of $\sigma$

::: 


#### Choosing the Remaining Parameters

Can set
$$
\mu = 0
$$

For sample sizes:

- Consider two: $N=20$ and $N=100$
- If asymptotic approximation problems exist, they should be less present with larger sample sizes

::: {.left-color}

For $\sigma$: logarithmic grid between $10^{-3}$ and $10$

:::


### Implementing The Simulation {background="#43464B"}

#### Approach to Code: Functions

Our simulation 

- Quite small
- Unlikely to need many more DGPs and different estimators

::: {.left-color}

Today will organize as a small collection of functions

:::

Contrast with the modular approach of the previous two examples

#### Code Structure

```
├── main.py
├── results 
│   └── ...
└── sim_infrastructure
    ├── __init__.py 
    ├── core.py
    └── plotting.py
```

`main.py`:

1. Runs the simulation using function from `sim_infrastructure.core`
2.  Makes plots using functions from `sim_infrastructure.plotting`
   
#### `core` Simulation Module

``` {.python filename="sim_infrastructure/core.py"}
"""
Core simulation logic for evaluating confidence intervals for variance.

This module contains the core functions:
- mle_variance: maximum likelihood estimation of variance.
- asymptotic_ci_variance: construction of asymptotic confidence intervals.
- run_simulations: running Monte Carlo simulations for coverage and interval length.
"""

import numpy as np

from scipy.stats import norm

def mle_variance(data: np.ndarray) -> np.floating:
    """Estimate the variance of a normal distribution using MLE."""
    return np.var(data, ddof=0)  # MLE for variance is the sample variance with ddof=0

def asymptotic_ci_variance(data: np.ndarray, alpha: float = 0.05) -> tuple[float, float]:
    """Construct an asymptotic (Wald-type) confidence interval for the variance."""
    n = len(data)
    var_hat = mle_variance(data)
    se = np.sqrt(2 * var_hat**2 / n)
    z_crit = norm.ppf(1 - alpha / 2)
    lower = max(0, var_hat - z_crit * se)  # Ensure lower bound is non-negative
    upper = var_hat + z_crit * se
    return lower, upper

def run_simulations(
    true_vars: np.ndarray, 
    n_simulations: int = 1000,
    n_obs_list: list[int] = [20, 100],
    alpha: float = 0.05,
    seed: int = 1,
) -> dict[int, dict[float, dict]]:
    """
    Run Monte Carlo simulations for each true variance and sample size.

    Args:
        true_vars (np.ndarray): true variance values to simulate.
        n_simulations (int): number of Monte Carlo simulations per true variance.
        n_obs_list (list[int]): list of sample sizes to simulate.
        alpha (float): significance level for confidence intervals.
        seed (int): Random seed for reproducibility.

    Returns:
        Nested dictionary: {n_obs: {true_var: {'coverage': ..., 'errors': ..., 'lengths': ...}}}
    """
    rng = np.random.default_rng(seed)
    results = {n_obs: {} for n_obs in n_obs_list}

    # Loop over sample sizes
    for n_obs in n_obs_list:
        # Loop over true variables
        for true_var in true_vars:
            errors = []
            lengths = []
            coverage = []
            for _ in range(n_simulations):
                # Draw data and run estimation
                data = rng.normal(0, np.sqrt(true_var), n_obs)
                var_hat = mle_variance(data)

                # Record estimation error in variance
                errors.append(var_hat - true_var)

                # Construct CI and check if variance belongs to it
                ci_lower, ci_upper = asymptotic_ci_variance(data, alpha)
                lengths.append((ci_upper - ci_lower) / true_var) 
                coverage.append(ci_lower <= true_var <= ci_upper)
 
            results[n_obs][true_var] = {
                'coverage': np.mean(coverage),
                'errors': np.array(errors),
                'lengths': np.array(lengths)
            }

    return results
```

#### `main.py`

```{.python filename="main.py"}
"""
Entry point for running simulations and generating plots.

Simulation 3: evaluating confidence intervals for variance of the normal
distribution based on the maximum likelihood estimator. Simultions consider
coverage and (scaled) length properties as true variance decreases to zero.

This script:
- Sets up simulation parameters.
- Runs the core simulation.
- Generates and saves all plots.
"""

from pathlib import Path

import numpy as np

from sim_infrastructure.core import run_simulations
from sim_infrastructure.plotting import plot_coverage_and_lengths, plot_error_kdes

# Simulation parameters: note upper case for constants
TRUE_VARS = np.logspace(-3, 1, 50)
N_OBS_LIST = [20, 100]
N_SIMS = 50000
CI_ALPHA = 0.05
SEED = 1
OUTPUT_DIR = Path() / "results"


def main():
    # Run simulations
    results = run_simulations(TRUE_VARS, N_SIMS, N_OBS_LIST, CI_ALPHA, SEED)

    # Generate plots
    plot_coverage_and_lengths(results, OUTPUT_DIR)
    plot_error_kdes(results, OUTPUT_DIR)


if __name__ == "__main__":
    main()
```

#### Core Organization: Discussion

- Tightly couple code
  - Sacrifice extensibility
  - But much easier to write, much less infrastructure
- Fine for our small targeted simulation
- Inside simulation: store CI coverage and length; estimation error of maximum likelihood estimator

## Simulation Results {background="#00100F"}

### Core Results {background="#43464B"}



#### Visualizing Core Results
 
- Our simulation produces coverage and length for each each value of $\sigma$
- How to look at these results?

<br>

. . .

::: {.left-color}

Can visually graphically with coverage and length plots

:::

For length: scale by magnitude to target value to get a "normalized" view

#### Coverage Plot

![](../../../example-codes/3-confidence-intervals-close-to-boundary/results/coverage_plot.svg)

#### Length Plot


![](../../../example-codes/3-confidence-intervals-close-to-boundary/results/length_plot.svg)

#### Interpretation of Simulation Results

For the core question:

::: {.left-color}

- Remarkably stable coverage and length properties
- No dependence on $\sigma$ 

$\Rightarrow$ no issue with $\sigma\approx 0$

:::

<br>

But do see coverage issues for $N=20$ despite much longer intervals (around 87% real coverage with 95% nominal level). <span class="highlight">Why?</span>


### Why Coverage Issues for Smaller Samples? {background="#43464B"}

#### Potential Explanations of Bad Coverage for $N=20$

Why the coverage issues for $N=20$?

- Normal approximation might be poor (incorrect shape of interval around its center)
- MLE estimator is biased downward (incorrect center of the interval)

::: {.left-color}

Can look at distribution of estimation errors and how it compares to the normal distribution with same variance

:::

#### Example Distribution Plot for $N=20$

![](../../../example-codes/3-confidence-intervals-close-to-boundary/results/errors_n20_var_0.0518.svg)

#### Example Distribution Plot for $N=100$

![](../../../example-codes/3-confidence-intervals-close-to-boundary/results/errors_n100_var_0.0518.svg)

#### Discussions of Distributional Approximation

From looking at the two plots:

- The shape is not perfect for $N=20$: longer right tail, some asymmetry
- Bias is present: noticeable shift to the left

. . .

But much better approximation for $N=100$

<br>


::: {.left-color}

$\Rightarrow$ Coverage issues are due to small sample effects

:::

## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 

- Reviewed key concepts for confidence intervals  
- Identified key metrics: coverage and length
- Saw a practical small simulation organized around tightly couple functions

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
