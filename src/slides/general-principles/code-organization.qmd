---
title: "Good Simulation Code III: Organization"
subtitle: "Defining Interfaces. Organizing Code Files"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="How  (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Code Design III: Interfaces. File Organization"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: "How   (lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is about organizing our code in two ways: defining interfaces and organizing our files

<br>

By the end, you should be able to

 
- a

#### References

::: {.nonincremental}

Statistics:
 
- 28.19-28.23 in @Hansen2022Econometrics regarding shrinkage
-  29.5 in @Hansen2022Econometrics about ridge regression 

Programming:
 
- 22 in @Lutz2025LearningPythonPowerful about modules and program structure
- 13 in @Ramalho2022FluentPythonClear and [this tutorial](https://realpython.com/python-protocol/) on protocols in Python

:::

 
```{python} 
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import Lasso as SklearnLasso

plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

BG_COLOR = "whitesmoke"
THEME_COLOR = "#3c165c"
```

## Scenario: Scope Change {background="#00100F"}
 

### Reminder: Previous Simulation Setting {background="#43464B"}


#### Reminder: Studying OLS Bias

Recall: studied <span class="highlight">bias of OLS estimator</span> in simple linear model:

$$
Y_{t} = \beta_0 + \beta_1 X + U_t, \quad t=1, \dots, T
$$ 
<br>

Metric of interest: absolute bias

$$
\text{Bias}(\hat{\beta}_1) = \E[\hat{\beta}_1] - \beta_1
$$


#### Reminder: Static and Dynamic DGPs 



:::: {.columns}

::: {.column width="46%"}

 
**Static** (`StaticNormalDGP`):

$$  \small
Y_{t} = \beta_0 + 0.5 X_{t} + U_{t}
$$

Covariate $X_t$ independent from $U_t$ and over time

:::

  
::: {.column width="2%"} 

:::

::: {.column width="52%"} 
  
**Dynamic** (`DynamicNormalDGP`):

$$ \small
Y_{t} = \beta_0 + \beta_1 Y_{t-1} + U_{t}
$$

Dependence: $\beta_1$ value

$$  \small
\beta_1 \in \curl{0, 0.5, 0.95}
$$

:::

::::

- One DGP per $U_t, X_t, Y_0$: each $N(0, 1)$
- $T=50, 200$ 


```{python}  
class StaticNormalDGP:
    """A data-generating process (DGP) for a static linear model

    Attributes:
        beta0 (float): Intercept term.
        beta1 (float): Slope coefficient.
    """

    def __init__(self, beta0: float = 0.0, beta1: float = 0.5) -> None:
        """Initializes the DGP with intercept and slope.

        Args:
            beta0 (float): Intercept term. Defaults to 0.0.
            beta1 (float): Slope coefficient. Defaults to 1.0.
        """
        self.beta0: float = beta0
        self.beta1: float = beta1

    def sample(
        self, n_obs: int, seed: int | None = None
    ) -> tuple[np.ndarray, np.ndarray]:
        """Samples data from the static DGP.

        Args:
            n_obs (int): Number of observations to sample.
            seed (int, optional): Random seed for reproducibility. Defaults to None.

        Returns:
            tuple: (x, y) arrays, each of length n_obs.
        """
        rng = np.random.default_rng(seed)
        x = rng.normal(size=n_obs)
        u = rng.normal(size=n_obs)
        y = self.beta0 + self.beta1 * x + u
        return x, y

  
class DynamicNormalDGP:
    """A data-generating process (DGP) for a dynamic linear model: y_t = beta0 + beta1*y_{t-1} + u_t.

    Attributes:
        beta0 (float): Intercept term.
        beta1 (float): AR(1) coefficient.
    """

    def __init__(self, beta0: float = 0.0, beta1: float = 0.5):
        """Initializes the DGP with intercept and AR(1) coefficient.

        Args:
            beta0 (float): Intercept term. Defaults to 0.0.
            beta1 (float): AR(1) coefficient. Defaults to 0.5.
        """
        self.beta0: float = beta0
        self.beta1: float = beta1

    def sample(
        self, n_obs: int, seed: int | None = None
    ) -> tuple[np.ndarray, np.ndarray]:
        """Samples data from the dynamic DGP.

        Args:
            n_obs (int): Number of observations to sample.
            seed (int, optional): Random seed for reproducibility. Defaults to None.

        Returns:
            tuple: (x, y) arrays, each of length n_obs.
                  x is y_{t-1} (lagged y), and y is y_t.
        """
        rng = np.random.default_rng(seed)
        y = np.zeros(n_obs + 1)  # Extra observation for lag
        u = rng.normal(size=n_obs + 1)
        y[0] = self.beta0 + u[0]  # Initial condition
        for t in range(1, n_obs + 1):
            y[t] = self.beta0 + self.beta1 * y[t - 1] + u[t]
        # Return lagged y as x and y[1:] as y
        return y[:-1], y[1:]
```

#### Reminder: `SimpleOLS`

OLS logic captured by `SimpleOLS` class:

```{python}
#| echo: true
class SimpleOLS:
    """A simple OLS estimator for the linear model y = beta0 + beta1*x + u.

    Attributes:
        beta0_hat (float): Estimated intercept. NaN until fit is called.
        beta1_hat (float): Estimated slope. NaN until fit is called.
    """

    def __init__(self) -> None:
        """Initializes the OLS estimator with no estimates."""
        self.beta0_hat: float = np.nan
        self.beta1_hat: float = np.nan

    def fit(self, x: np.ndarray, y: np.ndarray) -> None:
        """Fit OLS to the provided data.

        Args:
            x (np.ndarray): Independent variable (1D array).
            y (np.ndarray): Dependent variable (1D array).
        """
 
        # Add constant to x
        X = np.column_stack([np.ones(len(x)), x])
        # OLS estimation
        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
        self.beta0_hat, self.beta1_hat = beta_hat[0], beta_hat[1]
```
 
::: {.footer}

Notice: has `beta1_hat` attribute and `fit()` method

:::

#### Reminder: `SimulationRunner`

Simulation logic captured by `SimulationRunner` class:

```{python}
#| echo: true  
class SimulationRunner:
    """Runs Monte Carlo simulations for a given DGP and estimator.

    Attributes:
        dgp: Data-generating process with a `sample` method.
        estimator: Estimator with a `fit` method and `beta1_hat` attribute.
        errors: array of estimation errors (beta1_hat - beta1) for each simulation.
    """

    def __init__(
        self,
        dgp: "StaticNormalDGP" | "DynamicNormalDGP",
        estimator: SimpleOLS,
    ) -> None:
        """Initializes the simulation runner.

        Args:
            dgp: An instance of a DGP class (must implement `sample`).
            estimator: An instance of an estimator class (must implement `fit`).
        """
        self.dgp = dgp
        self.estimator = estimator
        self.errors = None

    def simulate(self, n_sim: int, n_obs: int, first_seed: int | None = None) -> None:
        """Runs simulations and stores estimation errors.

        Args:
            n_sim (int): number of simulations to run.
            n_obs (int): Number of observations per simulation.
            first_seed (int | None): Random seed for reproducibility. Defaults to None.
        """
        # Preallocate array to hold estimation errors
        self.errors = np.empty(n_sim)

        # Run simulation
        for sim_id in range(n_sim):
            # Draw data
            x, y = self.dgp.sample(n_obs, seed=first_seed + sim_id if first_seed else None)
            # Fit model
            self.estimator.fit(x, y)
            # Store error
            self.errors[sim_id] = self.estimator.beta1_hat - self.dgp.beta1

    def summarize_bias(self) -> None:
        """Prints the average estimation error (bias) for beta1. 
        """ 
        print(f"Average estimation error (bias): {np.mean(self.errors):.4f}")
```






### Shifting Scope and Adding a New Estimator {background="#43464B"}

#### Problem: Scope Change

Now imagine that the <span class="highlight">scope</span> of the overall project <span class="highlight">changes</span> to a broader one:

<div class="left-color">

New goal: comparing bias of several "near-OLS" estimators in a static and dynamic settings

</div>
 
- Scope changes are natural, especially in "experimental" scenarios
- Need to be flexible both in statistics and code


. . . 

How to proceed? 

#### Add a New Estimator (Ridge)

First OLS-like estimator is <span class="highlight">ridge</span>
<div class="left-color">


$$ \small
\hat{\bbeta}^{Ridge} = (\bX'\bX+\lambda I)^{-1} \bX'\bY
$$
</div>

- Ridge — one of simpler examples of <span class="highlight">shrinkage</span> estimators
- Shrinkage strength controlled by $\lambda \geq 0$
- Minimizes sum of squared residuals with $L^2$ penalty:
$$ \small
\hat{\bbeta}^{Ridge} = \argmin \sum_{t=1}^T (Y_t - b_0 - b_1 X_t)^2 + \lambda \norm{(b_0, b_1)}_2^2
$$


#### Implementation of Simple Ridge Estimator

Implement ridge with same interface as `SimpleOLS`:
```{python}
#| echo: true
#| code-line-numbers: "1,10,14,16,27"
class SimpleRidge:
    """A simple ridge estimator for the linear model y = beta0 + beta1*x + u.

    Attributes:
        beta0_hat (float): Estimated intercept. NaN until fit is called.
        beta1_hat (float): Estimated slope. NaN until fit is called.
        reg_param (float): Strength of regularization. Defaults to 0.01.
    """

    def __init__(self, reg_param: float = 0.01) -> None:
        """Initializes the OLS estimator with no estimates."""
        self.beta0_hat: float = np.nan
        self.beta1_hat: float = np.nan
        self.reg_param = reg_param

    def fit(self, x: np.ndarray, y: np.ndarray) -> None:
        """Fits the ridge estimator to the provided data.

        Args:
            x (np.ndarray): Independent variable (1D array).
            y (np.ndarray): Dependent variable (1D array).
        """
 
        # Add constant to x
        X = np.column_stack([np.ones(len(x)), x])
        # OLS estimation
        beta_hat = np.linalg.inv(X.T @ X + self.reg_param * np.eye(2)) @ X.T @ y
        self.beta0_hat, self.beta1_hat = beta_hat[0], beta_hat[1]
```




#### Bias vs. Regularization Strength

```{python}
#| echo: true
#| code-line-numbers: "4-5,8-9"
# Initialize DGP and estimator
static_dgp = StaticNormalDGP(beta0=0.0, beta1=0.5)
# Estimators
ridge_weak = SimpleRidge(reg_param=0.01)
ridge_strong = SimpleRidge(reg_param=100)

# Initialize and run simulations
sim_ridge_weak = SimulationRunner(static_dgp, ridge_weak)
sim_ridge_strong = SimulationRunner(static_dgp, ridge_strong)
sim_ridge_weak.simulate(n_sim=1000, n_obs=50, first_seed=1)
sim_ridge_strong.simulate(n_sim=1000, n_obs=50, first_seed=1)
# Summarize bias 
sim_ridge_weak.summarize_bias()
sim_ridge_strong.summarize_bias()
```

<div class="left-color">

More regularization $\Rightarrow$ more bias

</div>

## Defining Interfaces {background="#00100F"}
 

### Organizing Classes with Protocols {background="#43464B"}

#### Results for Ridge: Discussion

<div class="rounded-box">

`SimulationRunner` works with new `SimpleRidge`, even though `SimulationRunner` was made with `SimpleOLS` in mind

</div>

Why?

- `SimpleOLS` and `SimpleRidge` look same to `SimulationRunner`: same `fit()` method and `beta1_hat` 
- Example of <span class="highlight">duck typing</span> in action

. . . 

<div class="left-color">

$\Rightarrow$ can we just keep adding new estimators to answer new expanded study question?

</div>

#### Problem 1: Communicating Estimator Interface

Problem with "just adding":
<div class="left-color">

How do you remember or tell teammates what interface the estimators should have?

</div>


- *Could write docs*: good, but extra effort that a lot of people do not take until later
- *Could look at how `SimulationRunner` uses the estimator*: works best it's your fresh code. Otherwise harder, and can miss something

. . .

Better to be explicit about what new estimators need

::: {.footer}

Working with "*what estimators need*" helps us program to an *interface*, not a specific implementation

:::
  
#### Problem 2: Incorrect Type Hints

A smaller but annoying problem:

- Our type hints in `SimulationRunner` don't allow for `SimpleRidge` as an estimator
- Not a problem for execution: Python doesn't care about type hints
- But code quality tools/IDEs will (rightfully) complain:

. . .

![Figure: Issue raised by Pylance](../../images/code-screenshots/ridge-pylance.png)


#### Solution: Protocols  

Python [protocols](https://typing.python.org/en/latest/spec/protocol.html) allow us to: 

- Express that every estimator should have `fit()` and `beta1_hat`
- Capture in one place the expectations for our estimators (and DGPs)
- Type hint this 

. . .

Protocols again work like <span class="highlight">contracts</span>:

<div class="left-color">

If estimator meets protocol, it is compatible with `SimulationRunner`

</div>

#### Estimator Protocol

Here's how to express our two requirements:
```{python}
#| echo: true
from typing import Protocol

class EstimatorProtocol(Protocol):
    def fit(self, x: np.ndarray, y: np.ndarray) -> None: ...    # <1>

    @property
    def beta1_hat(self) -> float: ...                           # <2>
```

1. A `fit()` method that takes `x` and `y` numpy arrays
2. A `float` attribute called `beta1_hat`


<br>

Both `SimpleOLS` and `SimpleRidge` follow our protocol

::: {.footer}

Notice: protocol methods do not have bodies, just use `...` (ellipsis)

:::

#### Protocol for DGPs

What about DGPs? Two requirements:

- `sample()` method with two arguments (`n_obs` and `seed`)
- True `beta1` value (for estimation error computation)

. . .

Same approach: 

```{python}
#| echo: true

class DGPProtocol(Protocol):
    def sample(
        self, n_obs: int, seed: int | None = None
    ) -> tuple[np.ndarray, np.ndarray]:
        ...

    @property
    def beta1(self) -> float: ...
```

#### How To Use Protocols?

To repeat:

<div class="left-color">

Protocols communicate information about interfaces both to readers and type checkers — like a <span class="highlight">checklist</span>

</div>

To use them modify `SimulationRunner` constructor:

- Insert `DGPProtocol` as type hint for `dgp` argument
- Insert `EstimatorProtocol` as type hint for `estimator` argument

. . .


Seeing a `Protocol` in type hints suggests that some particular interface is expected here (e.g. `fit()` and `beta1_hat`)

#### Updating `SimulationRunner`

Updated implementation with protocols in place:
```{python}
#| echo: true
#| code-line-numbers: "12-13,21-22"
class SimulationRunner:
    """Runs Monte Carlo simulations for a given DGP and estimator.

    Attributes:
        dgp: data-generating process with a sample() method and beta1 attribute.
        estimator: estimator with a fit() method and beta1_hat attribute.
        errors: array of estimation errors (beta1_hat - beta1) for each simulation.
    """

    def __init__(
        self,
        dgp: DGPProtocol,
        estimator: EstimatorProtocol,
    ) -> None:
        """Initializes the simulation runner.

        Args:
            dgp: An instance of a DGP class (must implement `sample`).
            estimator: An instance of an estimator class (must implement `fit`).
        """
        self.dgp: DGPProtocol = dgp
        self.estimator: EstimatorProtocol = estimator
        self.errors: np.ndarray | None = None

    def simulate(self, n_sim: int, n_obs: int, first_seed: int | None = None) -> None:
        """Runs simulations and stores estimation errors.

        Args:
            n_sim (int): number of simulations to run.
            n_obs (int): Number of observations per simulation.
            first_seed (int | None): Starting random seed for reproducibility.
                Defaults to None.
        """
        # Preallocate array to hold estimation errors
        self.errors = np.empty(n_sim)

        # Run simulation
        for sim_id in range(n_sim):
            # Draw data
            x, y = self.dgp.sample(
                n_obs, seed=first_seed + sim_id if first_seed else None
            )
            # Fit model
            self.estimator.fit(x, y)
            # Store error
            self.errors[sim_id] = self.estimator.beta1_hat - self.dgp.beta1

    def summarize_bias(self) -> None:
        """Prints the average estimation error (bias) for beta1. 
        """ 
        print(f"Average estimation error (bias): {np.mean(self.errors):.4f}")
```

::: {.footer}

:::
 
#### Aside: Alternative Approach with ABCs

<div class="left-color">

<span class="highlight">Abstract base classes</span> provide another way to specify the desired interface contract

</div>

- With ABCs you define an abstract `BaseDGP` and `BaseEstimator` with abstract methods of desired form
- Concrete estimators and DGPs inherit from from bases and implement `sample()` and `fit()`

. . . 

Protocol vs. ABCs: depends on whether you want an inheritance structure (no inheritance needed with protocols)

::: {.footer}

See chapter 13 of @Ramalho2022FluentPythonClear on this point

::: 

### Adding Lasso Following Our Protocols {background="#43464B"}


#### Lasso

Another penalized SSR ("OLS-like") estimator for $\beta_1$ is *lasso*, which solves:
$$ \small
\hat{\bbeta}^{Lasso} = \argmin \sum_{t=1}^T (Y_t - b_0 - b_1 X_t)^2 + \lambda \norm{(b_0, b_1)}_1
$$


- Lasso penalizes $L^1$ norm of $(b_0, b_1)$, not $L^2$ like ridge
- Want to add to our simulation under new scope


#### How to Add Lasso?

- Unlike ridge, $\hat{\bbeta}^{Lasso}$ has no closed-form solution
- Can implement numerical minimization logic ourselves

. . .

But don't want to implement it ourselves (hard, etc.)

- There is already a well-optimized implemenation in `scikit-learn`
- Problem: `scikit-learn` lasso does not follow our protocol

<div class="left-color">

Not a problem: can wrap it in a compatible manner

</div>

#### Point of This Section

Why are we discussing this?
 

<br>

Three reasons:


- Seeing how our protocol guides us and serves as a contract
- Seeing how you can make use of existing things from other sources
- Seeing another new estimator

#### A Look at `scikit-learn` Lasso

In general, would use Lasso like this
```{python}

```

#### Lasso Wrapper Implementation

```{python}
#| echo: true
from sklearn.linear_model import Lasso as SklearnLasso 

class LassoWrapper:
    """A wrapper for scikit-learn's Lasso estimator to match the EstimatorProtocol interface.

    Attributes:
        model (sklearn.linear_model.Lasso): a scikit-learn Lasso instance.
        beta0_hat (float): Estimated intercept. Initialized as np.nan.
        beta1_hat (float): Estimated slope. Initialized as np.nan.
    """

    def __init__(self, reg_param: float = 1.0) -> None:
        """Initializes the Lasso wrapper with a scikit-learn Lasso model.

        Args:
            reg_param (float): Regularization strength (alpha). Defaults to 1.0.
        """
        self.model = SklearnLasso(alpha=reg_param)
        self.beta0_hat: float = np.nan
        self.beta1_hat: float = np.nan

    def fit(self, x: np.ndarray, y: np.ndarray) -> None:
        """Fits the Lasso model to the provided data.

        Args:
            x (np.ndarray): Independent variable (1D array).
            y (np.ndarray): Dependent variable (1D array).
        """ 
        x = x.reshape(-1, 1) # sklearn expects 2d array inputs
        self.model.fit(x, y)
        self.beta0_hat = float(self.model.intercept_)
        self.beta1_hat = float(self.model.coef_[0]) 

```

#### Lasso Wrapper in Action

```{python}
#| echo: true
# Initialize DGP and estimator
static_dgp = StaticNormalDGP(beta0=0.0, beta1=0.5)
# Estimators
lasso_weak = LassoWrapper(reg_param=0.01)
lasso_strong = LassoWrapper(reg_param=100)

# Initialize and run simulations
sim_lasso_weak = SimulationRunner(static_dgp, lasso_weak)
sim_lasso_strong = SimulationRunner(static_dgp, lasso_strong)
sim_lasso_weak.simulate(n_sim=1000, n_obs=50, first_seed=1)
sim_lasso_strong.simulate(n_sim=1000, n_obs=50, first_seed=1)
# Summarize bias 
sim_lasso_weak.summarize_bias()
sim_lasso_strong.summarize_bias() 
```
 

#### Discussion

"Remember how we had to update type hints for Ridge? With protocols, we don’t need to change SimulationRunner to add Lasso!"
"This is why protocols are better than concrete classes in type hints."

## Organizing Files {background="#00100F"}
 


### Splitting Code Appropriately {background="#43464B"}


#### Modules


conventions differ

#### A

```
project/
├── dgps/
│   ├── __init__.py
│   ├── static.py       # StaticNormalDGP
│   └── dynamic.py      # DynamicNormalDGP
├── estimators/
│   ├── __init__.py
│   ├── ols-like.py     # SimpleOLS, SimpleRidge, SimpleLasso
├── main.py             # Main script that we call
├── protocols.py        # DGPProtocol, EstimatorProtocol
└── runner.py           # SimulationRunner
```

::: {.footer}

See chapter 24 in @Lutz2025LearningPythonPowerful about `__init__.py`

:::

####

makes sense

- Maybe split more, maybe split less: choose what's appropriate, easy for understanding and collaboration, etc


### Executing {background="#43464B"}


####

automatically only when
the module is run as a script, not when it is imported. The line you add will probably test the value
of `__name__` for the string `__main__` 

#### Running


```bash

py main.py

```

## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 

- Discussed   
 
#### Next Questions

<br>
 
- How to organize running many simulations? 
- How to apply this logic to deeper statistical scenarios?

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
