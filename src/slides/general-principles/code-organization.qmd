---
title: "Good Simulation Code III: Organization"
subtitle: "Bringing Together Classes and Organization Code Files"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="How  (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Why Simulate and General Principles"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: "How   (lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is a 

<br>

By the end, you should be able to

 
- Refactor code into modular reusable classes
- Compose DGPs and estimators into simulations
- Understand how modularity leads to clearer, more extensible code  

#### References

::: {.nonincremental}

Statistics:
 
- 28.19-28.23 in @Hansen2022Econometrics regarding shrinkage
-  29.5 in @Hansen2022Econometrics about ridge regression 

Programming:
 
- 22 in @Lutz2025LearningPythonPowerful about modules and program structure
- [This tutorial](https://realpython.com/python-protocol/) on protocols in Python

:::

 

## Scope Change {background="#00100F"}
 

### Reminder: Previous Simulation Setting {background="#43464B"}


#### Reminder: Studying OLS Bias

This time: continue studying <span class="highlight">bias of OLS estimator</span> in simple linear model:

$$
Y_{t} = \beta_0 + \beta_1 X + U_t, \quad t=1, \dots, T
$$ 
<br>

Metric of interest: absolute bias

$$
\text{Bias}(\hat{\beta}_1) = \E[\hat{\beta}_1] - \beta_1
$$


#### Reminder: Static and Dynamic DGPs 



:::: {.columns}

::: {.column width="46%"}

 
**Static**:

$$  \small
Y_{t} = \beta_0 + 0.5 X_{t} + U_{t}
$$

Covariate $X_t$ independent from $U_t$ and over time

:::

  
::: {.column width="8%"} 

:::

::: {.column width="46%"} 
  
**Dynamic**:

$$ \small
Y_{t} = \beta_0 + \beta_1 Y_{t-1} + U_{t}
$$

Dependence: $\beta_1$ value

$$  \small
\beta_1 \in \curl{0, 0.5, 0.95}
$$

:::

::::

- One DGP per $U_t, X_t, Y_0$: each $N(0, 1)$
- $T=50, 200$ 

#### Reminder: Current State of Simulation

Last time: implemented things with a basic function
 

```{python} 
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from typing import Tuple
```


```{python}

plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

BG_COLOR = "whitesmoke"
THEME_COLOR = "#3c165c"
```


```{python}  
class StaticNormalDGP:
    """A data-generating process (DGP) for a static linear model

    Attributes:
        beta0 (float): Intercept term.
        beta1 (float): Slope coefficient.
    """

    def __init__(self, beta0: float = 0.0, beta1: float = 0.5) -> None:
        """Initializes the DGP with intercept and slope.

        Args:
            beta0 (float): Intercept term. Defaults to 0.0.
            beta1 (float): Slope coefficient. Defaults to 1.0.
        """
        self.beta0: float = beta0
        self.beta1: float = beta1

    def sample(
        self, n_obs: int, seed: int | None = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Samples data from the static DGP.

        Args:
            n_obs (int): Number of observations to sample.
            seed (int, optional): Random seed for reproducibility. Defaults to None.

        Returns:
            tuple: (x, y) arrays, each of length n_obs.
        """
        rng = np.random.default_rng(seed)
        x = rng.normal(size=n_obs)
        u = rng.normal(size=n_obs)
        y = self.beta0 + self.beta1 * x + u
        return x, y

  
class DynamicNormalDGP:
    """A data-generating process (DGP) for a dynamic linear model: y_t = beta0 + beta1*y_{t-1} + u_t.

    Attributes:
        beta0 (float): Intercept term.
        beta1 (float): AR(1) coefficient.
    """

    def __init__(self, beta0: float = 0.0, beta1: float = 0.5):
        """Initializes the DGP with intercept and AR(1) coefficient.

        Args:
            beta0 (float): Intercept term. Defaults to 0.0.
            beta1 (float): AR(1) coefficient. Defaults to 0.5.
        """
        self.beta0: float = beta0
        self.beta1: float = beta1

    def sample(
        self, n_obs: int, seed: int | None = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Samples data from the dynamic DGP.

        Args:
            n_obs (int): Number of observations to sample.
            seed (int, optional): Random seed for reproducibility. Defaults to None.

        Returns:
            tuple: (x, y) arrays, each of length n_obs.
                  x is y_{t-1} (lagged y), and y is y_t.
        """
        rng = np.random.default_rng(seed)
        y = np.zeros(n_obs + 1)  # Extra observation for lag
        u = rng.normal(size=n_obs + 1)
        y[0] = self.beta0 + u[0]  # Initial condition
        for t in range(1, n_obs + 1):
            y[t] = self.beta0 + self.beta1 * y[t - 1] + u[t]
        # Return lagged y as x and y[1:] as y
        return y[:-1], y[1:]
```


```{python}
#| echo: true 
#| code-line-numbers: "45-51"
class SimulationRunner:
    """Runs Monte Carlo simulations for a given DGP and estimator.

    Attributes:
        dgp: Data-generating process with a `sample` method.
        estimator: Estimator with a `fit` method and `beta1_hat` attribute.
        errors: array of estimation errors (beta1_hat - beta1) for each simulation.
    """

    def __init__(
        self,
        dgp: "StaticNormalDGP" | "DynamicNormalDGP",
        estimator: SimpleOLS,
    ) -> None:
        """Initializes the simulation runner.

        Args:
            dgp: An instance of a DGP class (must implement `sample`).
            estimator: An instance of an estimator class (must implement `fit`).
        """
        self.dgp = dgp
        self.estimator = estimator
        self.errors = None

    def simulate(self, n_sim: int, n_obs: int, first_seed: int | None = None) -> None:
        """Runs simulations and stores estimation errors.

        Args:
            n_sim (int): number of simulations to run.
            n_obs (int): Number of observations per simulation.
            first_seed (int | None): Random seed for reproducibility. Defaults to None.
        """
        # Preallocate array to hold estimation errors
        self.errors = np.empty(n_sim)

        # Run simulation
        for sim_id in range(n_sim):
            # Draw data
            x, y = self.dgp.sample(n_obs, seed=first_seed + sim_id if first_seed else None)
            # Fit model
            self.estimator.fit(x, y)
            # Store error
            self.errors[sim_id] = self.estimator.beta1_hat - self.dgp.beta1

    def summarize_bias(self) -> None:
        """Prints the average estimation error (bias) for beta1. 
        """ 
        print(f"Average estimation error (bias): {np.mean(self.errors):.4f}")
```



```{python}
class SimpleOLS:
    """A simple OLS estimator for the linear model y = beta0 + beta1*x + u.

    Attributes:
        beta0_hat (float | None): Estimated intercept. None until fit is called.
        beta1_hat (float | None): Estimated slope. None until fit is called.
    """

    def __init__(self) -> None:
        """Initializes the OLS estimator with no estimates."""
        self.beta0_hat: float | None = None
        self.beta1_hat: float | None = None

    def fit(self, x: np.ndarray, y: np.ndarray) -> None:
        """Fit OLS to the provided data.

        Args:
            x (np.ndarray): Independent variable (1D array).
            y (np.ndarray): Dependent variable (1D array).
        """
 
        # Add constant to x
        X = np.column_stack([np.ones(len(x)), x])
        # OLS estimation
        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
        self.beta0_hat, self.beta1_hat = beta_hat[0], beta_hat[1]
```
 



### Shifting Scope and Adding Other Estimators {background="#43464B"}

#### Problem: Scope Change

Now imagine that the scope of the overall project changes to a broader one:

<div class="left-color">

Comparing bias of several "near-OLS" estimators in a static and dynamic settings

</div>
 
- Scope changes are natural, especially in research (often based in preliminary results)
- Need to be flexible both in statistics and code


. . . 

Can our code accommodate more estimators?

#### Adding a New Estimator: Ridge


<div class="left-color">

Consider the simple *ridge* estimator:
$$
\hat{\bbeta} = (\bX'\bX+\lambda I)^{-1} \bX'\bY
$$
$\lambda$ — regularization strength

</div>

- Ridge — one of simpler examples of <span class="highlight">shrinkage</span> estimators
- Ridge solves $L^2$-regularized 

#### Ridge Estimator and `SimulationRunner`



#### Implementation of Simple Ridge Estimator

```{python}
#| echo: true
#| code-line-numbers: "1,10,14,16,27"
class SimpleRidge:
    """A simple ridge estimator for the linear model y = beta0 + beta1*x + u.

    Attributes:
        beta0_hat (float | None): Estimated intercept. None until fit is called.
        beta1_hat (float | None): Estimated slope. None until fit is called.
        reg_param (float): Strength of regularization. Defaults to 0.01.
    """

    def __init__(self, reg_param: float = 0.01) -> None:
        """Initializes the OLS estimator with no estimates."""
        self.beta0_hat: float | None = None
        self.beta1_hat: float | None = None
        self.reg_param = reg_param

    def fit(self, x: np.ndarray, y: np.ndarray) -> None:
        """Fits the ridge estimator to the provided data.

        Args:
            x (np.ndarray): Independent variable (1D array).
            y (np.ndarray): Dependent variable (1D array).
        """
 
        # Add constant to x
        X = np.column_stack([np.ones(len(x)), x])
        # OLS estimation
        beta_hat = np.linalg.inv(X.T @ X + self.reg_param * np.eye(2)) @ X.T @ y
        self.beta0_hat, self.beta1_hat = beta_hat[0], beta_hat[1]
```

#### Bias vs. Regularization Strength

```{python}
#| echo: true
#| code-line-numbers: "5-6,9-10"
# Initialize DGP and estimator
static_dgp = StaticNormalDGP(beta0=0.0, beta1=0.5)

# Estimators
ridge_weak = SimpleRidge(reg_param=0.01)
ridge_strong = SimpleRidge(reg_param=100)

# Initialize and run simulations
sim_ridge_weak = SimulationRunner(static_dgp, ridge_weak)
sim_ridge_strong = SimulationRunner(static_dgp, ridge_strong)
sim_ridge_weak.simulate(n_sim=1000, n_obs=50, first_seed=1)
sim_ridge_strong.simulate(n_sim=1000, n_obs=50, first_seed=1)

# Summarize bias 
sim_ridge_weak.summarize_bias()
sim_ridge_strong.summarize_bias()
```

#### Technical

even though type hints (recall Python does not enforce them, though code quality checks can catch them)


duck typing saved us, but still would be nice to fix that — next time

## Organizing Classes {background="#00100F"}
 

### Organizing Classes with Protocols {background="#43464B"}


####  

Couple of problems

- How do you communicate and remember 
  - You can write docs: that's good, but extra effort that a lot of people do not take until later
  - You can try to look at code: works best when it's freshly written by you, but harder otherwise
- Our type hints

#### A  

#### Alternative: ABCs

Protocols are not the only way

One can also use abstract base classes 

There is a bit more infrastructure

## Organizing Files {background="#00100F"}
 


### Lecture Info {background="#43464B"}


#### Modules


conventions differ

#### A

```
project/
├── dgps/
│   ├── __init__.py
│   ├── static.py       # StaticNormalDGP
│   └── dynamic.py      # DynamicNormalDGP
├── estimators/
│   ├── __init__.py
│   ├── ols-like.py     # SimpleOLS 
├── runner.py           # SimulationRunner
└── main.py             # CLI entry point
```

::: {.footer}

See chapter 24 in @Lutz2025LearningPythonPowerful about `__init__.py`

:::

####

makes sense

- Maybe split more, maybe split less: choose what's appropriate, easy for understanding and collaboration, etc


### Executing {background="#43464B"}


####

automatically only when
the module is run as a script, not when it is imported. The line you add will probably test the value
of `__name__` for the string `__main__` 

#### Running


```bash

py main.py

```

## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 

- Discussed   
 
#### Next Questions

<br>
 
- How to organize running many simulations? 
- How to apply this logic to deeper statistical scenarios?

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
