---
title: "Writing Good Simulation Code"
subtitle: "How to Structure Code: From Monolithic to Modular Design"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="  (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Why Simulate and General Principles"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: " (lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is  

<br>

By the end, you should be able to


- Discuss  

#### References

::: {.nonincremental}

Statistics:

- Chapter 4 of @Hansen2022Econometrics for OLS
- Chapter 14 of @Hansen2022Econometrics for time series basics

Programming

- Chapter 26-27 of @Lutz2025LearningPythonPowerful on OOP in Python

:::


### Simulation Setting {background="#43464B"}


#### Context: Bias of OLS Estimator

Consider the simple causal linear model:

$$
Y_{t}^x = \beta_0 + \beta_1 x + U_t, \quad t=1, \dots, T
$$
$Y_t^x$ — potential outcome under $x$ for observation $t$

<br>

Basic econometrics tells us that OLS estimator in regressing $Y_t$ on $(1, X_t)$ is

- Unbiased if $\E[U_t|X_1, \dots, X_T] = 0$
- Biased otherwise 

#### Why Care About Bias?

<div class="left-color">

Why should a user of OLS care about this bias?  
 
</div>


<br>

 
If bias is big, can lead to

- Improperly centered confidence intervals
- Wrong sign of point estimate
 
<br>

$\Rightarrow$ both can lead to incorrect policy conclusions
 
#### Today's Question

How big the bias is 

A more targeted approach would tabulate incorrect signs



## Setting Out Simulation Goals {background="#00100F"}
 

 

### Choosing Context {background="#43464B"}



#### Why Simulations?

Theory not fully satisfactory: 

- Expression for bias depends on the DGP for $(X_t, U_t)$
- @Bao2007SecondOrderBiasMean: asymptotic order $O(1/T)$
- Unclear if actual magnitude "important in practice"


. . .

Empirical data validation not an option, as cannot measure true bias in practice due to causal nature


<div class="left-color">

Have to recur to simulations

</div>

#### Specifying Simulations


Recall that simulations should be 

- Realistic 
- Targeted


### Specifying DGPs {background="#43464B"}

#### 

$T$

## Naïve Approach {background="#00100F"}
 

 

### Basic Function {background="#43464B"}

#### Learning Outcomes

This lecture is  

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def run_ols_simulation(n_sim=1000, n_obs=100, beta0=0, beta1=1, rho=0, dgp_type="strict"):
    """
    Monolithic function to simulate OLS bias under two DGPs.

    Args:
        n_sim: Number of simulations
        n_obs: Number of observations per simulation
        beta0: Intercept
        beta1: Slope coefficient
        rho: AR(1) coefficient (only used if dgp_type="ar1")
        dgp_type: "strict" (strict exogeneity) or "ar1" (AR(1) errors)
    """
    results = []

    for _ in range(n_sim):
        if dgp_type == "strict":
            # Strict exogeneity DGP
            x = np.random.normal(size=n_obs)
            epsilon = np.random.normal(size=n_obs)
            y = beta0 + beta1 * x + epsilon
        elif dgp_type == "ar1":
            # AR(1) DGP
            u = np.zeros(n_obs)
            for t in range(1, n_obs):
                u[t] = rho * u[t-1] + np.random.normal()
            x = np.random.normal(size=n_obs)
            y = beta0 + beta1 * x + u
        else:
            raise ValueError("dgp_type must be 'strict' or 'ar1'")

        # OLS estimation (manual for clarity)
        X = np.column_stack([np.ones(n_obs), x])
        beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
        results.append(beta_hat[1])  # Store slope estimate

    # Plot results
    plt.figure(figsize=(10, 6))
    plt.hist(results, bins=30, edgecolor="k", alpha=0.7)
    plt.axvline(beta1, color="red", linestyle="--", label="True $\\beta_1$")
    plt.title(f"Distribution of OLS Estimates ({dgp_type} DGP, ρ={rho})")
    plt.xlabel("Estimated $\\beta_1$")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()

    return results

# Example usage:
strict_results = run_ols_simulation(dgp_type="strict")
ar1_results = run_ols_simulation(dgp_type="ar1", rho=0.7)

```

## Writing Better Code {background="#00100F"}
 
  

### Further Improvements {background="#43464B"}

#### Unifying 

This lecture is  


## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 
 

#### Next Questions

<br>

Now  
#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
