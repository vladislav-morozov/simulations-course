---
title: "Writing Good Simulation Code"
subtitle: "How to Structure Code: From Monolithic to Modular Design"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="  (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Why Simulate and General Principles"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: " (lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is  

<br>

By the end, you should be able to


- Discuss  

#### References

::: {.nonincremental}

Statistics:

- Chapter 4 of @Hansen2022Econometrics for OLS
- Chapter 14 of @Hansen2022Econometrics for time series basics

Programming

- Chapter 26-27 of @Lutz2025LearningPythonPowerful on OOP in Python

:::


### Simulation Setting {background="#43464B"}


#### Context: Bias of OLS Estimator

Consider the simple causal linear model:

$$
Y_{t}^x = \beta_0 + \beta_1 x + U_t, \quad t=1, \dots, T
$$
$Y_t^x$ — potential outcome under $x$ for observation $t$

<br>

Basic econometrics tells us that OLS estimator in regressing $Y_t$ on $(1, X_t)$ is

- Unbiased if $\E[U_t|X_1, \dots, X_T] = 0$
- Biased otherwise 

#### Why Care About Bias?

<div class="left-color">

Why should a user of OLS care about this bias?  
 
</div>


<br>

 
If bias is big, can lead to

- Improperly centered confidence intervals
- Wrong sign of point estimate
 
<br>

$\Rightarrow$ both can lead to incorrect policy conclusions
 
#### Today's Questions

::: {.nonincremental}

<br>

<div class="rounded-box">

- How big is the bias?
- How to think about and structure simulations in general?
  
 
</div>

:::



<br> 

Today — a showcase of 

- Our three-step simulation anatomy
- A sketch for extensible simulation design — foundation for later

#### Imports

```{python}
#| echo: true
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
```


```{python}

plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial"]

BG_COLOR = "whitesmoke"
THEME_COLOR = "#3c165c"
```


## Setting Out Simulation Goals {background="#00100F"}
 

 

### Choosing Context {background="#43464B"}



#### Why Not Theory?

Theory not fully satisfactory: expression for bias depends on the DGP for $(X_t, U_t)$:
$$
\E[\hat{\bbeta}] - \bbeta= \E\left[ (\bW'\bW)^{-1}\bW'\bU \right]
$$
where $\bW$ — $T\times 2$ with $t$th row given by $(1, X_t)$


- @Bao2007SecondOrderBiasMean: asymptotic order $O(1/T)$
- Unclear if actual magnitude "important in practice"


#### Why Not Use Real Data?

 
Empirical data validation not an option both in causal and predictive settings:

- Can never measure true bias even if believe in linear model
- In predictive settings: don't even care about $\hat{\bbeta}$, only about predicted $\hat{Y}_t$

. . .

<br>


<div class="left-color">

Have to recur to simulations

</div>

#### Simulation Requirements



<br>

Recall that simulations should be 

- Realistic: need to think about relevant real world scenario to emulate

- Targeted: 
    - Be specific in terms of target metrics
    - Focus in DGPs that cause differences in target metrics
 
#### Choosing Scenario: Time Series

As an example, we care about <span class="highlight">time series</span>

<br>

What's relevant?

- Possibility of dependence across time
- Different strength of dependence
- Different lengths of time series
- (More advanced): data drift/time-varying DGP

. . . 

We'll include the first three

#### Metrics

For simplicity, we will consider just one explicit metric:

<div class="rounded-box">

Our metric: absolute bias of OLS estimator of $\beta_1$:

$$
\text{Bias}(\hat{\beta}_1) = \E[\hat{\beta}_1] - \beta_1
$$

</div>

 


Other metrics:

- Coverage of CIs based on the OLS estimator
- Proportion of incorrect signs ($\mathrm{sgn}(\hat{\beta}_1)\neq \mathrm{sgn}(\beta_1)$)



### Specifying DGPs {background="#43464B"}

#### The Problem of DGP Choice

We now have to choose data generating processes that reflect

- Dynamics of different strength
- Different sample sizes

. . .

Ideally: would specify based on some reference empirical datasets

<div class="left-color">

Here: talk in a very stylized manner. Later — more specific designs

</div>

#### Static vs. Dynamic

First dimension: include both static   and dynamic cases



:::: {.columns}

::: {.column width="46%"}

 
**Static**:

$$  \small
Y_{t} = \beta_0 + 0.5 X_{t} + U_{t}
$$

Covariate $X_t$ independent from $U_t$ and over time

:::

  
::: {.column width="8%"} 

:::

::: {.column width="46%"} 
  
**Dynamic**:

$$ \small
Y_{t} = \beta_0 + \beta_1 Y_{t-1} + U_{t}
$$

Dependence: $\beta_1$ value

$$  \small
\beta_1 \in \curl{0, 0.5, 0.9}
$$

:::

::::


<div class="left-color">

Bias appears in dynamic setting, but not static. Checking this dimension is <span class="highlight">targeted</span>

</div>


#### Sample Size and Distributions of $U_t, X_t, Y_0$

<div class="left-color">


::: {.nonincremental}

Second and third design dimension:

2. Sample sizes
3. DGPs for variables not determined by model

:::

</div>


To start with — keep things simple to focus on essentials:

- One DGP per $U_t, X_t, Y_0$: each $N(0, 1)$
- $T=50, 200$ 


. . .

Different sample sizes: how bias changes (targeted)






## First Implementation {background="#00100F"}
 

 

### Basic Function {background="#43464B"}


#### Summary So Far

So far specified:

- Metric
- How each dataset should be generate
- Size of each dataset

. . .

<br>

<div class="left-color">

Now time to actually implement — but how?

</div>

#### General Advice

How to not get overwhelmed — a good universal rule: 

<div class="rounded-box">

Start with the simplest pieces and iterate from there


</div>

<br>

- Simulations are often fairly complex piece of code
- Do not try to write the whole thing in one go
- Do not be afraid of rewriting and improving things later

::: {.footer}

A good way to work is to follow a lightweight agile approach

:::

#### Practice: Starting with the Static Case
 

Our simplest thing: static model with a small sample
$$
Y_{t} = \beta_0 + \beta_1 X_t + U_t, \quad T=50
$$



Remember simulation anatomy: `for` many datasets we

1. Draw 50 points $(X_t, Y_t)$
2. Run OLS estimator $\hat{\beta}_1$
3. Compute error $\hat{\beta}_1-\beta_1$

Simplest: wrap in a single function

#### Example: Basic Simulation Function {.scrollable}



```{python}
#| echo: true
def simulate_ols(n_sim=1000, n_obs=50, beta0=0, beta1=0.5):
    """Simulates OLS estimation for a static/exogenous DGP: y = beta0 + beta1*x + u.

    Args:
        n_sim (int): Number of simulations to run. Defaults to 1000.
        n_obs (int): Number of observations per simulation. Defaults to 100.
        beta0 (float): True intercept value. Defaults to 0.
        beta1 (float): True slope coefficient. Defaults to 0.5.

    Returns:
        list[float]: List of errors of beta1 estimates for each simulation.
    """

    # Create container to store results
    results_ols_errors = []

    # Initialize the RNG
    rng = np.random.default_rng()

    # Core simulation loop
    for _ in range(n_sim):
        # 1. Draw data
        x = rng.normal(size=n_obs)
        u = rng.normal(size=n_obs)
        y = beta0 + beta1 * x + u

        # 2. Run OLS estimation
        W = np.column_stack([np.ones(n_obs), x])
        beta_hat = np.linalg.inv(W.T @ W) @ W.T @ y

        # 3. Compute error
        results_ols_errors.append(beta_hat[1]-beta1)

    return results_ols_errors
```

#### Our First Results: Static Model

Now can execute our simulation by calling function with default arguments:
```{python}
#| echo: true
static_results = simulate_ols()
print(f"Bias (static DGP): {np.mean(static_results)}")
```

Qualitatively:

- Bias small relative to coefficients (recall $\beta_1=0.5$)
- Theory is confirmed

::: {.footer}

In practice you would usually wrap the function in a script and call from the command line

:::

#### Distribution of MC Estimates

Can also take a look at density of estimates:


```{python}
fig, ax = plt.subplots(figsize=(14, 4.5))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor(THEME_COLOR)
fig.patch.set_linewidth(5)
sns.kdeplot(static_results, ax=ax, color=THEME_COLOR)
plt.axvline(0, color=THEME_COLOR, linestyle="--", label="No bias")
plt.axvline(np.mean(static_results), color="brown", linestyle="--", label="MC Estimate")
plt.title(
    f"Distribution of OLS Estimates under Static Model",
    loc="left",
)
plt.xlabel("Estimation error")
plt.legend()
plt.show()

```


 
### Expanding the Simulation {background="#43464B"}




#### Expanding the Simulation
 
<div class='left-color'>

Now need to expand simulations to add AR(1) design

</div>
 
Simplest way: just expand existing function

- Add new DGP option `ar1`
- Involves changing `simulate_ols` to have a new argument for `dgp_type`
  - Old beahvior: `dgp_type="static"`
  - New behavior: `dgp_type="ar1"`
- Also deal with losing one observation to $Y_{t-1}$

#### Expanded Simulation Function: With AR(1) DGP

```{python}
#| echo: true
#| code-line-numbers: "1,27-35"
def simulate_ols(n_sim=1000, n_obs=50, beta0=0, beta1=0.5, dgp_type="static"):
    """Simulates OLS estimation for static or AR(1) DGP.
    Args:
        n_sim (int): Number of simulations to run. Defaults to 1000.
        n_obs (int): Number of observations per simulation. Defaults to 100.
        beta0 (float): True intercept value. Defaults to 0.
        beta1 (float): True slope coefficient. Defaults to 0.5
        dgp_type (str): Type of DGP: "static" or "ar1". Defaults to "static".

    Returns:
        list[float]: List of errors of beta1 estimates for each simulation.
    """

    # Create container to store results
    results_ols_errors = []

    # InitializeRNG
    rng = np.random.default_rng()

    # Core simulation loop
    for _ in range(n_sim):
        # 1. Draw data
        if dgp_type == "static":
            x = rng.normal(size=n_obs)
            u = rng.normal(size=n_obs)
            y = beta0 + beta1 * x + epsilon
        elif dgp_type == "ar1":
            y = np.zeros(n_obs + 1)
            u = rng.normal(size=n_obs + 1)
            y[0] = beta0 + u[0]
            for t in range(1, n_obs + 1):
                y[t] = beta0 + beta1 * y[t-1] + u[t]
            # Use the last n_obs elements of y as and the first n_obs as x
            x = y[:-1]  # x is y lagged (n_obs elements)
            y = y[1:]  # y[1:] is the dependent variable (n_obs elements)
            

        # 2. Run OLS estimation
        W = np.column_stack([np.ones(n_obs), x])
        beta_hat = np.linalg.inv(W.T @ W) @ W.T @ y
        
        # 3. Compute error
        results_ols_errors.append(beta_hat[1]-beta1)   

    return results_ols_errors
```

#### Running AR(1) Simulation
 
Can now run our simulations with $\beta \in \curl{0, 0.5, 0.95}$:

```{python}
#| echo: true
ar_results_no_pers = simulate_ols(beta1=0, dgp_type="ar1")
ar_results_med_pers = simulate_ols(beta1=0.5, dgp_type="ar1")
ar_results_high_pers = simulate_ols(beta1=0.95, dgp_type="ar1")
print(f"Bias (no persistence): {np.mean(ar_results_no_pers):.3f}")
print(f"Bias (medium persistence): {np.mean(ar_results_med_pers):.3f}")
print(f"Bias (high persistence): {np.mean(ar_results_high_pers):.3f}")
```

. . .

Conclusions:

<div class='left-color'>

- More persistence = larger absolute bias
- Direction: downward (underestimating)

</div>
  


 

## A More Robust Implementation {background="#00100F"}
 

### Modular Design {background="#43464B"}

#### More DGPs

more sample sizes, more DGPs (e.g. heavier tails)

#### Issues

We had to rewrite

Note: it's not bad if you just want run one thing


What if we want?

Also didn't set a seed.



#### Problem: No Separation of Concerns

Things are all jumbled up and hardcoded 

- DGPs
- Estimator

But does the simulation loop really need to know that? From the point of simulation loop, so

#### Separation of Concerns

Every part occupied with its own activity


####

Also preallocate space with a NumPy array (sicne we know)


### Further Improvements {background="#43464B"}

#### Wrapping Into a Single Block

So far — just called things separately

- Dangerous: what if you forget something? 
- Also annoying

#### Unifying 
 

#### How Much Information


#### Better DGPs

attached to some empirical dataset


## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 
 

#### Next Questions

<br>

Now  

#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
