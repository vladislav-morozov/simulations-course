---
title: "Evaluating Machine Learning Algorithms"
subtitle: "Working with Predictive Settings"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content="Learn how to evaluate (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Evaluating Predictive Algorithms"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: "Learn how to evaluate (lecture note slides)" 
---






## Introduction {background="#00100F"}

```{python}
import matplotlib.pyplot as plt

from itertools import product

from sklearn.datasets import make_classification

BG_COLOR = "whitesmoke"
```

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is about evaluating machine learning algorithms

<br>

By the end, you should be able to

 

- Understand 

### Lecture Example: Dealing with Class Imbalance {background="#43464B"}

#### Problem of Class Imbalance in Classification

Setting of today: classification with class imbalance

<br>

Empirically know that class imbalance may cause classifier bias towards the majority classes, leading to

- Poor generalization
- Misleading accuracy metrics
- Potentially incorrect policy recommendations


::: {.footer}

Intuitively: if one class is 99% of the data, you can get 99% accuracy for free by always predicting that class

:::


#### Example Datasets With Different Imbalances

::: {.nonincremental}

- Two class example
- Each row: progressively stronger class imbalance

:::

```{python}
N_ROWS = 3
N_COLS = 4
fig, axs = plt.subplots(nrows=N_ROWS, ncols=N_COLS, figsize=(16,5))
fig.patch.set_color(BG_COLOR)
for row_id, col_id in product(range(N_ROWS), range(N_COLS)):
    X, y = make_classification(
      n_samples=300,
      n_features=2,
      n_redundant=0,
      weights=[0.95 - 0.4*(N_ROWS - 1 - row_id)/(N_ROWS-1), 0.05 + 0.4*(N_ROWS - 1 - row_id)/(N_ROWS-1)],
      class_sep=1.7,
      flip_y=0.05,
      random_state= 67*row_id + 237*col_id
    )
    axs[row_id, col_id].scatter(X[y==1, 0], X[y==1, 1], color='gold', marker='x')
    axs[row_id, col_id].scatter(X[y==0, 0], X[y==0, 1], color='#3c165c')
    axs[row_id, col_id].set_xticks([]) 
    axs[row_id, col_id].set_yticks([]) 
```

::: {.footer}

Data sampled using `sklearn.datasets.make_classification`

:::

#### Solutions to Class Imbalance

There are some solutions:

- Undersampling majority classes (throwing away data)
- Oversample minority classes (possibly with synthetic examples, e.g. SMOTE)
- Biasing algorithms towards minority classes by modifying training (e.g. weights in objective functions)

::: {.left-color}

Which of these are better?

:::

::: {.footer}

SMOTE: Synthetic Minority Over-sampling Technique

:::

#### Goal of Today: Evaluating Solutions

<br>

Today:

::: {.left-color}

Compact simulation in low-dimensional setting comparing

- Not doing anything
- Synthetic oversampling
- Weights in objective function

:::



## Theory Essentials for Evaluating ML Algorithms {background="#00100F"}
  
### Core Concepts {background="#43464B"}

#### Goal of Statistical/Machine Learning


<div class="rounded-box">

Key goal of prediction --- predicting <span class="highlight">well</span>

</div> 

Other goals:

- Computational efficiency: better to have a cheaper and quicker way to produce a new prediction
- Interpretability: why does the algorithm predict what it does?
- Scalability: can it handle increasing loads, run in distributed manner, etc

#### Monte Carlo vs. Goals of Prediction


::: {.left-color}

Monte Carlo simulations can be used to check all four criteria

:::

In particular: 

- Predictive quality thanks to access to true labels
- Efficiency and scalability --- easy to produce more and different kinds of data to test systems
- Interpretability --- have true DGP, can contrast explanations with true dependence


#### Components of a Setting

A predictive problem is defined by

- DGP
- Risk function that expresses our preference
- A machine learning algorithm: 
  - Class $\Hcal$ of hypotheses
  - Some decision rule that selects an $\hat{h}\in\Hcal$ after seeing data


#### Loss and Risk

Quality of prediction measured with *risk* function

Let $h(\bX)$ be a prediction of $Y$ given $\bX$  (<span class="highlight">hypothesis</span>) 


<div class="rounded-box">

 

Let the <span class="highlight">loss function </span>$l(y, \hat{y})$ satisfy $l(y, \hat{y})\geq 0$ and $l(y, y)=0$ for all $y, \hat{y}$.


The <span class="highlight">risk function</span> of the hypothesis (prediction) $h(\cdot)$ is the expected loss:
$$
R(h) = \E_{(Y, \bX)}\left[ l(Y, h(\bX))  \right]
$$


 


</div>


 
#### Interpretation: Generalization Error

<div class="rounded-box">

Risk measures how well the hypothesis $h$ performs on unseen data --- <span class="highlight">generalization error</span>  

</div>

Example: indicator risk:
$$
\E[\I\curl{Y\neq h(\bX)}] = P(Y\neq h(\bX))
$$
Probability of incorrectly predicting $Y$ with $h(\bX)$ --- where $Y$ and $\bX$ are a new observation


#### Algorithms

::: {.left-color}

Informally, a machine learning algorithm is a rule for picking some hypothesis from some $\Hcal$

:::

Algorithms differ in 

- $\Hcal$ (e.g. linear functions of $\bX$, ensembles of decision trees in $\bX$, chains of affine functions and nonlinear transforms (NNs), etc.)
- Rule for selecting from $\Hcal$: minimizing empirical risk; minimizing surrogate risk; adding or not adding a complexity penalty, etc.


### Evaluation Metrics {background="#43464B"}

#### Core Metrics: Risk

::: {.left-color}


Basic metr

:::


#### How To Evaluate Risk in Monte Carlo

#### Other Metrics
 

## Simulation Design {background="#00100F"}

#### DGP

Try a couple of degrees of imbalance

#### Algorithms Considered

::: {.footer}

As always, I suggest that you play with the code, add your own algorithms and see the effects

:::

#### Approaches for Dealing with Class Imbalance




#### Metrics


## Simulation Implementation and Solutions {background="#00100F"}
  
### Implementation {background="#43464B"}

#### Overall Organization Strategy

Again a more complex setting with possible desire to 

#### Runner Design

Do a bit differently: use the same dataset for all 

Makes

#### Our `SimulationRunner`

#### What `SimulationRunner` Does



### Results {background="#43464B"}

#### Results For Balanced Classes (50/50)

|                                                                          |   Precision |   Recall |   $F_1$ |
|:-------------------------------------------------------------------------|--------------:|-----------:|-------:|
| No correction    |         0.907 |      0.906 |  0.906 | 
| With SMOTE      |         0.907 |      0.906 |  0.906 | 
| Balanced likelihood weights |         0.907 |      0.906 |  0.906 | 

<br>

::: {.left-color}

Effectively no difference in the results

:::


#### Results with Strong Imbalance (90/10)

|                                                                          |   Precision |   Recall |   $F_1$ |
|:-------------------------------------------------------------------------|--------------:|-----------:|-------:| 
| No correction   |         0.838 |      0.612 |  0.695 | 
| With SMOTE            |         0.549 |      0.871 |  0.662 | 
| Balanced likelihood weights |         0.533 |      0.876 |  0.652 |



#### References 
  
::: {.nonincremental}
 
:::
 
 

## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 

- Reviewed  
 


#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
