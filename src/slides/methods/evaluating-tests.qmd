---
title: "Evaluating Hypothesis Tests"
subtitle: "SUBTITLE"
author: Vladislav Morozov  
format:
  revealjs:
    include-in-header: 
      text: |
        <meta name="description" content=" (lecture note slides)"/> 
    width: 1150
    slide-number: true
    sc-sb-title: true
    incremental: true   
    logo: ../../themes/favicon.ico
    footer: "Evaluating Hypothesis Tests"
    footer-logo-link: "https://vladislav-morozov.github.io/simulations-course/"
    theme: ../../themes/slides_theme.scss
    toc: TRUE
    toc-depth: 2
    toc-title: Contents
    transition: convex
    transition-speed: fast
slide-level: 4
title-slide-attributes:
    data-background-color: "#3c165cff"
    data-footer: " "
filters:
  - reveal-header 
embed-resources: true
include-in-header: ../../themes/mathjax.html 
highlight-style: tango
open-graph:
    description: "  (lecture note slides)" 
---






## Introduction {background="#00100F"}
 
  
```{python}
import matplotlib.pyplot as plt
import numpy as np

from scipy.stats import chi2
BG_COLOR = "whitesmoke"
THEME_COLOR = "#3c165c"
```

### Lecture Info {background="#43464B"}

#### Learning Outcomes

This lecture is about evaluating hypothesis tests

<br>

By the end, you should be able to

 
- Capture 


#### References

<br>

::: {.nonincremental}

 
- [My slides from undergraduate econometrics](https://vladislav-morozov.github.io/econometrics-2/slides/vector/ols-inference.html) for some more theory and examples on linear hypothesis tests


- @Hansen2022Econometrics

- Chapter 9 in @Lehmann2022TestingStatisticalHypotheses on multiple comparison problems

:::

### Motivating Example Question {background="#43464B"}

#### Setting: Joint Hypotheses
 
Often want to test <span class="highlight">joint hypotheses</span>: hypotheses that involve several statements about parameters at the same time:

<div class="left-color">
$$
\begin{aligned}
& H_0: \text{for all $k$ the statement $A_k$ is true} \quad \text{vs.} \\
&  H_1: \text{for at least some $k$, the statement $A_k$ is not true}
\end{aligned}
$$

</div>

<br>

Example: null that coefficients are zero ($A_k=\curl{\theta_k=0}$)
$$
\begin{aligned}
H_0: \theta_k = 0, \quad k=1, \dots, p
\end{aligned}
$$

#### Motivating Question for Simulation

Two example approaches:

- With a simultaneous (joint) test (e.g. Wald/$F$, LM, LR)
- Testing each component separately (e.g. with a $t$-test) and combining the results (with adjusted $p$-values)

. . . 


<br>

Question of today:

<div class="left-color">

When do we generally use the first approach? 

</div>
 
#### Goals for Today:

How to answer this question? 

<br>

Today talk about

- What matters for comparing hypothesis tests
- How to simulate tests
- How to interpret results

## Theory Essentials for Hypothesis Testings {background="#00100F"}
 

### Definitions {background="#43464B"}



#### Basic Setup: Hypotheses

Suppose that we have a model with some parameters $\theta$ (of whatever nature)

<br>

<div class="left-color">

Two competing *hypotheses* (statements about parameters $\theta$)
$$
H_0: \theta\in \Theta_0 \text{  vs.  } H_1: \theta \in \Theta_1 
$$
for some non-intersecting $\Theta_0$ and $\Theta_1$

</div>

#### Examples of Hypotheses

Example: linear model of the form:

$$\small
Y_i  = \sum_{k=0}^{p} \beta_k X_i^{(k)}
$$

- Hypothesis on one parameter: $H_0: \beta_k \leq 0$ vs. $H_1: \beta_k >0$
- Hypothesis about many parameters: 
$$ \small
H_0: \beta_0= \dots = \beta_p =0 \quad \text{ vs. } H_1: \beta_k\neq 0 \text{ for some }k
$$

#### Definition of a Test

A test is a <span class="highlight">decision rule</span>: you see the sample and then you decide in favor of $H_0$ or $H_1$


. . . 

<br>

Formally:

<div class="rounded-box">

::: {#def-vector-inference-test}

A test $T$ is a function of the sample $(X_1, \dots, X_N)$ to the space $\{ \text{Reject } H_0$,  $\text{Do not reject }H_0 \}$

:::

</div>

::: {.footer}

Technically, @def-vector-inference-test is about <span class="highlight">deterministic</span> tests, there are also randomized tests

:::



#### Power

<br>

Key property of a hypothesis test: 
<div class="rounded-box">

::: {#def-vector-inference-power}

The power function $\text{Power}_T(\theta)$ of the test $T$ is the probability that $T$ rejects if $\theta$ is the true parameter value:
$$
\text{Power}_T(\theta) = P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta)
$$

:::

</div>

 

#### Test Size

Maximal power under the null has a special name 
<div class="rounded-box">

::: {#def-vector-inference-size}

The <span class="highlight"> size </span> $\alpha$ of the test $T$ is 
$$
\alpha = \max_{\theta\in\Theta_0} \text{Power}_T(\theta)
$$

:::

</div>

In other words, the probability of falsely rejecting the null — <span class="highlight">type I error</span>


#### What Defines a Good Test? 

The best possible test has perfect detection:

- Never rejects under $H_0$
- Always reject under $H_1$

. . . 

<br>

Usually impossible in practice. Instead, we ask

- Not too much false rejection under $H_0$ (e.g. $\leq 5\%$ of the time)
- As much rejection as possible under $H_1$ 
 

#### Role of Size and Power


<div class="rounded-box">

Power function — key metric in comparing tests

</div>

<br>

::: {.nonincremental}

 

Give preference to tests that

- Control size correctly 
- Reject alternative more often (=use data more efficiently)

:::

### Tests and Test Statistics {background="#43464B"}

 
 
 


#### How Testing Works in General

<div class="left-color"> 

How do we construct a test/decision rule?

</div>

<br>

. . .

Basic approach:

1. Pick a "statistic" (=some known function of the data) that behaves "<span class="highlight">differently</span>" under $H_0$ and $H_1$
2. Is the observed value of the statistic <span class="highlight">compatible</span> with $H_0$? 
   - No $\Rightarrow$ reject $H_0$ in favor or $H_1$
   - Yes $\Rightarrow$ do not reject $H_0$






#### Measuring Compatibility with $H_0$

<div class="left-color">

Recall: compare with <span class="highlight">critical values</span> to measure compatibility with $H_0$

</div>

<br>

Critical values chosen to guarantee

- Exact size (when possible)
- Asymptotic size (otherwise) 

At least in the limit, the test should not falsely reject $H_0$ "too much"

::: {.footer}

The asymptotic size $\alpha$ of the test $T$ is $\alpha = \lim_{N\to\infty} \max_{\theta\in\Theta_0}  P(T(X_1, \dots, X_N)=\text{Reject }H_0|\theta)$

:::
 

#### Reminder: Main Classes of Statistics

- In principle, can pick any statistic. Some are more "standard"
- For testing hypotheses about coefficients, there are three main classes:
  - Wald statistics: need only *unrestricted* estimates 
  - Lagrange multiplier (LM): need *restricted* estimates 
  - Likelihood ratio (LR): need both
 

#### Example I: $t$-Test for $H_0: \theta_k=0$ vs. $H_1: \theta_k\neq 0$
 

Suppose that have estimator $\hat{\theta}_k$ such that
$$ \small
\sqrt{N}(\hat{\theta}_k-\theta_k)\xrightarrow{d} N(0, \avar(\hat{\theta}_k))
$$
where $N$ is sample size


Will base the test on the <span class="highlight">$t$-statistic</span>:
$$ \small
t = \dfrac{\hat{\theta}_k}{\sqrt{ \widehat{\avar}(\hat{\theta}_k)/N }  }
$$


#### Example I: Definition of $t$-Test  

<br>


We call the following the <span class="highlight">asymptotic size $\alpha$ $t$-test</span>:

<div class="rounded-box">

Let $z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)$. Then

- Reject $H_0$ is $\abs{t}>z_{1-\alpha/2}$
- Do not reject $H_0$ is $\abs{t}\leq z_{1-\alpha/2}$


</div>

 

#### Example II: Wald Test for $H_0: \bR\btheta =\bc$ vs. $H_1: \bR\btheta \neq \bc$
 
Let $\btheta= (\theta_0, \theta_1, \dots, \theta_p$, $\bR$ some matrix, $\bc$ some vector

Suppose that have estimator $\hat{\btheta}$ such that
$$ \small
\sqrt{N}(\hat{\btheta}-\btheta)\xrightarrow{d} N(0, \avar(\hat{\btheta}))
$$


Will base the test on the <span class="highlight">Wald statistic</span>:
$$ \small
W = N\left(  \bR\hat{\bbeta}-\bq   \right)'\left(\bR\widehat{\avar}(\bbeta)\bR'\right)^{-1}\left(  \bR\hat{\bbeta}-\bq   \right)
$$ 

#### Example II: Definition of Wald Test

We call the following the <span class="highlight">asymptotic size $\alpha$ Wald-test</span>:


<br>

<div class="rounded-box">

Let $c_{1-\alpha}$ solve $P(\chi^2_k\leq c_{1-\alpha})=1-\alpha$ where $k$ is the number of rows and rank of $\bR$. Then

- Reject $H_0$ if $W>c_{1-\alpha}$
- Do not reject $H_0$ if $W\leq c_{1-\alpha}$


</div>

#### Plot: PDF of $\chi^2_k$ and Rejection Region (Shaded)

```{python}


# Parameters for the chi-squared distribution
df = 5

# Generate the x values for the PDF
x_vals = np.linspace(0, 20, 1000)

# Compute the PDF values
pdf_vals = chi2.pdf(x_vals, df)

# Compute the 95th quantile
quantile_95 = chi2.ppf(0.95, df)

# Set up the figure and the axes
fig, ax = plt.subplots(figsize=(15, 6))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor(THEME_COLOR)
fig.patch.set_linewidth(5)

# Plot the PDF
ax.plot(x_vals, pdf_vals, label=f'PDF of $\\chi^2_k$ distribution', color=THEME_COLOR)

# Fill the area under the PDF to the right of the 95th quantile
ax.fill_between(x_vals, pdf_vals, where=(x_vals >= quantile_95), color='darkorange', alpha=0.3)

# Mark the 95th quantile
ax.axvline(quantile_95, color='red', linestyle='--', label="$c_{1-\\alpha}$")

# Set the labels and title 
ax.set_ylabel('Density') 
ax.legend(loc='upper right')
ax.set_ylim(0, 0.17)
ax.set_xlim(0, 20)
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_facecolor(BG_COLOR)

# Show the plot
plt.show()

```
 
## Preparing Simulation: Joint vs. Multiple Tests {background="#00100F"}

#### Plan For This Part

Now turn to answering our motivating question: joint vs. mulitple tests for joint nulls

<br>

Plan:

- A reminder on joint vs. multiple tests
- Choosing simulation design: DGP, tests to compare 

After: implementation

### Background: Joint and Multiple Tests {background="#43464B"}

#### Example Setting

Suppose that have coefficients $\theta_k$ and want to test the joint null
$$
H_0: \theta_k = c_k, \quad k=1, \dots, p
$$
Have estimator $\hat{\btheta}$ such that $\sqrt{N}(\hat{\btheta}-\btheta)\xrightarrow{d} N(0, \avar(\hat{\btheta}))$

. . . 

::: {.left-color}

Two main approaches for testing $H_0$:

- Single joint test
- Multiple tests


:::

#### Joint Tests

 
::: {.left-color}


A <span class="highlight">joint test</span> uses a single statistic to conduct the text


:::

<br>

Example: Wald test based on

$$
W = N(\hat{\btheta}-\bc)'(\widehat{\avar}(\hat{\btheta}))^{-1}(\hat{\btheta}-\bc),
$$
Test based on comparing $W$ with $(1-\alpha)$th quantile of $\chi^2_p$

#### Multiple Test

::: {.left-color}


A <span class="highlight">multiple test</span> aggregates the decisions of many separate tests


:::

For example, let 
$$ \small 
t_k = \dfrac{\sqrt{N}(\hat{\btheta}_k-c_k)}{\widehat{\avar}(\hat{\btheta}_k)}
$$
 
A  <span class="highlight">multiple $t$-test</span> for $H_0$ rejects if $\max_{k=1, \dots, p}\lbrace \abs{t_k}\rbrace$ exceeds some prespecified critical value $c_{\alpha}$


#### Critical Values and Multiple Comparisons Problem


::: {.left-color}


Critical values of multiple tests have to be set to ensure overall correct size

:::

Have to be careful: e.g. with $p=2$ have
$$
\begin{aligned}
& P(\text{test rejects}|H_0) \\
& = P\left( \lbrace |{t}_1| \geq c_{\alpha}\rbrace   |H_0 \right) + P\left(   \lbrace |{t}_2| \geq c_{\alpha}\rbrace |H_0 \right)  \\
& \quad - P\left( \lbrace |{t}_1| \geq c_{\alpha}\rbrace \cap \lbrace |{t}_2| \geq c_{\alpha}\rbrace |H_0 \right),
\end{aligned}
$$

Hence if each $t$-test is a size $\alpha$-test, resulting multiple test has

$$
P(\text{test rejects}|H_0) \in [\alpha, 2\alpha]
$$

::: {.footer}

See [this xckd](https://xkcd.com/882/) on multiple comparisons. Or see chapter 9 of @Lehmann2022TestingStatisticalHypotheses for theory

::: 

### Choosing a Basic DGP {background="#43464B"}

#### Recap
 
Now know the most important metrics:

 
::: {.left-color}

Should compare joint vs. multiple approach to testing in terms of <span class="highlight">power</span>

::: 

<br>

But now key open questions:

- Which tests exactly to use? 
- In which scenario? 

 
#### Technique: Reference Scenario

Today: 

- Trying to understand joint vs. multiple choice 
- <span class="highlight">Without any constraints on setting</span>
 
::: {.rounded-box}

When no constraint on setting is present, start with

- Simplest DGPs
- Simplest methods (here tests)

::: 
Ideally: simplest = some kind of "textbook" reference setting
 
#### Nice Reference Scenarios for Testing

Very often in testing:

- The nicest scenario is a simple linear model w
- Everything is normally distribution

. . .

<br>

Key reason: 

- Many practical tests are based on asymptotic normality
- Hence exact normality = close to idealized asymptotic setting

::: {.footer}

There are theoretical reasons for importance of normality as a "base" case, see chapters 6-7 in @VanderVaart1998AsymptoticStatistics

:::

#### Reference Scenario: Simple Linear Model

Simple linear model:
$$
Y_i = \theta_0 + \theta_1 X_i^{(1)} + \theta_2 X_{i}^{(2)} + U_i
$$


. . . 

<br>

Very simple kind of joint hypothesis:
$$
H_0: \theta_1 = \theta_2 = 0
$$

#### Reference DGPs: Distributions for $U_i$ and $(X_i^{(1)}, X_i^{(2)})$
 
$$
\begin{aligned}
U_{i} & \sim N(0, 1), \\
\begin{pmatrix}
X_i^{(1)} \\
X_i^{(2)}
\end{pmatrix} & \sim N\left(\begin{pmatrix}
1 \\
1
\end{pmatrix}, \begin{pmatrix}
1 & \rho \\
\rho & 1
\end{pmatrix} \right)
\end{aligned},
$$
where the correlation $\rho$ runs between $-1$ and $1$

. . . 

::: {.left-color}

Varying $\rho$ — key axis controlling geometry of many asymptotic joint tests — they depend on $\avar(\hat{\bbeta})$

::: 

#### DGPs: Coefficient Values

Power function is function of $\btheta=(\theta_0, \theta_1, \theta_2)$ 

$$ \small
\text{Power}_T(\btheta) = P(T(X_1, \dots, X_N)=\text{Reject }H_0|\btheta)
$$  

::: {.left-color}

Always need to look at DGPs that vary at least in coefficients in that enter in $H_0$ 

::: 

- Here: consider $\theta_1=\theta_2=c$ and vary $c$
- Under each $c$ test $H_0:\theta_1=\theta_2=0$ 

. . . 


$\Rightarrow$ DGPs vary in $c$ and $\rho$

::: {.footer}

Not that restrictive to have $\theta_1=\theta_2$: can suitably rotate $(X_{i}^{(1)}, X_{i}^{(2)})$ to make any $(\theta_1, \theta_2)$ to  $\theta_1=\theta_2$.

::: 



### Choosing Tests {background="#43464B"}


#### Which Tests to Use?

In our case: think about which tests are the most popular

<br>

- For joint: Wald test is easiest theoretically and practically
- For multiple: individual $t$ tests for $H_0: \theta_1=0$ and $H_0: \theta_2=0$




Will compare those based on OLS estimator, just need to choose <span class="highlight">adjusted critical values for multiple $t$-test</span>


#### Bonferroni Correction

Recall: if each $t$-test is done at size $\alpha$, then multiple $t$-test based on two has
$$
P(\text{test rejects}|H_0) \leq 2\alpha
$$

::: {.left-color}

If want overall multiple test to have size $\leq \alpha$, need to do each component test with size $\alpha/2$ 
:::

- Example: 2.5% individual tests for 5% overall
- This is called the <span class="highlight">Bonferroni correction</span>

#### Role of $\rho$

$\rho$ controls correlation between $\hat{\theta}_1$ and $\hat{\theta}_2$ with $\mathrm{corr}(\hat{\theta}_1, \hat{\theta}_2) \approx -\rho$
```{python}
from matplotlib.patches import Rectangle
from scipy.stats.distributions import chi2, norm

# Define contour value c
c = chi2.ppf(0.95, df=2)  # Change this to the desired value of c
t_bound = norm.ppf(0.025 / 2)

# Create a grid of x1 and x2 values
x1 = np.linspace(-5, 5, 400)
x2 = np.linspace(-5, 5, 400)
X1, X2 = np.meshgrid(x1, x2)

rhos = [-0.99, 0, 0.99]

# Plot the contour for the specified value c
fig, axs = plt.subplots(1, 3, figsize=(16, 4))
fig.patch.set_facecolor(BG_COLOR)
fig.patch.set_edgecolor(THEME_COLOR)
fig.patch.set_linewidth(5)


for ax, rho in zip(axs, rhos): 
    # Define the matrix A
    A = np.array([[1, rho], [rho, 1]])  # Example symmetric positive-definite matrix
    A = np.linalg.inv(A)

    # Compute the quadratic form x'Ax
    Z = A[0, 0] * X1**2 + A[1, 1] * X2**2 + 2 * A[0, 1] * X1 * X2


    ax.contourf(X1, X2, Z, levels=[c, Z.max()], hatches=["."], colors=["gold"], alpha=0) 
    ax.set_xlabel("$\\hat{\\beta}_1$", color="white")
    ax.set_ylabel("$\\hat{\\beta}_2$", color="white")

 
    ax.set_xbound(-3, 3)
    ax.set_ylim(-4.3, 4.3)

    ax.add_patch(
        Rectangle(
            (t_bound, t_bound),
            -2 * t_bound,
            -2 * t_bound,
            facecolor="none",
            ec=THEME_COLOR,
            lw=2,
            linestyle="--",
        )
    )
 

    # Mask the region inside the rectangle
    Z_masked = np.ma.masked_where(
        (X1 >= t_bound) & (X1 <= -1 * t_bound) & (X2 >= t_bound) & (X2 <= -1 * t_bound), Z
    )
 
    ax.contour(X1, X2, Z, levels=[c], colors="gold")

    ax.text(-5, 4.7, "Correlation($\\hat{\\theta}_1$, $\\hat{\\theta}_2$) = " + str(rho),     fontsize=16,  color="black")

axs[0].set_title(
    "Rejection regions: Wald (dotted) vs. adjusted multiple $t$-test (shaded)\n\n",
    color="black",
    loc="left",
    fontsize=16, 
    weight="bold",
);
plt.show()

```

## Simulating Hypothesis Test Performance {background="#00100F"}
 

### Implementing Individual Components {background="#43464B"}




#### Overall Code Structure

Will keep using the code structure developed in 

actually many DGPs from one class. Creation of multiple things — handled by `scenarios.py`


#### Simulation Runner

#### Handling Results


::: {.footer}

Averages of bounded random variables converge exponentially quickly by [Hoeffding's inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality) 

:::

#### Our `main.py`


## Simulation Results {background="#00100F"}
 
#### 

Now have a larger array of simulation results. 

#### 

Creating graphs — unfortunately very idiosyncratic and lots of trial and error specific to each graph

## Recap and Conclusions {background="#00100F"}
 
#### Recap

<br>

In this lecture we 

- Discussed  




#### References {.allowframebreaks visibility="uncounted"}

::: {#refs}
:::
