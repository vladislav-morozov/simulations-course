@book{Alley2013CraftScientificPresentations,
  title = {The {{Craft}} of {{Scientific Presentations}}: {{Critical Steps}} to {{Succeeed}} and {{Critical Errors}} to {{Avoid}}},
  shorttitle = {The Craft of Scientific Presentations},
  author = {Alley, Michael},
  year = {2013},
  edition = {2. ed},
  publisher = {Springer},
  address = {New York, NY},
  abstract = {The Craft of Scientific Presentations, 2nd edition aims to strengthen you as a presenter of science and engineering. The book does so by identifying what makes excellent presenters such as Brian Cox, Jane Goodall, Richard Feynman, and Jill Bolte Taylor so strong. In addition, the book explains what causes so many scientific presentations to flounder.One of the most valuable contributions of this text is that it teaches the assertion-evidence approach to scientific presentations. Instead of building presentations, as most engineers and scientists do, on the weak foundation of topic phrases and},
  isbn = {978-1-4419-8278-0 978-1-4419-8279-7},
  langid = {english}
}

@article{Nikolov2022WritingTipsEconomics,
  title = {Writing {{Tips}} for {{Economics Research Papers}}:  2021-2022 {{Edition}}},
  author = {Nikolov, Plamen},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4114601},
  urldate = {2025-10-12},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\E2SNXBAP\Nikolov - 2022 - Writing Tips for Economics Research Papers  2021-2022 Edition.pdf}
}

@book{Schimel2012WritingScienceHow,
  title = {Writing {{Science}}: {{How}} to {{Write Papers That Get Cited}} and {{Proposals That Get Funded}}},
  shorttitle = {Writing Science},
  editor = {Schimel, Joshua},
  year = {2012},
  publisher = {Oxford University Press},
  address = {Oxford New York},
  abstract = {"As a scientist, you are a professional writer: your career is built on successful proposals and papers. Success isn't defined by getting papers into print, but by getting them into the reader's consciousness. Writing Science is built upon the idea that successful science writing tells a story. It uses that insight to discuss how to write more effectively. Integrating lessons from other genres of writing with those from the author's years of experience as author, reviewer, and editor, the book shows scientists and students how to present their research in a way that is clear and that will maximize reader comprehension. The book takes an integrated approach, using the principles of story structure to discuss every aspect of successful science writing, from the overall structure of a paper or proposal to individual sections, paragraphs, sentences, and words. It begins by building core arguments, analyzing why some stories are engaging and memorable while others are quickly forgotten, and proceeds to the elements of story structure, showing how the structures scientists and researchers use in papers and proposals fit into classical models. The book targets the internal structure of a paper, explaining how to write clear and professional sections, paragraphs, and sentences in a way that is clear and compelling. The ideas within a paper should flow seamlessly, drawing readers along. The final section of the book deals with special challenges, such as how to discuss research limitations and how to write for the public. Writing Science is a much-needed guide to succeeding in modern science. Its insights and strategies will equip science students, scientists, and professionals across a wide range of scientific and technical fields with the tools needed to communicate effectively."--Provided by publisher},
  isbn = {978-0-19-976023-7 978-0-19-990951-3 978-1-283-42787-6},
  langid = {english}
}


@book{Dormann2025StatisticsSimulationSynthetic,
  title = {Statistics by {{Simulation}}: {{A Synthetic Data Approach}}},
  shorttitle = {Statistics by Simulation},
  author = {Dormann, Carsten F. and Ellison, Aaron M.},
  year = {2025},
  publisher = {Princeton University Press},
  address = {Princeton Oxford},
  abstract = {"An accessible guide to understanding statistics using simulations, with examples from a range of scientific disciplines Real-world challenges such as small sample sizes, skewed distributions of data, biased sampling designs, and more predictors than data points are pushing the limits of classical statistical analysis. This textbook provides a new tool for the statistical toolkit: data simulations. It shows that using simulation and data-generating models is an excellent way to validate statistical reasoning and to augment study design and statistical analysis with planning and visualization. Although data simulations are not new to professional statisticians, Statistics by Simulation makes the approach accessible to a broader audience, with examples from many fields. It introduces the reasoning behind data simulation and then shows how to apply it in planning experiments or observational studies, developing analytical workflows, deploying model diagnostics, and developing new indices and statistical methods. - Covers all steps of statistical practice, from planning projects to post-hoc analysis and model checking - Provides examples from disciplines including sociology, psychology, ecology, economics, physics, and medicine - Includes R code for all examples, with data and code freely available online - Offers bullet-point outlines and summaries of each chapter - Minimizes the use of jargon and requires only basic statistical background and skills"-- Provided by publisher},
  isbn = {978-0-691-25877-5 978-0-691-27389-1},
  langid = {english}
}

@book{Lutz2025LearningPythonPowerful,
  title = {Learning {{Python}}: {{Powerful Object-Oriented Programming}}},
  shorttitle = {Learning {{Python}}},
  author = {Lutz, Mark},
  year = {2025},
  edition = {Sixth edition},
  publisher = {O'Reilly},
  address = {Santa Rosa, CA},
  isbn = {978-1-0981-7130-8 978-1-0981-7126-1},
  langid = {english}
}

@book{Skoulikari2023LearningGitHandson,
  title = {Learning {{Git}}: A {{Hands-on}} and {{Visual Guide}} to the {{Basics}} of {{Git}}},
  shorttitle = {Learning {{Git}}},
  author = {Skoulikari, Anna},
  year = {2023},
  edition = {First edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-0981-3391-7 978-1-0981-3388-7},
  langid = {english}
}

@book{Wager2024CausalInferenceStatistical,
  title = {Causal {{Inference}}: {{A Statistical Learning Approach}}},
  author = {Wager, Stefan},
  year = {2024}
}

@article{Chernozhukov2019InferenceCausalStructural,
  title = {Inference on {{Causal}} and {{Structural Parameters}} Using {{Many Moment Inequalities}}},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
  editor = {De Paula, Aureo},
  year = {2019},
  month = oct,
  journal = {The Review of Economic Studies},
  volume = {86},
  number = {5},
  pages = {1867--1900},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdy065},
  urldate = {2025-09-29},
  abstract = {This article considers the problem of testing many moment inequalities where the number of moment inequalities, denoted by p, is possibly much larger than the sample size n. There is a variety of economic applications where solving this problem allows to carry out inference on causal and structural parameters; a notable example is the market structure model of Ciliberto and Tamer (2009) where p = 2m+1 with m being the number of firms that could possibly enter the market. We consider the test statistic given by the maximum of p Studentized (or t-type) inequality-specific statistics, and analyse various ways to compute critical values for the test statistic. Specifically, we consider critical values based upon (1) the union bound combined with a moderate deviation inequality for self-normalized sums, (2) the multiplier and empirical bootstraps, and (3) two-step and three-step variants of (1) and (2) by incorporating the selection of uninformative inequalities that are far from being binding and a novel selection of weakly informative inequalities that are potentially binding but do not provide first-order information. We prove validity of these methods, showing that under mild conditions, they lead to tests with the error in size decreasing polynomially in n while allowing for p being much larger than n; indeed p can be of order exp(nc) for some c {$>$} 0. Importantly, all these results hold without any restriction on the correlation structure between p Studentized statistics, and also hold uniformly with respect to suitably large classes of underlying distributions. Moreover, in the online supplement, we show validity of a test based on the block multiplier bootstrap in the case of dependent data under some general mixing conditions.},
  copyright = {https://academic.oup.com/journals/pages/about\_us/legal/notices},
  langid = {english},
  file = {/home/vladislav/Zotero/storage/5MJLIAYB/Chernozhukov et al. (2019) Inference on Causal and Structural Parameters using Many Moment Inequalities.pdf}
}

@article{Delaigle2004PracticalBandwidthSelection,
	title = {Practical {{Bandwidth Selection}} in {{Deconvolution Kernel Density Estimation}}},
	author = {Delaigle, A. and Gijbels, I.},
	year = {2004},
	month = mar,
	journal = {Computational Statistics \& Data Analysis},
	volume = {45},
	number = {2},
	pages = {249--267},
	issn = {01679473},
	doi = {10.1016/S0167-9473(02)00329-8},
	urldate = {2025-10-01},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	langid = {english}
}


@article{Li1998NonparametricEstimationMeasurement,
  title = {Nonparametric {{Estimation}} of the {{Measurement Error Model Using Multiple Indicators}}},
  author = {Li, Tong and Vuong, Quang},
  year = {1998},
  month = may,
  journal = {Journal of Multivariate Analysis},
  volume = {65},
  number = {2},
  pages = {139--165},
  issn = {0047259X},
  doi = {10.1006/jmva.1998.1741},
  urldate = {2025-09-29},
  abstract = {This paper considers the nonparametric estimation of the densities of the latent variable and the error term in the standard measurement error model when two or more measurements are available. Using an identification result due to Kotlarski we propose a two-step nonparametric procedure for estimating both densities based on their empirical characteristic functions. We distinguish four cases according to whether the underlying characteristic functions are ordinary smooth or supersmooth. Using the loglog Law and von Mises differentials we show that our nonparametric density estimators are uniformly convergent. We also characterize the rate of uniform convergence in each of the four cases. 1998 Academic Press AMS Subject classification: primary, 62G05; secondary, 62G30.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/vladislav/Zotero/storage/8I9IZBEV/Li and Vuong (1998) Nonparametric Estimation of the Measurement Error Model Using Multiple Indicators.pdf}
}

@inproceedings{Simard1992EfficientPatternRecognition,
  title = {Efficient {{Pattern Recognition Using}} a {{New Transformation Distance}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Simard, Patrice and Le Cun, Yann and Denker, John},
  year = {1992},
  month = nov,
  series = {{{NIPS}}'92},
  pages = {50--58},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2025-07-15},
  abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases.},
  isbn = {978-1-55860-274-8}
}

@inproceedings{Montufar2014NumberLinearRegions,
  title = {On the {{Number}} of {{Linear Regions}} of {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Mont{\'u}far, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = jun,
  doi = {10.48550/arXiv.1402.1869},
  urldate = {2025-08-10},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  isbn = {978-1-5108-0041-0},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\moren\\Zotero\\storage\\B2M9LSHW\\Montúfar et al. - 2014 - On the Number of Linear Regions of Deep Neural Networks.pdf;C\:\\Users\\moren\\Zotero\\storage\\KMLZ87PI\\1402.html}
}

@article{Cybenko1989ApproximationSuperpositionsSigmoidal,
  title = {Approximation by {{Superpositions}} of a {{Sigmoidal Function}}},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals, and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2025-08-10},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Deep Learning\Approximation Properties\Cybenko - 1989 - Approximation by Superpositions of a Sigmoidal Function.pdf}
}

@article{Callaway2021DifferenceinDifferencesMultipleTime,
  title = {Difference-in-{{Differences With Multiple Time Periods}}},
  author = {Callaway, Brantly and Sant'Anna, Pedro H.C.},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {200--230},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.12.001},
  urldate = {2025-08-27},
  abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the ``parallel trends assumption" holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001--2007. Open-source software is available for implementing the proposed methods. {\copyright} 2020 Elsevier B.V. All rights reserved.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\DiDs and Event Studies\DiD\Callaway and Sant’Anna - 2021 - Difference-in-Differences With Multiple Time Periods.pdf}
}

@article{Goodman-Bacon2021DifferenceinDifferencesVariationTreatment,
  title = {Difference-in-{{Differences With Variation}} in {{Treatment Timing}}},
  author = {{Goodman-Bacon}, Andrew},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {254--277},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2021.03.014},
  urldate = {2025-08-27},
  abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, "pre" and "post", and two groups, "treatment" and "control". Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of twoway fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\DiDs and Event Studies\DiD\Goodman-Bacon - 2021 - Difference-in-Differences With Variation in Treatment Timing.pdf}
}

@article{Rambachan2023MoreCredibleApproach,
  title = {A {{More Credible Approach}} to {{Parallel Trends}}},
  author = {Rambachan, Ashesh and Roth, Jonathan},
  year = {2023},
  month = sep,
  journal = {Review of Economic Studies},
  volume = {90},
  number = {5},
  pages = {2555--2591},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdad018},
  urldate = {2025-08-27},
  abstract = {Abstract             This paper proposes tools for robust inference in difference-in-differences and event-study designs where the parallel trends assumption may be violated. Instead of requiring that parallel trends holds exactly, we impose restrictions on how different the post-treatment violations of parallel trends can be from the pre-treatment differences in trends (``pre-trends''). The causal parameter of interest is partially identified under these restrictions. We introduce two approaches that guarantee uniformly valid inference under the imposed restrictions, and we derive novel results showing that they have desirable power properties in our context. We illustrate how economic knowledge can inform the restrictions on the possible violations of parallel trends in two economic applications. We also highlight how our approach can be used to conduct sensitivity analyses showing what causal conclusions can be drawn under various restrictions on the possible violations of the parallel trends assumption.},
  copyright = {https://academic.oup.com/pages/standard-publication-reuse-rights},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\DiDs and Event Studies\DiD\Rambachan and Roth - 2023 - A More Credible Approach to Parallel Trends.pdf}
}

@article{Sun2021EstimatingDynamicTreatment,
  title = {Estimating {{Dynamic Treatment Effects}} in {{Event Studies With Heterogeneous Treatment Effects}}},
  author = {Sun, Liyang and Abraham, Sarah},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {175--199},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.09.006},
  urldate = {2025-08-27},
  abstract = {To estimate the dynamic effects of an absorbing treatment, researchers often use twoway fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\DiDs and Event Studies\DiD\Sun and Abraham - 2021 - Estimating Dynamic Treatment Effects in Event Studies With Heterogeneous Treatment Effects.pdf}
}

@article{Hornik1989MultilayerFeedforwardNetworks,
  title = {Multilayer {{Feedforward Networks Are Universal Approximators}}},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2025-08-10},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{Sutskever2008DeepNarrowSigmoid,
  title = {Deep, {{Narrow Sigmoid Belief Networks Are Universal Approximators}}},
  author = {Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2008},
  month = nov,
  journal = {Neural Computation},
  volume = {20},
  number = {11},
  pages = {2629--2636},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.12-07-661},
  urldate = {2025-08-10},
  abstract = {In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.},
  langid = {english}
}

@book{Murphy2022ProbabilisticMachineLearning,
  title = {Probabilistic {{Machine Learning}}: {{An Introduction}}},
  shorttitle = {Probabilistic Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2022},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts London, England},
  abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
  isbn = {978-0-262-04682-4 978-0-262-36930-5},
  langid = {english}
}

@inproceedings{Boser1992TrainingAlgorithmOptimal,
  title = {A {{Training Algorithm}} for {{Optimal Margin Classifiers}}},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  month = jul,
  pages = {144--152},
  publisher = {ACM},
  address = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130401},
  urldate = {2025-08-01},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptions, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Kernel methods\Boser et al. - 1992 - A Training Algorithm for Optimal Margin Classifiers.pdf}
}

@article{Breiman1996BaggingPredictors,
  title = {Bagging {{Predictors}}},
  author = {Breiman, Leo},
  year = {1996},
  month = aug,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00058655},
  urldate = {2025-08-03},
  abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Ensemble Methods\Breiman - 1996 - Bagging Predictors.pdf}
}

@article{Breiman2001RandomForests,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2025-08-03},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Ensemble Methods\Breiman - 2001 - Random Forests.pdf}
}

@incollection{Scholkopf2001GeneralizedRepresenterTheorem,
  title = {A {{Generalized Representer Theorem}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
  editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {416--426},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44581-1_27},
  urldate = {2025-08-01},
  abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Kernel methods\Schölkopf et al. - 2001 - A Generalized Representer Theorem.pdf}
}

@article{Smola1998ConnectionRegularizationOperators,
  title = {The {{Connection Between Regularization Operators}} and {{Support Vector Kernels}}},
  author = {Smola, Alex J. and Sch{\"o}lkopf, Bernhard and M{\"u}ller, Klaus-Robert},
  year = {1998},
  month = jun,
  journal = {Neural Networks},
  volume = {11},
  number = {4},
  pages = {637--649},
  issn = {08936080},
  doi = {10.1016/S0893-6080(98)00032-X},
  urldate = {2025-08-01},
  abstract = {In this paper a correspondence is derived between regularization operators used in regularization networks and support vector kernels. We prove that the Green's Functions associated with regularization operators are suitable support vector kernels with equivalent regularization properties. Moreover, the paper provides an analysis of currently used support vector kernels in the view of regularization theory and corresponding operators associated with the classes of both polynomial kernels and translation invariant kernels. The latter are also analyzed on periodical domains. As a by-product we show that a large number of radial basis functions, namely conditionally positive definite functions, may be used as support vector kernels. q 1998 Elsevier Science Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Kernel methods\Smola et al. - 1998 - The Connection Between Regularization Operators and Support Vector Kernels.pdf}
}


@misc{An2020EnsembleSimpleConvolutional,
  title = {An {{Ensemble}} of {{Simple Convolutional Neural Network Models}} for {{MNIST Digit Recognition}}},
  author = {An, Sanghyeon and Lee, Minjun and Park, Sanglee and Yang, Heerin and So, Jungmin},
  year = {2020},
  month = oct,
  number = {arXiv:2008.10400},
  eprint = {2008.10400},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.10400},
  urldate = {2025-07-17},
  abstract = {We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3x3, 5x5, and 7x7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, and pooling is not used. Rotation and translation is used to augment training data, which is frequently used in most image classification tasks. A majority voting using the three models independently trained on the training data set can achieve up to 99.87\% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91\% test accuracy. The results can be reproduced by using the code at: https://github.com/ansh941/MnistSimpleCNN},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{Wooldridge2023WhatStandardError,
  title = {What Is a {{Standard Error}}? (And {{How Should We Compute It}}?)},
  shorttitle = {What Is a Standard Error?},
  author = {Wooldridge, Jeffrey M.},
  year = {2023},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {237},
  number = {2},
  pages = {105517},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2023.105517},
  urldate = {2025-04-18},
  abstract = {I review the definition of a standard error from a frequentist perspective, including both exact analysis and asymptotic analysis. Using the linear model for illustration, I discuss the model-based, design-based, and sampling-based approaches to uncertainty in obtaining standard errors. The model-based approach is widely applicable and produces reasonable measures of estimator precision in many settings. In some situations, particularly in the context of clustering, the model-based approach can suffer from ambiguity, and can lead to standard errors that are systematically biased. A combination of the design-based and sampling-based approaches requires the researcher to think about the variation in key explanatory variables when computing standard errors, and it can even apply to cases where the entire population is observed.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Confidence Sets\Wooldridge - 2023 - What is a Standard Error (and How Should We Compute It).pdf}
}

@incollection{Johnson1984ExtensionsLipschitzMappings,
  title = {Extensions of {{Lipschitz Mappings}} into a {{Hilbert Space}}},
  booktitle = {Contemporary {{Mathematics}}},
  author = {Johnson, William B. and Lindenstrauss, Joram},
  editor = {Beals, Richard and Beck, Anatole and Bellow, Alexandra and Hajian, Arshag},
  year = {1984},
  volume = {26},
  pages = {189--206},
  publisher = {American Mathematical Society},
  address = {Providence, Rhode Island},
  doi = {10.1090/conm/026/737400},
  urldate = {2025-07-05},
  isbn = {978-0-8218-5030-5 978-0-8218-7611-4},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Dimensionality Reduction\Johnson and Lindenstrauss - 1984 - Extensions of Lipschitz Mappings into a Hilbert Space.pdf}
}

@book{Breiman1984ClassificationRegressionTrees,
  title = {Classification and {{Regression Trees}}},
  editor = {Breiman, Leo},
  year = {1984},
  edition = {1. CRC Press repr},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton, Fla.},
  isbn = {978-0-412-04841-8},
  langid = {english}
}

@article{Elman1990FindingStructureTime,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = mar,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {0364-0213, 1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  urldate = {2025-06-30},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english}
}

@book{Burkov2025HundredPageLanguageModels,
  title = {The {{Hundred-Page Language Models Book}}},
  author = {Burkov, Andriy},
  year = {2025},
  publisher = {True Positive Inc},
  address = {Quebec},
  isbn = {978-1-7780427-2-0},
  langid = {english}
}

@book{Huyen2022DesigningMachineLearning,
  title = {Designing {{Machine Learning Systems}}: {{An Iterative Process}} for {{Production-Ready Applications}}},
  shorttitle = {Designing Machine Learning Systems},
  author = {Huyen, Chip},
  year = {2022},
  edition = {First edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  isbn = {978-1-0981-0796-3},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\9E58A49S\Huyen - 2022 - Designing machine learning systems an iterative process for production-ready applications.pdf}
}

@article{Valiant1984TheoryLearnable,
  title = {A {{Theory}} of the {{Learnable}}},
  author = {Valiant, Leslie G.},
  year = {1984},
  month = nov,
  journal = {Communications of the ACM},
  volume = {27},
  number = {11},
  pages = {1134--1142},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1968.1972},
  urldate = {2025-06-29},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Learnability\Valiant - 1984 - A Theory of the Learnable.pdf}
}

@misc{Mikolov2013EfficientEstimationWord,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2025-06-27},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\NLP\Embeddings\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf}
}

@book{Alammar2024HandsOnLargeLanguage,
  title = {Hands-{{On Large Language Models}}: {{Language Understanding}} and {{Generation}}},
  shorttitle = {Hands-on Large Language Models},
  author = {Alammar, Jay and Grootendorst, Maarten},
  year = {2024},
  edition = {First edition},
  publisher = {O'Reilly Media, Inc},
  address = {Sebastopol, CA},
  abstract = {AI has acquired startling new language capabilities in just the past few years. Driven by rapid advances in deep learning, language AI systems are able to write and understand text better than ever before. This trend is enabling new features, products, and entire industries. Through his book's visually educational nature, readers will learn practical tools and concepts they need to use these capabilities today. You'll understand how to use pretrained language models for use cases like copywriting and summarization; create semantic search systems that go beyond keyword matching; and use existing libraries and pretrained models for text classification, search, and clusterings. This book also helps you: Understand the architecture of transformer language models that excel at text generation and representation ; Build advanced LLM pipelines to cluster text documents and explore the topics they cover ; Build semantic search engines that go beyond keyword search, using methods like dense retrieval and rerankers ; Explore how generative models can be used, from prompt engineering all the way to retrieval-augmented generation ; Gain a deeper understanding of how to train LLMs and optimize them for specific applications using generative model fine-tuning, contrastive fine-tuning, and in-context learning},
  isbn = {978-1-0981-5096-9},
  lccn = {QA76.9.N38 A43 2024},
  keywords = {Application software,Applications en ingenierie,Apprentissage automatique,Artificial intelligence,Computer programs,Development,Developpement,Engineering applications,Generation automatique de texte,Generative programming (Computer science),Genie logiciel,Intelligence artificielle,Logiciels,Logiciels d'application,Machine learning,Natural language generation (Computer science),Natural language processing (Computer science),Programmation generative,Software engineering,Traitement automatique des langues naturelles},
  annotation = {OCLC: on1428779513}
}

@book{Raschka2022MachineLearningPyTorch,
  title = {Machine {{Learning}} with {{PyTorch}} and {{Scikit-Learn}}: {{Develop Machine Learning}} and {{Deep Learning Models}} with {{Python}}},
  shorttitle = {Machine {{Learning}} with {{PyTorch}} and {{Scikit-Learn}}},
  author = {Raschka, Sebastian and Liu, Yuxi and Mirjalili, Vahid},
  year = {2022},
  edition = {1},
  publisher = {Packt Publishing Limited},
  address = {Birmingham},
  abstract = {bThis book of the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple to code framework/bh4Key Features/h4ulliLearn applied machine learning with a solid foundation in theory/liliClear, intuitive explanations take you deep into the theory and practice of Python machine learning/liliFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practices/li/ulh4Book Description/h4Machine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems. Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself. Why PyTorch? PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric. You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP). This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.h4What you will learn/h4ulliExplore frameworks, models, and techniques for machines to 'learn' from data/liliUse scikit-learn for machine learning and PyTorch for deep learning/liliTrain machine learning classifiers on images, text, and more/liliBuild and train neural networks, transformers, and boosting algorithms/liliDiscover best practices for evaluating and tuning models/liliPredict continuous target outcomes using regression analysis/liliDig deeper into textual and social media data using sentiment analysis/li/ulh4Who this book is for/h4If you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch.Before you get started with this book, you'll need a good understanding of calculus, as well as linear algebra},
  collaborator = {Dzhulgakov, Dmytro},
  isbn = {978-1-80181-931-2 978-1-80181-638-0},
  langid = {english}
}

@misc{Vaswani2017AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-06-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Deep Learning\Architectures\Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@article{Breiman1999PastingSmallVotes,
  title = {Pasting {{Small Votes}} for {{Classification}} in {{Large Databases}} and {{On-Line}}},
  author = {Breiman, Leo},
  year = {1999},
  month = jul,
  journal = {Machine Learning},
  volume = {36},
  number = {1-2},
  pages = {85--103},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1007563306331},
  urldate = {2025-08-03},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Ensemble Methods\Breiman - 1999 - Pasting Small Votes for Classification in Large Databases and On-Line.pdf}
}

@inproceedings{He2016DeepResidualLearning,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2025-06-27},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\I22N8I3Q\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@book{Bishop2024DeepLearningFoundations,
  title = {Deep {{Learning}}: {{Foundations}} and {{Concepts}}},
  shorttitle = {Deep {{Learning}}},
  author = {Bishop, Christopher M. and Bishop, Hugh},
  year = {2024},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-45468-4},
  urldate = {2025-06-16},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-45467-7 978-3-031-45468-4},
  langid = {english}
}

@book{Goodfellow2016DeepLearning,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {The MIT press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-03561-3},
  langid = {english},
  lccn = {006.31}
}

@misc{Morozov2025EconometricsUnobservedHeterogeneity,
  title = {Econometrics with {{Unobserved Heterogeneity}}},
  author = {Morozov, Vladislav},
  year = {2025},
  journal = {Econometrics with Unobserved Heterogeneity},
  doi = {10.5281/ZENODO.15459848},
  urldate = {2025-05-23},
  copyright = {Creative Commons Attribution 4.0 International},
  howpublished = {https://zenodo.org/doi/10.5281/zenodo.15459848}
}

@article{White1980HeteroskedasticityConsistentCovarianceMatrix,
  title = {A {{Heteroskedasticity-Consistent Covariance Matrix Estimator}} and a {{Direct Test}} for {{Heteroskedasticity}}},
  author = {White, Halbert},
  year = {1980},
  month = may,
  journal = {Econometrica},
  volume = {48},
  number = {4},
  eprint = {1912934},
  eprinttype = {jstor},
  pages = {817},
  issn = {00129682},
  doi = {10.2307/1912934},
  urldate = {2025-05-18},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Asymptotic Theory\White - 1980 - A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.pdf}
}

@techreport{Freyaldenhoven2021VisualizationIdentificationEstimation,
  title = {Visualization, {{Identification}}, and {{Estimation}} in the {{Linear Panel Event-Study Design}}},
  author = {Freyaldenhoven, Simon and Hansen, Christian and P{\'e}rez, Jorge P{\'e}rez and Shapiro, Jesse},
  year = {2021},
  month = aug,
  number = {w29170},
  pages = {w29170},
  address = {Cambridge, MA},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w29170},
  urldate = {2025-03-26},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\HXRZDNKP\Freyaldenhoven et al. - 2021 - Visualization, Identification, and Estimation in the Linear Panel Event-Study Design.pdf}
}

@book{Lau2023LearningDataScience,
  title = {Learning {{Data Science}}},
  author = {Lau, Sam},
  year = {2023},
  edition = {1st ed},
  publisher = {O'Reilly Media, Incorporated},
  address = {Sebastopol},
  collaborator = {Gonzalez, Joseph and Nolan, Deborah Ann},
  isbn = {978-1-0981-1300-1 978-1-0981-1297-4},
  langid = {english}
}

@book{Strang2016IntroductionLinearAlgebra,
  title = {Introduction to {{Linear Algebra}}},
  author = {Strang, Gilbert},
  year = {2016},
  edition = {5th edition},
  publisher = {Cambridge press},
  address = {Wellesley},
  isbn = {978-0-9802327-7-6},
  langid = {english},
  lccn = {512.5}
}

@book{Kaye1998LinearAlgebra,
  title = {Linear {{Algebra}}},
  author = {Kaye, Richard and Wilson, Robert},
  year = {1998},
  series = {Oxford Science Publications},
  publisher = {Oxford Univ. Press},
  address = {Oxford},
  isbn = {978-0-19-850238-8 978-0-19-850237-1},
  langid = {english}, 
}

@book{Wooldridge2020IntroductoryEconometricsModern,
  title = {Introductory {{Econometrics}}: A {{Modern Approach}}},
  shorttitle = {Introductory Econometrics},
  author = {Wooldridge, Jeffrey M.},
  year = {2020},
  edition = {Seventh edition},
  publisher = {Cengage},
  address = {Boston, MA},
  abstract = {Wooldridge uses a systematic approach motivated by the major problems facing applied researchers. This text provides important understanding for empirical work in many social sciences, as well as for carrying out research projects},
  isbn = {978-1-337-55886-0},
  langid = {english},
  annotation = {OCLC: 1030403996}
}

@article{Dolley1933CharacteristicsProcedureCommon,
  title = {Characteristics and {{Procedure}} of {{Common Stock Split-ups}}},
  author = {Dolley, James Clay},
  year = {1933},
  journal = {Harvard Business Review},
  volume = {11},
  number = {3},
  pages = {316--326}
}

@article{Snow1856ModeCommunicationCholera,
  title = {On the {{Mode}} of {{Communication}} of {{Cholera}}},
  author = {Snow, John},
  year = {1856},
  month = jan,
  journal = {Edinburgh Medical Journal},
  volume = {1},
  number = {7},
  pages = {668--670},
  issn = {0367-1038},
  urldate = {2025-03-27},
  pmcid = {PMC5307547},
  pmid = {29647347},
  file = {C:\Users\moren\Zotero\storage\74SPD8MK\Snow - 1856 - On the Mode of Communication of Cholera.pdf}
}

@article{MacKinlay1997EventStudiesEconomics,
  title = {Event {{Studies}} in {{Economics}} and {{Finance}}},
  author = {MacKinlay, A. Craig},
  year = {1997},
  journal = {Journal of Economic Literature},
  volume = {35},
  number = {1},
  pages = {13--39},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Event Studies\Mackinlay - 1997 - Event Studies in Economics and Finance.pdf}
}

@book{Huntington-Klein2025EffectIntroductionResearch,
  title = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  author = {{Huntington-Klein}, Nick},
  year = {2025},
  publisher = {{Chapman and Hall/CRC}},
  address = {S.l.},
  isbn = {978-1-032-58022-7},
  langid = {english},
  annotation = {OCLC: 1460916892}
}

@article{Miller2023IntroductoryGuideEvent,
  title = {An {{Introductory Guide}} to {{Event Study Models}}},
  author = {Miller, Douglas L.},
  year = {2023},
  month = may,
  journal = {Journal of Economic Perspectives},
  volume = {37},
  number = {2},
  pages = {203--230},
  issn = {0895-3309},
  doi = {10.1257/jep.37.2.203},
  urldate = {2025-03-25},
  abstract = {The event study model is a powerful econometric tool used for the purpose of estimating dynamic treatment effects. One of its most appealing features is that it provides a built-in graphical summary of results, which can reveal rich patterns of behavior. Another value of the picture is the estimated pre-event pseudo-``effects,'' which provide a type of placebo test. In this essay I aim to provide a framework for a shared understanding of these models. There are several (sometimes subtle) decisions and choices faced by users of these models, and I offer guidance for these decisions.},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\E29HRW9W\Miller (2023) An Introductory Guide to Event Study Models.pdf}
}

@article{Card1994MinimumWagesEmployment,
  title = {Minimum {{Wages}} and {{Employment}}: {{A Case Study}} of the {{Fast Food Industry}} in {{New Jersey}} and {{Pennsylvania}}},
  author = {Card, David and Krueger, Alan},
  year = {1994},
  journal = {American Economic Review},
  volume = {84},
  number = {4},
  pages = {772--793},
  doi = {10.3386/w4509},
  urldate = {2025-03-26},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Labor\Minimum Wage\Card and Krueger - 1994 - Minimum Wages and Employment A Case Study of the Fast Food Industry in New Jersey and Pennsylvania.pdf}
}

@book{Zingaro2021LearnCodeSolving,
  title = {Learn to {{Code}} by {{Solving Problems}}: {{A Python Programming Primer}}},
  shorttitle = {Learn to {{Code}} by {{Solving Problems}}},
  author = {Zingaro, Daniel},
  year = {2021},
  publisher = {No Starch Press},
  address = {New York},
  isbn = {978-1-7185-0132-4},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\2NK6VACQ\Zingaro - 2021 - Learn to Code by Solving Problems A Python Programming Primer.pdf}
}

@book{Hayashi2000Econometrics,
  title = {Econometrics},
  author = {Hayashi, Fumio},
  year = {2000},
  publisher = {Princeton University Press},
  address = {New Jersey},
  abstract = {Hayashi's Econometrics promises to be the next great synthesis of modern econometrics. It introduces first year Ph.D. students to standard graduate econometrics material from a modern perspective. It covers all the standard material necessary for understanding the principal techniques of econometrics from ordinary least squares through cointegration. The book is also distinctive in developing both time-series and cross-section analysis fully, giving the reader a unified framework for understanding and integrating results. Econometrics has many useful features and covers all the important topics in econometrics in a succinct manner. All the estimation techniques that could possibly be taught in a first-year graduate course, except maximum likelihood, are treated as special cases of GMM (generalized methods of moments). Maximum likelihood estimators for a variety of models (such as probit and tobit) are collected in a separate chapter. This arrangement enables students to learn various estimation techniques in an efficient manner. Eight of the ten chapters include a serious empirical application drawn from labor economics, industrial organization, domestic and international finance, and macroeconomics. These empirical exercises at the end of each chapter provide students a hands-on experience applying the techniques covered in the chapter. The exposition is rigorous yet accessible to students who have a working knowledge of very basic linear algebra and probability theory. All the results are stated as propositions, so that students can see the points of the discussion and also the conditions under which those results hold. Most propositions are proved in the text. For those who intend to write a thesis on applied topics, the empirical applications of the book are a good way to learn how to conduct empirical research. For the theoretically},
  isbn = {978-0-691-01018-2 978-1-4008-2383-3},
  langid = {english}
}

@article{Wolpert1992StackedGeneralization,
  title = {Stacked {{Generalization}}},
  author = {Wolpert, David H.},
  year = {1992},
  month = jan,
  journal = {Neural Networks},
  volume = {5},
  number = {2},
  pages = {241--259},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80023-1},
  urldate = {2025-07-02},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Boosting\Wolpert - 1992 - Stacked Generalization.pdf}
}

@article{Pearson1901LinesPlanesClosest,
  title = {On {{Lines}} and {{Planes}} of {{Closest Fit}} to {{Systems}} of {{Points}} in {{Space}}},
  author = {Pearson, Karl},
  year = {1901},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  issn = {1941-5982, 1941-5990},
  doi = {10.1080/14786440109462720},
  urldate = {2025-07-05},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Dimensionality Reduction\Pearson - 1901 - On Lines and Planes of Closest Fit to Systems of Points in Space.pdf}
}

@article{Friedman2001GreedyFunctionApproximation,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  shorttitle = {Greedy Function Approximation},
  author = {Friedman, Jerome H.},
  year = {2001},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  issn = {0090-5364},
  doi = {10.1214/aos/1013203451},
  urldate = {2025-07-02},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Boosting\Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf}
}

@article{Freund1997DecisionTheoreticGeneralizationOnLine,
  title = {A {{Decision-Theoretic Generalization}} of {{On-Line Learning}} and an {{Application}} to {{Boosting}}},
  author = {Freund, Yoav and Schapire, Robert E},
  year = {1997},
  month = aug,
  journal = {Journal of Computer and System Sciences},
  volume = {55},
  number = {1},
  pages = {119--139},
  issn = {00220000},
  doi = {10.1006/jcss.1997.1504},
  urldate = {2025-07-02},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Boosting\Freund and Schapire - 1997 - A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting.pdf}
}

@book{Hastie2009ElementsStatisticalLearning,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {2nd ed},
  publisher = {Springer},
  address = {New York, NY},
  isbn = {978-0-387-84857-0},
  lccn = {Q325.5 .H39 2009},
  keywords = {Bioinformatics,Computational intelligence,Data mining,Forecasting,Inference,Machine learning,Methodology,Statistics}
}

@book{James2023IntroductionStatisticalLearning,
  title = {An {{Introduction}} to {{Statistical Learning}}: {{With Applications}} in {{Python}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan E.},
  year = {2023},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  address = {Cham},
  isbn = {978-3-031-38746-3 978-3-031-38747-0},
  langid = {english}
}

@book{Heiss2024UsingPythonIntroductory,
  title = {Using {{Python}} for {{Introductory Econometrics}}},
  author = {Heiss, Florian and Brunner, Daniel},
  year = {2024},
  edition = {2nd edition},
  publisher = {Independently Published},
  address = {New York},
  isbn = {979-8-6484-3676-3},
  langid = {english}
}

@article{Card2000MinimumWagesEmployment,
  title = {Minimum {{Wages}} and {{Employment}}: {{A Case Study}} of the {{Fast-Food Industry}} in {{New Jersey}} and {{Pennsylvania}}: {{Reply}}},
  shorttitle = {Minimum {{Wages}} and {{Employment}}},
  author = {Card, David and Krueger, Alan B.},
  year = {2000},
  month = dec,
  journal = {American Economic Review},
  volume = {90},
  number = {5},
  pages = {1397--1420},
  issn = {0002-8282},
  doi = {10.1257/aer.90.5.1397},
  urldate = {2025-03-26},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Labor\Minimum Wage\Card and Krueger - 2000 - Minimum Wages and Employment A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.pdf}
}

@article{Neumark2000MinimumWagesEmployment,
  title = {Minimum {{Wages}} and {{Employment}}: {{A Case Study}} of the {{Fast-Food Industry}} in {{New Jersey}} and {{Pennsylvania}}: {{Comment}}},
  shorttitle = {Minimum {{Wages}} and {{Employment}}},
  author = {Neumark, David and Wascher, William},
  year = {2000},
  month = dec,
  journal = {American Economic Review},
  volume = {90},
  number = {5},
  pages = {1362--1396},
  issn = {0002-8282},
  doi = {10.1257/aer.90.5.1362},
  urldate = {2025-03-26},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Labor\Minimum Wage\Neumark and Wascher - 2000 - Minimum Wages and Employment A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.pdf}
}

@article{Neumark2014RevisitingMinimumWage,
  title = {Revisiting the {{Minimum Wage}}---{{Employment Debate}}: {{Throwing Out}} the {{Baby}} with the {{Bathwater}}?},
  author = {Neumark, David and Salas, J M Ian and Wascher, William},
  year = {2014},
  journal = {ILR Review},
  volume = {67},
  number = {3},
  pages = {608--648},
  doi = {10.1177/00197939140670S307},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Labor\Minimum Wage\Neumark et al. - 2014 - Revisiting the Minimum Wage—Employment Debate Throwing Out the Baby with the Bathwater.pdf}
}

@article{Davison2015StatisticsExtremes,
	title = {Statistics of {{Extremes}}},
	author = {Davison, A.C. and Huser, R.},
	year = {2015},
	month = apr,
	journal = {Annual Review of Statistics and Its Application},
	volume = {2},
	number = {1},
	pages = {203--235},
	issn = {2326-8298, 2326-831X},
	doi = {10.1146/annurev-statistics-010814-020133},
	urldate = {2025-03-03},
	abstract = {Statistics of extremes concerns inference for rare events. Often the events have never yet been observed, and their probabilities must therefore be estimated by extrapolation of tail models fitted to available data. Because data concerning the event of interest may be very limited, efficient methods of inference play an important role. This article reviews this domain, emphasizing current research topics. We first sketch the classical theory of extremes for maxima and threshold exceedances of stationary series. We then review multivariate theory, distinguishing asymptotic independence and dependence models, followed by a description of models for spatial and spatiotemporal extreme events. Finally, we discuss inference and describe two applications. Animations illustrate some of the main ideas.},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Davison and Huser - 2015 - Statistics of Extremes.pdf}
}

@misc{BancoDeEspana2024MicrodataIndividualEnterprises,
	title = {Microdata on {{Individual Enterprises}}},
	author = {{Banco De Espa{\~n}a}},
	year = {2024},
	publisher = {The Banco de Espa{\~n}a data Laboratory (BELab)},
	doi = {10.48719/BELAB.CBI9523_01},
	urldate = {2025-03-05},
	abstract = {Microdata available at the Central Balance Sheet Data Office on individual enterprises.  The Banco de Espa{\~n}a Central Balance Sheet Data Office is a service that analyses the economic and financial information of Spanish non-financial corporations and groups, including both that directly contributed by the reporting firms and that provided by the Spanish Association of Mercantile and Property Registrars (CORPME), which collaborates with BELab.},
	langid = {english}
}

@article{Bekaert2015BadEnvironmentsGood,
	title = {Bad {{Environments}}, {{Good Environments}}: A {{Non-Gaussian Asymmetric Volatility Model}}},
	shorttitle = {Bad Environments, Good Environments},
	author = {Bekaert, Geert and Engstrom, Eric and Ermolov, Andrey},
	year = {2015},
	month = may,
	journal = {Journal of Econometrics},
	volume = {186},
	number = {1},
	pages = {258--275},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2014.06.021},
	urldate = {2025-02-02}, 
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Financial Econometrics\Volatilty Modeling\Bekaert et al. - 2015 - Bad Environments, Good Environments a Non-gaussian Asymmetric Volatility Model.pdf}
}

@unpublished{Chavleishvili2024ModelingAsymmetricTail,
	title = {Modeling {{Asymmetric Tail Dependence}} in a {{Non-Gaussian Framework}}},
	author = {Chavleishvili, Sulkhan},
	year = {2024}, 
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Financial Econometrics\Volatilty Modeling\Chavleishvili - 2024 - Modeling Asymmetric Tail Dependence in a Non-Gaussian Framework.pdf}
}

@misc{Xenophontos2021FormulaNthDerivative,
	title = {A Formula for the Nth {{Derivative}} of the {{Quotient}} of {{Two Functions}}},
	author = {Xenophontos, Christos},
	year = {2021},
	month = oct,
	number = {arXiv:2110.09292},
	eprint = {2110.09292},
	primaryclass = {math},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2110.09292},
	urldate = {2025-01-21},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Mathematics - General Mathematics},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Random Useful Math\Analysis\Xenophontos - 2021 - A formula for the nth Derivative of the Quotient of Two Functions.pdf}
}

@article{Imai2021UseTwoWayFixed,
  title = {On the {{Use}} of {{Two-Way Fixed Effects Regression Models}} for {{Causal Inference}} with {{Panel Data}}},
  author = {Imai, Kosuke and Kim, In Song},
  year = {2021},
  month = jul,
  journal = {Political Analysis},
  volume = {29},
  number = {3},
  pages = {405--415},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.33},
  urldate = {2025-01-17},
  abstract = {The two-way linear fixed effects regression (2FE) has become a default method for estimating causal effects from panel data. Many applied researchers use the 2FE estimator to adjust for unobserved unit-specific and time-specific confounders at the same time. Unfortunately, we demonstrate that the ability of the 2FE model to simultaneously adjust for these two types of unobserved confounders critically relies upon the assumption of linear additive effects. Another common justification for the use of the 2FE estimator is based on its equivalence to the difference-in-differences estimator under the simplest setting with two groups and two time periods. We show that this equivalence does not hold under more general settings commonly encountered in applied research. Instead, we prove that the multi-period difference-in-differences estimator is equivalent to the weighted 2FE estimator with some observations having negative weights. These analytical results imply that in contrast to the popular belief, the 2FE estimator does not represent a designbased, nonparametric estimation strategy for causal inference. Instead, its validity fundamentally rests on the modeling assumptions.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Linear Heterogeneous Panels\Imai and Kim - 2021 - On the Use of Two-Way Fixed Effects Regression Models for Causal Inference with Panel Data.pdf}
}

@article{Sarafidis2012CrossSectionalDependencePanel,
  title = {Cross-{{Sectional Dependence}} in {{Panel Data Analysis}}},
  author = {Sarafidis, Vasilis and Wansbeek, Tom},
  year = {2012},
  month = sep,
  journal = {Econometric Reviews},
  volume = {31},
  number = {5},
  pages = {483--531},
  issn = {0747-4938, 1532-4168},
  doi = {10.1080/07474938.2011.611458},
  urldate = {2025-01-17},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Cross-Sectional Correlation\Sarafidis and Wansbeek - 2012 - Cross-Sectional Dependence in Panel Data Analysis.pdf}
}

@book{Antao2023FastPythonHigh,
  title = {Fast {{Python}}: {{High Performance Techniques}} for {{Large Datasets}}},
  shorttitle = {Fast {{Python}}},
  author = {Antao, Tiago},
  year = {2023},
  publisher = {Manning Publications},
  address = {Shelter Island},
  abstract = {Fast Python for Data Science is a hands-on guide to writing Python code that can process more data, faster, and with less resources. It takes a holistic approach to Python performance, showing you how your code, libraries, and computing architecture interact and can be optimized together. Written for experienced practitioners, Fast Python for Data Science dives right into practical solutions for improving computation and storage efficiency. You'll experiment with fun and interesting examples such as rewriting games in lower-level Cython and implementing a MapReduce framework from scratch. Finally, you'll go deep into Python GPU computing and learn how modern hardware has rehabilitated some former antipatterns and made counterintuitive ideas the most efficient way of working. About the technologyFast, accurate systems are vital for handling the huge datasets and complex analytical algorithms that are common in modern data science. Python programmers need to boost performance by writing faster pure-Python programs, optimizing the use of libraries, and utilizing modern multi-processor hardware; Fast Python for Data Science shows you how},
  isbn = {978-1-61729-793-9},
  langid = {english}
}
@book{Okken2021PythonTestingPytest,
  title = {Python {{Testing}} with Pytest: {{Simple}}, {{Rapid}}, {{Effective}} and {{Scalable}}},
  shorttitle = {Python {{Testing}} with Pytest},
  author = {Okken, Brian},
  year = {2021},
  series = {The {{Pragmatic Programmers}}},
  edition = {Second Edition},
  publisher = {The Pragmatic Bookshelf},
  address = {Raleigh, North Carolina},
  isbn = {978-1-68050-860-4}
}

@book{Matthes2023PythonCrashCourse,
  title = {Python {{Crash Course}}: A {{Hands-on}}, {{Project-based Introduction}} to {{Programming}}},
  shorttitle = {Python Crash Course},
  author = {Matthes, Eric},
  year = {2023},
  edition = {3rd edition},
  publisher = {No Starch Press},
  address = {San Francisco},
  abstract = {Includes instructions for basic concepts such as variables, lists, classes, and loops. Practice exercises come with each topic. The end goal is the creation of a Space Invaders-inspired arcade game to deploy online},
  isbn = {978-1-7185-0270-3},
  langid = {english}
}

@book{Hillard2020PracticesPythonPro,
  title = {Practices of the {{Python Pro}}},
  author = {Hillard, Dane},
  year = {2020},
  publisher = {Manning Publications Co},
  address = {Shelter Island, NY},
  abstract = {Professional-quality code does more than just run without bugs. It's clean, readable, and easy to maintain. To step up from a capable Python coder to a professional developer, you need to learn industry standards for coding style, application desgin, and development process. That's where this book is indispensable. "Practices of the Python Pro" teaches you to design and write professional-quality software that's understandable, maintainable, and extensible. Dane Hillard is a Python pro who has helped many dozens of developers make this step, and he knows what it takes. With helpful examples and exercises, he teaches you when, why, and how to modularize your code, how to improve quality by reducing complexity, and much more. Embrace these core principles, and your code will become easier for you and others to read, maintain, and reuse},
  isbn = {978-1-61729-608-6},
  lccn = {QA76.73.P98 H55 2020},
  keywords = {Computer programming,Python (Computer program language)},
  annotation = {OCLC: on1111382463}
}

@book{Hillard2023PublishingPythonPackages,
  title = {Publishing {{Python Packages}}: {{Test}}, {{Share}}, and {{Automate Your Projects}}},
  shorttitle = {Publishing {{Python}} Packages},
  author = {Hillard, Dane},
  year = {2023},
  publisher = {Manning Publications},
  address = {Shelter Island},
  abstract = {Publishing Python Packages teaches you how to easily share your Python code with your team and the outside world. Learn a repeatable and highly automated process for package maintenance that's based on the best practices, tools, and standards of Python packaging. This book walks you through creating a complete package, including a C extension, and guides you all the way to publishing on the Python Package Index. Whether you're entirely new to Python packaging or looking for optimal ways to maintain and scale your packages, this fast-paced and engaging guide is for you. Publishing Python Packages presents a practical process for sharing Python code in an automated and scalable way. Get hands-on experience with the latest packaging tools, and learn the ins and outs of package testing and continuous integration. You'll even get pro tips for setting up a maintainable open source project, including licensing, documentation, and nurturing a community of contributors},
  isbn = {978-1-61729-991-9},
  keywords = {Python (Computer program language),Software engineering}
}

@article{Bezuidenhout2024SingleWorldIntervention,
	title = {Single {{World Intervention Graphs}} ({{SWIGs}}): {{A Practical Guide}}},
	shorttitle = {Single {{World Intervention Graphs}} ({{SWIGs}})},
	author = {Bezuidenhout, Dana and Forthal, Sarah and Rudolph, Kara and Lamb, Matthew R.},
	year = {2024},
	month = sep,
	journal = {American Journal of Epidemiology},
	pages = {kwae353},
	issn = {0002-9262, 1476-6256},
	doi = {10.1093/aje/kwae353},
	urldate = {2024-11-29},
	copyright = {https://academic.oup.com/pages/standard-publication-reuse-rights},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Graphical Models\SWIGs\Bezuidenhout et al. - 2024 - Single World Intervention Graphs (SWIGs) A Practical Guide.pdf}
}

@article{Sarvet2020GraphicalDescriptionPartial,
	title = {A {{Graphical Description}} of {{Partial Exchangeability}}},
	author = {Sarvet, Aaron L. and Wanis, Kerollos Nashat and Stensrud, Mats J. and Hern{\'a}n, Miguel A.},
	year = {2020},
	month = may,
	journal = {Epidemiology},
	volume = {31},
	number = {3},
	pages = {365--368},
	issn = {1044-3983},
	doi = {10.1097/EDE.0000000000001165},
	urldate = {2024-12-02},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Graphical Models\SWIGs\Sarvet et al. - 2020 - A Graphical Description of Partial Exchangeability.pdf}
}

@article{Firpo2007EfficientSemiparametricEstimation,
	title = {Efficient {{Semiparametric Estimation}} of {{Quantile Treatment Effects}}},
	author = {Firpo, Sergio},
	year = {2007},
	month = jan,
	journal = {Econometrica},
	volume = {75},
	number = {1},
	pages = {259--276},
	issn = {0012-9682, 1468-0262},
	doi = {10.1111/j.1468-0262.2007.00738.x},
	urldate = {2024-11-13},
	abstract = {This paper develops estimators for quantile treatment effects under the identifying restriction that selection to treatment is based on observable characteristics. Identification is achieved without requiring computation of the conditional quantiles of the potential outcomes. Instead, the identification results for the marginal quantiles lead to an estimation procedure for the quantile treatment effect parameters that has two steps: nonparametric estimation of the propensity score and computation of the difference between the solutions of two separate minimization problems. Root-N consistency, asymptotic normality, and achievement of the semiparametric efficiency bound are shown for that estimator. A consistent estimation procedure for the variance is also presented. Finally, the method developed here is applied to evaluation of a job training program and to a Monte Carlo exercise. Results from the empirical application indicate that the method works relatively well even for a data set with limited overlap between treated and controls in the support of covariates. The Monte Carlo study shows that, for a relatively small sample size, the method produces estimates with good precision and low bias, especially for middle quantiles.},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	langid = {english},
	file = {C\:\\Users\\moren\\My Drive\\Econometrics Cloud\\Articles\\Causal Inference and Policy Design\\Distributional Analysis\\Quantile Treatment Effects\\Firpo - 2007 - Efficient Semiparametric Estimation of Quantile Treatment Effects.pdf;C\:\\Users\\moren\\My Drive\\Econometrics Cloud\\Articles\\Quantile and Distribution Regression\\Firpo - 2007 - Efficient Semiparametric Estimation of Quantile Treatment Effects.pdf}
}

@incollection{Koenker2017ComputationalMethodsQuantile,
	title = {Computational {{Methods}} for {{Quantile Regression}}},
	booktitle = {Handbook of {{Quantile Regression}}},
	author = {Koenker, Roger},
	editor = {Koenker, Roger and Chernozhukov, Victor and He, Xuming and Peng, Limin},
	year = {2017},
	month = oct,
	edition = {1},
	pages = {55--67},
	publisher = {{Chapman and Hall/CRC}},
	doi = {10.1201/9781315120256-5},
	urldate = {2024-12-09},
	isbn = {978-1-315-12025-6},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Koenker - 2017 - Computational Methods for Quantile Regression.pdf}
}

@article{Koenker1994QuantileSmoothingSplines,
	title = {Quantile {{Smoothing Splines}}},
	author = {Koenker, Roger and Ng, Pin and Portnoy, Stephen},
	year = {1994},
	journal = {Biometrika},
	volume = {81},
	number = {4},
	pages = {673--680},
	issn = {0006-3444, 1464-3510},
	doi = {10.1093/biomet/81.4.673},
	urldate = {2024-12-09},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Nonparametric\Koenker et al. - 1994 - Quantile Smoothing Splines.pdf}
}

@article{Meinshausen2006QuantileRegressionForests,
	title = {Quantile {{Regression Forests}}},
	author = {Meinshausen, Nicolai},
	year = {2006},
	journal = {Journal of Machine Learning Research},
	volume = {7},
	number = {35},
	pages = {983-999},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Nonparametric\Meinshausen - 2006 - Quantile Regression Forests.pdf}
}
@article{Bassett1982EmpiricalQuantileFunction,
	title = {An {{Empirical Quantile Function}} for {{Linear Models}} with {{IID Errors}}},
	author = {Bassett, Gilbert and Koenker, Roger},
	year = {1982},
	month = jun,
	journal = {Journal of the American Statistical Association},
	volume = {77},
	number = {378},
	pages = {407},
	issn = {01621459},
	doi = {10.2307/2287261},
	urldate = {2024-12-09},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Bassett and Koenker - 1982 - An Empirical Quantile Function for Linear Models with IID Errors.pdf}
}
@article{Mathur2024SimpleGraphicalRules,
	title = {Simple {{Graphical Rules}} for {{Assessing Selection Bias}} in {{General-population}} and {{Selected-sample Treatment Effects}}},
	author = {Mathur, Maya B. and Shpitser, Ilya},
	year = {2024},
	month = jun,
	journal = {American Journal of Epidemiology},
	pages = {kwae145},
	issn = {0002-9262, 1476-6256},
	doi = {10.1093/aje/kwae145},
	urldate = {2024-12-10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Graphical Models\SWIGs\Mathur and Shpitser - 2024 - Simple Graphical Rules for Assessing Selection Bias in General-population and Selected-sample Treatm.pdf}
}

@unpublished{Richardson2013SingleWorldIntervention,
	title = {Single {{World Intervention Graphs}}: {{A Primer}}},
	author = {Richardson, Thomas S. and Robins, James M.},
	year = {2013},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Graphical Models\SWIGs\Richardson and Robins - Single World Intervention Graphs A Primer.pdf},
	note = {2013}
}

@article{Sasaki2015WhatQuantileRegressions,
	title = {What {{Do Quantile Regressions Identify}} for {{General Structural Functions}}?},
	author = {Sasaki, Yuya},
	year = {2015},
	month = oct,
	journal = {Econometric Theory},
	volume = {31},
	number = {5},
	pages = {1102--1116},
	issn = {0266-4666, 1469-4360},
	doi = {10.1017/S0266466614000711},
	urldate = {2024-11-11},
	copyright = {https://www.cambridge.org/core/terms},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Missspecification\Sasaki - 2015 - What Do Quantile Regressions Identify for General Structural Functions.pdf}
}

@article{Horvitz1952GeneralizationSamplingReplacement,
	title = {A {{Generalization}} of {{Sampling Without Replacement}} from a {{Finite Universe}}},
	author = {Horvitz, D. G. and Thompson, D. J.},
	year = {1952},
	month = dec,
	journal = {Journal of the American Statistical Association},
	volume = {47},
	number = {260},
	pages = {663--685},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1952.10483446},
	urldate = {2024-11-28},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Experiments\Horvitz and Thompson - 1952 - A Generalization of Sampling Without Replacement from a Finite Universe.pdf}
}

@article{Cytrynbaum2024CovariateAdjustmentStratified,
	title = {Covariate {{Adjustment}} in {{Stratified Experiments}}},
	author = {Cytrynbaum, Max},
	year = {2024},
	journal = {Quantitative Economics},
	volume = {15},
	number = {4},
	pages = {971--998},
	issn = {1759-7323},
	doi = {10.3982/QE2475},
	urldate = {2024-11-29},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/legalcode},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Experiments\Cytrynbaum - 2024 - Covariate Adjustment in Stratified Experiments.pdf}
}

@article{Donald2014EstimationInferenceDistribution,
	title = {Estimation and {{Inference}} for {{Distribution Functions}} and {{Quantile Functions}} in {{Treatment Effect Models}}},
	author = {Donald, Stephen G. and Hsu, Yu-Chin},
	year = {2014},
	month = jan,
	journal = {Journal of Econometrics},
	volume = {178},
	pages = {383--397},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2013.03.010},
	urldate = {2024-12-17},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Distributional Analysis\Donald and Hsu - 2014 - Estimation and Inference for Distribution Functions and Quantile Functions in Treatment Effect Model.pdf}
}

@book{Molak2023CausalInferenceDiscovery,
	title = {Causal {{Inference}} and {{Discovery}} in {{Python}}},
	shorttitle = {Causal {{Inference}} and {{Discovery}} in {{Python}}},
	author = {Molak, Aleksander},
	year = {2023},
	publisher = {Packt Publishing},
	address = {Birmingham},
	abstract = {Demystify causal inference and casual discovery by uncovering causal principles and merging them with powerful machine learning algorithms for observational and experimental dataPurchase of the print or Kindle book includes a free PDF eBookKey FeaturesExamine Pearlian causal concepts such as structural causal models, interventions, counterfactuals, and moreDiscover modern causal inference techniques for average and heterogenous treatment effect estimationExplore and leverage traditional and modern causal discovery methodsBook DescriptionCausal methods present unique challenges compared to traditional machine learning and statistics. Learning causality can be challenging, but it offers distinct advantages that elude a purely statistical mindset. Causal Inference and Discovery in Python helps you unlock the potential of causality. You'll start with basic motivations behind causal thinking and a comprehensive introduction to Pearlian causal concepts, such as structural causal models, interventions, counterfactuals, and more. Each concept is accompanied by a theoretical explanation and a set of practical exercises with Python code. Next, you'll dive into the world of causal effect estimation, consistently progressing towards modern machine learning methods. Step-by-step, you'll discover Python causal ecosystem and harness the power of cutting-edge algorithms. You'll further explore the mechanics of how "e;causes leave traces"e, and compare the main families of causal discovery algorithms. The final chapter gives you a broad outlook into the future of causal AI where we examine challenges and opportunities and provide you with a comprehensive list of resources to learn more.What you will learnMaster the fundamental concepts of causal inferenceDecipher the mysteries of structural causal modelsUnleash the power of the 4-step causal inference process in PythonExplore advanced uplift modeling techniquesUnlock the secrets of modern causal discovery using PythonUse causal inference for social impact and community benefitWho this book is forThis book is for machine learning engineers, data scientists, and machine learning researchers looking to extend their data science toolkit and explore causal machine learning. It will also help developers familiar with causality who have worked in another technology and want to switch to Python, and data scientists with a history of working with traditional causality who want to learn causal machine learning. It's also a must-read for tech-savvy entrepreneurs looking to build a competitive edge for their products and go beyond the limitations of traditional machine learning},
	isbn = {978-1-80461-298-9 978-1-80461-173-9},
	langid = {english}
}

@book{Hernan2024CausalInferenceWhat,
	title = {Causal {{Inference}}: {{What If}}},
	shorttitle = {Causal Inference},
	author = {Hernan, Miguel A. and Robins, James M.},
	year = {2024},
	edition = {First edition},
	publisher = {{Taylor and Francis}},
	address = {Boca Raton},
	isbn = {978-1-315-37493-2},
	lccn = {Q175.32.I54},
	keywords = {Inference,Longitudinal method}
}

@misc{Savje2021RandomizationDoesNot,
	title = {Randomization {{Does Not Imply Unconfoundedness}}},
	author = {S{\"a}vje, Fredrik},
	year = {2021},
	month = jul,
	number = {arXiv:2107.14197},
	eprint = {2107.14197},
	primaryclass = {stat},
	publisher = {arXiv},
	doi = {10.48550/arXiv.2107.14197},
	urldate = {2024-11-27},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Statistics - Methodology},
	file = {C:\Users\moren\Zotero\storage\NUZZAK8C\Sävje - 2021 - Randomization does not imply unconfoundedness.pdf}
}

@article{White2009SettableSystemsExtension,
	title = {Settable {{Systems}}: {{An Extension}} of {{Pearl}}'s {{Causal Model}} with {{Optimization}}, {{Equilibrium}}, and {{Learning}}},
	author = {White, Halbert and Chalak, Karim},
	year = {2009},
	journal = {Journal of Machine Learning Research},
	volume = {10},
	number = {61},
	pages = {1759-1799},
	doi = {10.5555/1577069.1755844},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\White and Chalak - 2009 - Settable Systems An Extension of Pearl’s Causal Model with Optimization, Equilibrium, and Learning.pdf}
}

@article{Besstremyannaya2019ReconsiderationSimpleApproach,
	title = {Reconsideration of a {{Simple Approach}} to {{Quantile Regression}} for {{Panel Data}}},
	author = {Besstremyannaya, Galina and Golovan, Sergei},
	year = {2019},
	month = sep,
	journal = {The Econometrics Journal},
	volume = {22},
	number = {3},
	pages = {292--308},
	issn = {1368-4221, 1368-423X},
	doi = {10.1093/ectj/utz012},
	urldate = {2024-11-06},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Panel\Besstremyannaya and Golovan - 2019 - Reconsideration of a Simple Approach to Quantile Regression for Panel Data.pdf}
}
@article{Machado2005CounterfactualDecompositionChanges,
	title = {Counterfactual {{Decomposition}} of {{Changes}} in {{Wage Distributions Using Quantile Regression}}},
	author = {Machado, Jos{\'e} A. F. and Mata, Jos{\'e}},
	year = {2005},
	month = may,
	journal = {Journal of Applied Econometrics},
	volume = {20},
	number = {4},
	pages = {445--465},
	issn = {0883-7252, 1099-1255},
	doi = {10.1002/jae.788},
	urldate = {2024-11-13},
	abstract = {We propose a method to decompose the changes in the wage distribution over a period of time in several factors contributing to those changes. The method is based on the estimation of marginal wage distributions consistent with a conditional distribution estimated by quantile regression as well as with any hypothesized distribution for the covariates. Comparing the marginal distributions implied by different distributions for the covariates, one is then able to perform counterfactual exercises. The proposed methodology enables the identification of the sources of the increased wage inequality observed in most countries. Specifically, it decomposes the changes in the wage distribution over a period of time into several factors contributing to those changes, namely by discriminating between changes in the characteristics of the working population and changes in the returns to these characteristics. We apply this methodology to Portuguese data for the period 1986--1995, and find that the observed increase in educational levels contributed decisively towards greater wage inequality. Copyright  2005 John Wiley \& Sons, Ltd.},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Machado and Mata - 2005 - Counterfactual Decomposition of Changes in Wage Distributions Using Quantile Regression.pdf}
}

@article{Wuthrich2020ComparisonTwoQuantile,
	title = {A {{Comparison}} of {{Two Quantile Models With Endogeneity}}},
	author = {W{\"u}thrich, Kaspar},
	year = {2020},
	month = apr,
	journal = {Journal of Business \& Economic Statistics},
	volume = {38},
	number = {2},
	pages = {443--456},
	issn = {0735-0015, 1537-2707},
	doi = {10.1080/07350015.2018.1514307},
	urldate = {2024-11-12},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\IV\Wüthrich - 2020 - A Comparison of Two Quantile Models With Endogeneity.pdf}
}
@article{Pereda-Fernandez2024EstimationCounterfactualDistributions,
	title = {Estimation of {{Counterfactual Distributions With}} a {{Continuous Endogenous Treatment}}},
	author = {{Pereda-Fern{\'a}ndez}, Santiago},
	year = {2024},
	month = sep,
	journal = {Econometric Reviews},
	volume = {43},
	number = {8},
	pages = {595--637},
	issn = {0747-4938, 1532-4168},
	doi = {10.1080/07474938.2024.2357429},
	urldate = {2024-11-12},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Distribution Regression\Pereda-Fernández - 2024 - Estimation of counterfactual distributions with a continuous endogenous treatment.pdf}
}

@article{Frolich2010EstimationQuantileTreatment,
	title = {Estimation of {{Quantile Treatment Effects}} with {{Stata}}},
	author = {Fr{\"o}lich, Markus and Melly, Blaise},
	year = {2010},
	month = sep,
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	volume = {10},
	number = {3},
	pages = {423--457},
	issn = {1536-867X, 1536-8734},
	doi = {10.1177/1536867X1001000309},
	urldate = {2024-11-06},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fröolich and Melly - 2010 - Estimation of Quantile Treatment Effects with Stata.pdf}
}

@misc{Hosseini2010QuantilesEquivariance,
	title = {Quantiles {{Equivariance}}},
	author = {Hosseini, Reza},
	year = {2010},
	month = apr,
	number = {arXiv:1004.0533},
	eprint = {1004.0533},
	primaryclass = {math},
	publisher = {arXiv},
	urldate = {2024-11-07},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Statistics Theory},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Hosseini - 2010 - Quantiles Equivariance.pdf}
}

@article{Chernozhukov2010QuantileProbabilityCurves,
	title = {Quantile and {{Probability Curves Without Crossing}}},
	author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Galichon, Alfred},
	year = {2010},
	journal = {Econometrica},
	volume = {78},
	number = {3},
	pages = {1093--1125},
	issn = {0012-9682},
	doi = {10.3982/ECTA7880},
	urldate = {2024-11-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Chernozhukov et al. - 2010 - Quantile and Probability Curves Without Crossing.pdf}
}

@article{Chernozhukov2013QuantileModelsEndogeneity,
	title = {Quantile {{Models}} with {{Endogeneity}}},
	author = {Chernozhukov, V. and Hansen, C.},
	year = {2013},
	month = aug,
	journal = {Annual Review of Economics},
	volume = {5},
	number = {1},
	pages = {57--81},
	issn = {1941-1383, 1941-1391},
	doi = {10.1146/annurev-economics-080511-110952},
	urldate = {2024-10-31},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\IV\Chernozhukov and Hansen - 2013 - Quantile Models with Endogeneity.pdf}
}

@article{Chernozhukov2005IVModelQuantile,
	title = {An {{IV Model}} of {{Quantile Treatment Effects}}},
	author = {Chernozhukov, Victor and Hansen, Christian},
	year = {2005},
	month = jan,
	journal = {Econometrica},
	volume = {73},
	number = {1},
	pages = {245--261},
	issn = {0012-9682, 1468-0262},
	doi = {10.1111/j.1468-0262.2005.00570.x},
	urldate = {2024-10-31},
	abstract = {The ability of quantile regression models to characterize the heterogeneous impact of variables on different points of an outcome distribution makes them appealing in many economic applications. However, in observational studies, the variables of interest (e.g., education, prices) are often endogenous, making conventional quantile regression inconsistent and hence inappropriate for recovering the causal effects of these variables on the quantiles of economic outcomes. In order to address this problem, we develop a model of quantile treatment effects (QTE) in the presence of endogeneity and obtain conditions for identification of the QTE without functional form assumptions. The principal feature of the model is the imposition of conditions that restrict the evolution of ranks across treatment states. This feature allows us to overcome the endogeneity problem and recover the true QTE through the use of instrumental variables. The proposed model can also be equivalently viewed as a structural simultaneous equation model with nonadditive errors, where QTE can be interpreted as the structural quantile effects (SQE).},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\IV\Chernozhukov and Hansen - 2005 - An IV Model of Quantile Treatment Effects.pdf}
}

@article{Foresi1995ConditionalDistributionExcess,
	title = {The {{Conditional Distribution}} of {{Excess Returns}}: {{An Empirical Analysis}}},
	shorttitle = {The {{Conditional Distribution}} of {{Excess Returns}}},
	author = {Foresi, Silverio and Peracchi, Franco},
	year = {1995},
	month = jun,
	journal = {Journal of the American Statistical Association},
	volume = {90},
	number = {430},
	pages = {451--466},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.1995.10476537},
	urldate = {2024-10-31},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Distribution Regression\Foresi and Peracchi - 1995 - The Conditional Distribution of Excess Returns An Empirical Analysis.pdf}
}

@article{Rothe2013MisspecificationTestingClass,
	title = {Misspecification {{Testing}} in a {{Class}} of {{Conditional Distributional Models}}},
	author = {Rothe, Christoph and Wied, Dominik},
	year = {2013},
	month = mar,
	journal = {Journal of the American Statistical Association},
	volume = {108},
	number = {501},
	pages = {314--324},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2012.736903},
	urldate = {2024-10-31},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Rothe and Wied - 2013 - Misspecification Testing in a Class of Conditional Distributional Models.pdf}
}

@article{Angrist2006QuantileRegressionMisspecification,
	title = {Quantile {{Regression}} under {{Misspecification}}, with an {{Application}} to the {{U}}.{{S}}. {{Wage Structure}}},
	author = {Angrist, Joshua and Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n},
	year = {2006},
	month = mar,
	journal = {Econometrica},
	volume = {74},
	number = {2},
	pages = {539--563},
	issn = {0012-9682, 1468-0262},
	doi = {10.1111/j.1468-0262.2006.00671.x},
	urldate = {2024-10-30},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Angrist et al. - 2006 - Quantile Regression under Misspecification, with an Application to the U.S. Wage Structure.pdf}
}
@book{Koenker2017HandbookQuantileRegression,
	title = {Handbook of {{Quantile Regression}}},
	editor = {Koenker, Roger and Chernozhukov, Victor and He, Xuming and Peng, Limin},
	year = {2017},
	month = oct,
	edition = {1},
	publisher = {{Chapman and Hall/CRC}},
	doi = {10.1201/9781315120256},
	urldate = {2024-11-08},
	isbn = {978-1-315-12025-6},
	langid = {english},
	file = {C:\Users\moren\Zotero\storage\YFUN4ME9\Koenker et al. - 2017 - Handbook of Quantile Regression.pdf}
}

@article{Kato2017UsingLinearQuantile,
	title = {On {{Using Linear Quantile Regressions}} for {{Causal Inference}}},
	author = {Kato, Ryutah and Sasaki, Yuya},
	year = {2017},
	month = jun,
	journal = {Econometric Theory},
	volume = {33},
	number = {3},
	pages = {664--690},
	issn = {0266-4666, 1469-4360},
	doi = {10.1017/S0266466616000177},
	urldate = {2024-10-30},
	copyright = {https://www.cambridge.org/core/terms},
	langid = {english},
	file = {C:\Users\moren\Zotero\storage\6UEQB2UE\Kato and Sasaki - 2017 - ON USING LINEAR QUANTILE REGRESSIONS FOR CAUSAL INFERENCE.pdf}
}
@article{Doksum1974EmpiricalProbabilityPlots,
	title = {Empirical {{Probability Plots}} and {{Statistical Inference}} for {{Nonlinear Models}} in the {{Two-Sample Case}}},
	author = {Doksum, Kjell},
	year = {1974},
	month = mar,
	journal = {The Annals of Statistics},
	volume = {2},
	number = {2},
	issn = {0090-5364},
	doi = {10.1214/aos/1176342662},
	urldate = {2024-11-11},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Fixed Quantile Regression\Doksum - 1974 - Empirical Probability Plots and Statistical Inference for Nonlinear Models in the Two-Sample Case.pdf}
}

@article{Drees1998SelectingOptimalSample,
	title = {Selecting the {{Optimal Sample Fraction}} in {{Univariate Extreme Value Estimation}}},
	author = {Drees, Holger and Kaufmann, Edgar},
	year = {1998},
	month = jul,
	journal = {Stochastic Processes and their Applications},
	volume = {75},
	number = {2},
	pages = {149--172},
	issn = {03044149},
	doi = {10.1016/S0304-4149(98)00017-9},
	urldate = {2024-10-15},
	abstract = {In general, estimators of the extreme value index of i.i.d. random variables crucially depend on the sample fraction that is used for estimation. In case of the well-known Hill estimator the optimal number knopt of largest order statistics was given by Hall and Welsh (1985) as a function of some parameters of the unknown distribution function F, which was assumed to admit a certain expansion. Moreover, an estimator of knopt was proposed that is consistent if a second-order parameter   of F belongs to a bounded interval. In contrast, we introduce a sequential procedure that yields a consistent estimator of knopt in the full model without requiring prior information about  . Then it is demonstrated that even in a more general setup the resulting adaptive Hill estimator is asymptotically as e cient as the Hill estimator based on the optimal number of order statistics. Finally, it is shown by Monte Carlo simulations that also for moderate sample sizes the procedure shows a reasonable performance, which can be improved further if   is restricted to bounded intervals. c{\copyright} 1998 Elsevier Science B.V. All rights reserved.},
	langid = {english},
	file = {D:\Side Drives\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Tail Index Estimation\Drees and Kaufmann - 1998 - Selecting the Optimal Sample Fraction in Univariate Extreme Value Estimation.pdf}
}

@article{Bickel2008ChoiceOutBootstrapa,
	title = {On the {{Choice}} of m in the m {{Out}} of n {{Bootstrap}} and {{Confidence Bounds}} for {{Extrema}}},
	author = {Bickel, Peter J and Sakov, Anat},
	year = {2008},
	journal = {Statistica Sinica},
	volume = {18},
	number = {3},
	pages = {967--985},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Bootstrap and Subsampling\Bickel and Sakov - 2008 - On the Choice of m in the m Out of n Bootstrap and Confidence Bounds for Extrema.pdf}
}

@incollection{Caeiro2016ThresholdSelectionExtreme,
	title = {Threshold {{Selection}} in {{Extreme Value Analysis}}},
	booktitle = {Extreme {{Value Modeling}} and {{Risk Analysis}}},
	author = {Caeiro, Frederico and Gomes, M Ivette},
	year = {2016},
	pages = {69--85},
	publisher = {{Chapman and Hall/CRC}},
	address = {New York},
	isbn = {978-0-429-16119-3},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Bootstrap and Subsampling\Caeiro and Gomes - 2016 - Threshold Selection in Extreme Value Analysis.pdf}
}

@article{Caers1999StatisticsModelingHeavy,
	title = {Statistics for {{Modeling Heavy Tailed Distributions}} in {{Geology}}: {{Part I}}. {{Methodology}}},
	author = {Caers, Jef and Beirlant, Jan and Maes, Marc A.},
	year = {1999},
	journal = {Mathematical Geology},
	volume = {31},
	number = {4},
	pages = {391--410},
	issn = {08828121},
	doi = {10.1023/A:1007538624271},
	urldate = {2024-10-22},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Bootstrap and Subsampling\Caers et al. - 1999 - Statistics for Modeling Heavy Tailed Distributions in Geology Part I. Methodology.pdf}
}

@article{Romano2001SubsamplingIntervalsAutoregressive,
	title = {Subsampling {{Intervals}} in {{Autoregressive Models}} with {{Linear Time Trend}}},
	author = {Romano, Joseph P. and Wolf, Michael},
	year = {2001},
	month = sep,
	journal = {Econometrica},
	volume = {69},
	number = {5},
	pages = {1283--1314},
	issn = {0012-9682, 1468-0262},
	doi = {10.1111/1468-0262.00242},
	urldate = {2024-10-15},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	langid = {english},
	file = {D:\Side Drives\Econometrics Cloud\Articles\Resampling\Subsampling\Romano and Wolf - 2001 - Subsampling Intervals in Autoregressive Models with Linear Time Trend.pdf}
}
@article{Gomes2001BootstrapMethodologyStatistics,
	title = {The {{Bootstrap Methodology}} in {{Statistics}} of {{Extremes}}---{{Choice}} of the {{Optimal Sample Fraction}}},
	author = {Gomes, M. Ivette and Oliveira, Orlando},
	year = {2001},
	journal = {Extremes},
	volume = {4},
	number = {4},
	pages = {331--358},
	issn = {13861999},
	doi = {10.1023/A:1016592028871},
	urldate = {2024-10-15},
	file = {D:\Side Drives\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Tail Index Estimation\Gomes and Oliveira - 2001 - The Bootstrap Methodology in Statistics of Extremes—Choice of the Optimal Sample Fraction.pdf}
}

@article{Hall1990UsingBootstrapEstimate,
	title = {Using the {{Bootstrap}} to {{Estimate Mean Squared Error}} and {{Select Smoothing Parameter}} in {{Nonparametric Problems}}},
	author = {Hall, Peter},
	year = {1990},
	month = feb,
	journal = {Journal of Multivariate Analysis},
	volume = {32},
	number = {2},
	pages = {177--203},
	issn = {0047259X},
	doi = {10.1016/0047-259X(90)90080-2},
	urldate = {2024-10-15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	langid = {english},
	file = {D:\Side Drives\Econometrics Cloud\Articles\Resampling\Bootstrap\Hall - 1990 - Using the Bootstrap to Estimate Mean Squared Error and Select Smoothing Parameter in Nonparametric P.pdf}
}
	
@article{Sasaki2024UniformConfidenceIntervals,
	title = {On {{Uniform Confidence Intervals}} for the {{Tail Index}} and the {{Extreme Quantile}}},
	author = {Sasaki, Yuya and Wang, Yulong},
	year = {2024},
	month = aug,
	journal = {Journal of Econometrics},
	volume = {244},
	number = {1},
	pages = {105865},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2024.105865},
	urldate = {2024-10-11},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Extreme Value\Sasaki and Wang - 2024 - On Uniform Confidence Intervals for the Tail Index and the Extreme Quantile.pdf}
}

@article{Bonhomme2010GeneralizedNonParametricDeconvolution,
	title = {Generalized {{Non-Parametric Deconvolution}} with an {{Application}} to {{Earnings Dynamics}}},
	author = {Bonhomme, St{\'e}phane and Robin, Jean-Marc},
	year = {2010},
	month = apr,
	journal = {Review of Economic Studies},
	volume = {77},
	number = {2},
	pages = {491--533},
	issn = {00346527, 1467937X},
	doi = {10.1111/j.1467-937X.2009.00577.x},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Factor Models\Nonparametric\Bonhomme and Robin - 2010 - Generalized Non-Parametric Deconvolution with an Application to Earnings Dynamics.pdf}
}
@misc{Armstrong2023RobustEstimationInference,
	title = {Robust {{Estimation}} and {{Inference}} in {{Panels}} with {{Interactive Fixed Effects}}},
	author = {Armstrong, Timothy B. and Weidner, Martin and Zeleneev, Andrei},
	year = {2023},
	month = jul,
	number = {arXiv:2210.06639},
	eprint = {2210.06639},
	primaryclass = {econ, stat},
	publisher = {arXiv},
	urldate = {2024-10-09},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Economics - Econometrics,Statistics - Methodology},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Factor Models\Weak factors\Armstrong et al. - 2023 - Robust Estimation and Inference in Panels with Interactive Fixed Effects.pdf}
}

@article{DeChaisemartin2020TwoWayFixedEffects,
	title = {Two-{{Way Fixed Effects Estimators}} with {{Heterogeneous Treatment Effects}}},
	author = {De Chaisemartin, Cl{\'e}ment and D'Haultf{\oe}uille, Xavier},
	year = {2020},
	month = sep,
	journal = {American Economic Review},
	volume = {110},
	number = {9},
	pages = {2964--2996},
	issn = {0002-8282},
	doi = {10.1257/aer.20181169},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Regression\Two-Way FE\De Chaisemartin and D’Haultfœuille - 2020 - Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.pdf}
}
@article{Torgovitsky2015IdentificationNonseparableModels,
	title = {Identification of {{Nonseparable Models Using Instruments With Small Support}}: {{Identification}} of {{Nonseparable Models}}},
	shorttitle = {Identification of {{Nonseparable Models Using Instruments With Small Support}}},
	author = {Torgovitsky, Alexander},
	year = {2015},
	month = may,
	journal = {Econometrica},
	volume = {83},
	number = {3},
	pages = {1185--1197},
	issn = {00129682},
	doi = {10.3982/ECTA9984},
	urldate = {2024-10-08},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametric Analysis\Nonparametric IV\Torgovitsky - 2015 - Identification of Nonseparable Models Using Instruments With Small Support Identification of Nonsep.pdf}
}
@article{Chernozhukov2018SortedEffectsMethod,
	title = {The {{Sorted Effects Method}}: {{Discovering Heterogeneous Effects Beyond Their Averages}}},
	author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Luo, Ye},
	year = {2018},
	journal = {Econometrica},
	volume = {86},
	number = {6},
	pages = {1911--1938},
	issn = {0012-9682},
	doi = {10.3982/ECTA14415},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Distributional Analysis\Chernozhukov et al. - 2018 - The Sorted Effects Method Discovering Heterogeneous Effects Beyond Their Averages.pdf}
}


@article{DeChaisemartin2023TwowayFixedEffects,
	title = {Two-Way {{Fixed Effects}} and {{Differences-in-differences Estimators With Several Treatments}}},
	author = {De Chaisemartin, Cl{\'e}ment and D'Haultf{\oe}uille, Xavier},
	year = {2023},
	month = oct,
	journal = {Journal of Econometrics},
	volume = {236},
	number = {2},
	pages = {105480},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2023.105480},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Regression\Two-Way FE\De Chaisemartin and D’Haultfœuille - 2023 - Two-way Fixed Effects and Differences-in-differences Estimators With Several Treatments.pdf}
}

@article{Chernozhukov2020SemiparametricEstimationStructural,
	title = {Semiparametric {{Estimation}} of {{Structural Functions}} in {{Nonseparable Triangular Models}}},
	author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Newey, Whitney and Stouli, Sami and Vella, Francis},
	year = {2020},
	journal = {Quantitative Economics},
	volume = {11},
	number = {2},
	pages = {503--533},
	issn = {1759-7323},
	doi = {10.3982/QE1239},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\Zotero\storage\6HZ3NQN8\Chernozhukov et al. - 2020 - Semiparametric estimation of structural functions in nonseparable triangular models.pdf}
}
@article{Canay2011SimpleApproachQuantile,
	title = {A {{Simple Approach}} to {{Quantile Regression}} for {{Panel Data}}},
	author = {Canay, Ivan A.},
	year = {2011},
	month = oct,
	journal = {The Econometrics Journal},
	volume = {14},
	number = {3},
	pages = {368--386},
	issn = {1368-4221, 1368-423X},
	doi = {10.1111/j.1368-423X.2011.00349.x},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Panel QR\Canay - 2011 - A Simple Approach to Quantile Regression for Panel Data.pdf}
}

@article{Chernozhukov2024NetworkPanelQuantile,
	title = {Network and {{Panel Quantile Effects Via Distribution Regression}}},
	author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Weidner, Martin},
	year = {2024},
	journal = {Journal of Econometrics},
	volume = {240},
	number = {2},
	eprint = {1803.08154},
	primaryclass = {econ, stat},
	doi = {10.1016/j.jeconom.2020.08.009},
	urldate = {2024-10-08},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Economics - Econometrics,Statistics - Methodology},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Quantile and Distribution Regression\Distribution Regression\Chernozhukov et al. - 2024 - Network and Panel Quantile Effects Via Distribution Regression.pdf}
}

@article{Bonhomme2022DiscretizingUnobservedHeterogeneity,
	title = {Discretizing {{Unobserved Heterogeneity}}},
	author = {Bonhomme, St{\'e}phane and Lamadon, Thibaut and Manresa, Elena},
	year = {2022},
	journal = {Econometrica},
	volume = {90},
	number = {2},
	pages = {625--643},
	issn = {0012-9682},
	doi = {10.3982/ECTA15238},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Linear Heterogeneous Panels\Grouped Panels\Bonhomme et al. - 2022 - Discretizing Unobserved Heterogeneity.pdf}
}

@article{Kato2021RobustInferenceDeconvolutiona,
	title = {Robust {{Inference}} in {{Deconvolution}}},
	author = {Kato, Kengo and Sasaki, Yuya and Ura, Takuya},
	year = {2021},
	journal = {Quantitative Economics},
	volume = {12},
	number = {1},
	pages = {109--142},
	issn = {1759-7323},
	doi = {10.3982/QE1643},
	urldate = {2024-10-08},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametrics\Distribution Estimation\Deconvolution\Kato et al. - 2021 - Robust Inference in Deconvolution.pdf}
}
@article{Lumsdaine2023EstimationPanelGroup,
	title = {Estimation of {{Panel Group Structure Models With Structural Breaks}} in {{Group Memberships}} and {{Coefficients}}},
	author = {Lumsdaine, Robin L. and Okui, Ryo and Wang, Wendun},
	year = {2023},
	month = mar,
	journal = {Journal of Econometrics},
	volume = {233},
	number = {1},
	pages = {45--65},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2022.01.001},
	urldate = {2024-10-08},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Linear Heterogeneous Panels\Grouped Panels\Lumsdaine et al. - 2023 - Estimation of Panel Group Structure Models With Structural Breaks in Group Memberships and Coefficie.pdf}
}

@article{Bonhomme2009AssessingEqualizingForce,
	title = {Assessing the {{Equalizing Force}} of {{Mobility Using Short Panels}}: {{France}}, 1990--2000},
	author = {Bonhomme, St{\'e}phane and Robin, Jean-Marc},
	year = {2009},
	journal = {The Review of Economic Studies},
	volume = {76},
	number = {1},
	pages = {63--92},
	doi = {10.1111/j.1467-937X.2008.00521.x},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Labor\Earning Dynamics\Bonhomme and Robin - 2009 - Assessing the Equalizing Force of Mobility Using Short Panels France, 1990–2000.pdf}
}

@article{Mundlak1961EmpiricalProductionFunction,
	title = {Empirical {{Production Function Free}} of {{Management Bias}}},
	author = {Mundlak, Yair},
	year = {1961},
	month = feb,
	journal = {Journal of Farm Economics},
	volume = {43},
	number = {1},
	pages = {44},
	issn = {10711031},
	doi = {10.2307/1235460},
	urldate = {2024-10-07},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Mundlak - 1961 - Empirical Production Function Free of Management Bias.pdf}
}
@book{Angrist2009MostlyHarmlessEconometrics,
	title = {Mostly {{Harmless Econometrics}}},
	author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
	year = {2009},
	series = {An {{Empiricist}}'s {{Companion}}},
	publisher = {Princeton University Press},
	doi = {10.1515/9781400829828},
	urldate = {2024-10-07},
	isbn = {978-1-4008-2982-8}
}
@book{Brockwell2016IntroductionTimeSeries,
	title = {Introduction to {{Time Series}} and {{Forecasting}}},
	author = {Brockwell, Peter J. and Davis, Richard A.},
	year = {2016},
	series = {Springer {{Texts}} in {{Statistics}}},
	publisher = {Springer International Publishing},
	isbn = {978-3-319-29854-2}
}

@book{Hsiao2022AnalysisPanelData,
	title = {Analysis of {{Panel Data}}},
	author = {Hsiao, Cheng},
	year = {2022},
	series = {Econometric {{Society Monographs}}},
	edition = {4},
	publisher = {Cambridge University Press},
	address = {Cambridge},
	doi = {10.1017/9781009057745},
	isbn = {978-1-316-51210-4}
}

@article{Lewbel2019IdentificationZooMeanings,
	title = {The {{Identification Zoo}}: {{Meanings}} of {{Identification}} in {{Econometrics}}},
	shorttitle = {The {{Identification Zoo}}},
	author = {Lewbel, Arthur},
	year = {2019},
	month = dec,
	journal = {Journal of Economic Literature},
	volume = {57},
	number = {4},
	pages = {835--903},
	issn = {0022-0515},
	doi = {10.1257/jel.20181361},
	urldate = {2024-10-06},
	langid = {english},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Review Articles\Lewbel - 2019 - The Identification Zoo Meanings of Identification in Econometrics.pdf}
}

@article{Engel1857ProductionsundConsumtionsverhaltnisseKonigreichs,
	title = {Die {{Productions-und Consumtionsverh{\"a}ltnisse}} Des {{K{\"o}nigreichs Sachsen}}},
	author = {Engel, Ernst},
	year = {1857},
	journal = {Zeitschrift des Statistischen B{\"u}reaus des K{\"o}niglich S{\"a}chsischen Ministeriums des Innern},
	number = {7, 8},
	pages = {1--54}
}

@article{Lewbel2017UnobservedPreferenceHeterogeneity,
	title = {Unobserved {Preference} {Heterogeneity} in {Demand} {Using} {Generalized} {Random} {Coefficients}},
	volume = {125},
	issn = {0022-3808, 1537-534X},
	url = {https://www.journals.uchicago.edu/doi/10.1086/692808},
	doi = {10.1086/692808},
	language = {en},
	number = {4},
	journal = {Journal of Political Economy},
	author = {Lewbel, Arthur and Pendakur, Krishna},
	month = aug,
	year = {2017},
	pages = {1100--1148},
	file = {Lewbel and Pendakur (2017) Unobserved Preference Heterogeneity in Demand Using Generalized Random Coefficients.pdf:/home/vladislav/Insync/morenie@gmail.com/Google Drive/Econometrics Cloud/Articles/Applied (By Field)/Consumption/Lewbel and Pendakur (2017) Unobserved Preference Heterogeneity in Demand Using Generalized Random Coefficients.pdf:application/pdf},
}

@incollection{Henderson2024NonparametricModelsFixed,
	address = {Cham},
	title = {Nonparametric {Models} with {Fixed} {Effects}},
	isbn = {978-3-031-49849-7},
	language = {en},
	booktitle = {The {Econometrics} of {Multi}-dimensional {Panels}: {Theory} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Henderson, Daniel J. and Soberon, Alexandra},
	editor = {Matyas, Laszlo},
	year = {2024},
	doi = {10.1007/978-3-031-49849-7_9},
	pages = {285--323},
}

@article{Boneva2015SemiparametricModelHeterogeneous,
	series = {Heterogeneity in {Panel} {Data} and in {Nonparametric} {Analysis} in honor of {Professor} {Cheng} {Hsiao}},
	title = {A semiparametric model for heterogeneous panel data with fixed effects},
	volume = {188},
	issn = {0304-4076},
	doi = {10.1016/j.jeconom.2015.03.003},
	abstract = {This paper develops methodology for semiparametric panel data models in a setting where both the time series and the cross section are large. Such settings are common in finance and other areas of economics. Our model allows for heterogeneous nonparametric covariate effects as well as unobserved time and individual specific effects that may depend on the covariates in an arbitrary way. To model the covariate effects parsimoniously, we impose a dimensionality reducing common component structure on them. In the theoretical part of the paper, we derive the asymptotic theory for the proposed procedure. In particular, we provide the convergence rates and the asymptotic distribution of our estimators. In the empirical part, we apply our methodology to a specific application that has been the subject of recent policy interest, that is, the effect of trading venue fragmentation on market quality. We use a unique dataset that reports the location and volume of trading on the FTSE 100 and FTSE 250 companies from 2008 to 2011 at the weekly frequency. We find that the effect of fragmentation on market quality is nonlinear and non-monotonic. The implied quality of the market under perfect competition is superior to that under monopoly provision, but the transition between the two is complicated.},
	number = {2},
	journal = {Journal of Econometrics},
	author = {Boneva, Lena and Linton, Oliver and Vogt, Michael},
	month = oct,
	year = {2015},
	keywords = {Factor structure, Heterogeneous panel data, Kernel smoothing, Semiparametric estimation},
	pages = {327--345},
	file = {ScienceDirect Full Text PDF:/home/vladislav/Zotero/storage/HKDWZZY3/Boneva et al. (2015) A semiparametric model for heterogeneous panel data with fixed effects.pdf:application/pdf},
}

@article{Lee2015PanelNonparametricRegression,
	series = {Heterogeneity in {Panel} {Data} and in {Nonparametric} {Analysis} in honor of {Professor} {Cheng} {Hsiao}},
	title = {Panel nonparametric regression with fixed effects},
	volume = {188},
	issn = {0304-4076},
	doi = {10.1016/j.jeconom.2015.03.004},
	abstract = {Nonparametric regression is developed for data with both a temporal and a cross-sectional dimension. The model includes additive, unknown, individual-specific components and allows also for cross-sectional and temporal dependence and conditional heteroscedasticity. A simple nonparametric estimate is shown to be dominated by a GLS-type one. Asymptotically optimal bandwidth choices are justified for both estimates. Feasible optimal bandwidths, and feasible optimal regression estimates, are also asymptotically justified. Finite sample performance is examined in a Monte Carlo study.},
	number = {2},
	journal = {Journal of Econometrics},
	author = {Lee, Jungyoon and Robinson, Peter M.},
	month = oct,
	year = {2015},
	keywords = {Cross-sectional dependence, Generalized least squares, Nonparametric regression, Optimal bandwidth, Panel data},
	pages = {346--362},
	file = {ScienceDirect Full Text PDF:/home/vladislav/Zotero/storage/6IHNDVB2/Lee and Robinson (2015) Panel nonparametric regression with fixed effects.pdf:application/pdf},
}

@article{Henderson2008NonparametricEstimationTesting,
	title = {Nonparametric estimation and testing of fixed effects panel data models},
	volume = {144},
	issn = {0304-4076},
	doi = {10.1016/j.jeconom.2008.01.005},
	abstract = {In this paper we consider the problem of estimating nonparametric panel data models with fixed effects. We introduce an iterative nonparametric kernel estimator. We also extend the estimation method to the case of a semiparametric partially linear fixed effects model. To determine whether a parametric, semiparametric or nonparametric model is appropriate, we propose test statistics to test between the three alternatives in practice. We further propose a test statistic for testing the null hypothesis of random effects against fixed effects in a nonparametric panel data regression model. Simulations are used to examine the finite sample performance of the proposed estimators and the test statistics.},
	number = {1},
	journal = {Journal of Econometrics},
	author = {Henderson, Daniel J. and Carroll, Raymond J. and Li, Qi},
	month = may,
	year = {2008},
	keywords = {Fixed effects models, Model specification tests, Nonparametric kernel method, Panel data, Partially linear model, Profile method, Random effects models, Semiparametric efficiency bound},
	pages = {257--275},
	file = {ScienceDirect Full Text PDF:/home/vladislav/Zotero/storage/6A56YW9R/Henderson et al. (2008) Nonparametric estimation and testing of fixed effects panel data models.pdf:application/pdf;ScienceDirect Snapshot:/home/vladislav/Zotero/storage/G3C7GRMB/S030440760800016X.html:text/html},
}

@article{Beckert2008HeterogeneityNonParametricAnalysis,
	title = {Heterogeneity and the {Non}-{Parametric} {Analysis} of {Consumer} {Choice}: {Conditions} for {Invertibility}},
	volume = {75},
	issn = {0034-6527},
	shorttitle = {Heterogeneity and the {Non}-{Parametric} {Analysis} of {Consumer} {Choice}},
	doi = {10.1111/j.1467-937X.2008.00500.x},
	number = {4},
	journal = {The Review of Economic Studies},
	author = {Beckert, Walter and Blundell, Richard},
	year = {2008},
	pages = {1069--1080},
	file = {Full Text PDF:/home/vladislav/Zotero/storage/Z5EPYY9K/Beckert and Blundell (2008) Heterogeneity and the Non-Parametric Analysis of Consumer Choice Conditions for Invertibility.pdf:application/pdf;Snapshot:/home/vladislav/Zotero/storage/CCAZZYD2/1560361.html:text/html},
}

@article{Freyberger2018NonparametricPanelData,
	title = {Non-parametric {Panel} {Data} {Models} with {Interactive} {Fixed} {Effects}},
	volume = {85},
	issn = {0034-6527},
	doi = {10.1093/restud/rdx052},
	number = {3},
	journal = {The Review of Economic Studies},
	author = {Freyberger, Joachim},
	month = jul,
	year = {2018},
	pages = {1824--1851},
}

@article{Koenker1978RegressionQuantiles,
	title = {Regression {Quantiles}},
	volume = {46},
	issn = {0012-9682},
	doi = {10.2307/1913643},
	number = {1},
	urldate = {2024-09-11},
	journal = {Econometrica},
	author = {Koenker, Roger and Bassett, Gilbert},
	year = {1978},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {33--50},
}

@article{Koenker2017QuantileRegression40,
	title = {Quantile {Regression}: 40 {Years} {On}},
	volume = {9},
	doi = {10.1146/annurev-economics-063016-103651},
	language = {en},
	journal = {Annual Review of Economics},
	author = {Koenker, Roger},
	year = {2017},
	pages = {155--176},
	file = {Koenker (Quantile Regression 40 Years On.pdf:/home/vladislav/Insync/morenie@gmail.com/Google Drive/Econometrics Cloud/Articles/Quantile/Fixed Quantile Regression/Koenker (2017) Quantile Regression 40 Years On.pdf:application/pdf},
}

@book{Hansen2022Econometrics,
  title = {Econometrics},
  author = {Hansen, Bruce},
  year = {2022},
  publisher = {Princeton\_University\_Press},
  isbn = {978-0-691-23589-9}
}

@article{Sury2011SelectionComparativeAdvantage,
  title = {Selection and {{Comparative Advantage}} in {{Technology Adoption}}},
  author = {Sury, Tavneet},
  year = {2011},
  journal = {Econometrica},
  volume = {79},
  number = {1},
  pages = {159--209},
  issn = {0012-9682},
  doi = {10.3982/ecta7749},
  abstract = {This paper investigates an empirical puzzle in technology adoption for developing countries: the low adoption rates of technologies like hybrid maize that increase average farm profits dramatically. I offer a simple explanation for this: benefits and costs of technologies are heterogeneous, so that farmers with low net returns do not adopt the technology. I examine this hypothesis by estimating a correlated random coefficient model of yields and the corresponding distribution of returns to hybrid maize. This distribution indicates that the group of farmers with the highest estimated gross returns does not use hybrid, but their returns are correlated with high costs of acquiring the technology (due to poor infrastructure). Another group of farmers has lower returns and adopts, while the marginal farmers have zero returns and switch in and out of use over the sample period. Overall, adoption decisions appear to be rational and well explained by (observed and unobserved) variation in heterogeneous net benefits to the technology.},
  file = {C:\Users\moren\Downloads\Econometrica - 2011 - Suri - Selection and Comparative Advantage in Technology Adoption.pdf}
}

@book{Rieder1994RobustAsymptoticStatistics,
  title = {Robust {{Asymptotic Statistics}}},
  author = {Rieder, Helmut},
  year = {1994},
  publisher = {Springer New York},
  doi = {978-1-4684-0626-9},
  isbn = {978-1-4684-0624-5}
}

@book{Schilling2012BrownianMotionIntroduction,
  title = {Brownian {{Motion}}: {{An Introduction}} to {{Stochastic Processes}}},
  author = {Schilling, Ren{\'e} L. and Partzsch, Lothar},
  year = {2012},
  publisher = {De Gruyter},
  doi = {10.1515/9783110278989},
  isbn = {978-3-11-027898-9}
}

@article{Chen2019InferenceFunctionalsFirst,
  title = {Inference on {{Functionals}} under {{First Order Degeneracy}}},
  author = {Chen, Qihui and Fang, Zheng},
  year = {2019},
  journal = {Journal of Econometrics},
  volume = {210},
  number = {2},
  pages = {459--481},
  publisher = {Elsevier B.V.},
  issn = {18726895},
  doi = {10.1016/j.jeconom.2019.01.011},
  abstract = {This paper presents a unified framework for inference on parameters of the form {$\phi$}({\th}eta 0 ), where {\th}eta 0 is unknown but can be estimated by {\th}eta{\textasciicircum} n , and {$\phi$} is known with null first order derivative at {\th}eta 0 . We show the ``standard'' bootstrap is consistent if and only if the second order derivative {$\phi$} {\th}eta 0{$\prime\prime$} =0 under regularity conditions, thereby identifying a source of bootstrap failures distinct from that in Fang and Santos (2018). Two consistent bootstrap schemes are proposed: one based on Babu (1984)that applies to differentiable maps and the other one based on Fang and Santos (2018)that applies to (second order)nondifferentiable maps. As an illustration, we develop a test of existence of (potentially multiple)common conditional heteroskedasticity features that improves upon Dovonon and Renault (2013).},
  keywords = {Babu correction,Bootstrap consistency,Common CH features,First order degeneracy,J-test,Second order Delta method},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Asymptotic Theory\Chen, Fang (2019) Inference on functionals under first order degeneracy.pdf}
}

@article{Darling1957KolmogorovSmirnovCramervonMises,
  title = {The {{Kolmogorov-Smirnov}}, {{Cramer-von Mises Tests}}},
  author = {Darling, Donald A.},
  year = {1957},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {4},
  pages = {823--838},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177706788},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Density Estimation\Distribution Equality Tests\Darling (1957) The Kolmogorov-Smirnov, Cramer-von Mises Tests.pdf}
}

@article{Munk1998NonparametricValidationSimilar,
  title = {Nonparametric {{Validation}} of {{Similar Distributions}} and {{Assessment}} of {{Goodness}} of {{Fit}}},
  author = {Munk, Axel and Czado, Claudia},
  year = {1998},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  volume = {60},
  number = {1},
  pages = {223--241},
  issn = {13697412},
  doi = {10.1111/1467-9868.00121},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Density Estimation\Distribution Equality Tests\Munk, Czado (2002) Nonparametric Validation of Similar Distributions and Assessment of Goodness of Fit.pdf}
}


@unpublished{Gobillon2010,
	author = {Gobillon, Laurent and Roux, S{\'{e}}bastien},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Gobillon, Roux (2008) Quantile-based inference of parametric transformations.pdf:pdf},
	keywords = {and discussions,as well as the,c12,c24,c52,cation,distributions,edwin leuven and dominique,jel classi,marc gurgand,meurs for useful comments,model estimation,participants of the microeconometrics,quantiles,seminar at crest,st{\'{e}}phane gr{\'{e}}goir,we are grateful to},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	pages = {1--44},
	title = {{Quantile-Based Inference of Parametric Transformations between Two Distributions}},
	year = {2010}
}
@unpublished{Li2022,
	archivePrefix = {arXiv},
	arxivId = {2202.11031},
	author = {Li, Xingyu and Song, Xiaojun and Sun, Zhenting},
	eprint = {2202.11031},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Li, Song, Sun (2022) A Unified Nonparametric Test of Transformations on Distribution Functions with Nuisance Parameters.pdf:pdf},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	pages = {1--28},
	title = {{A Unified Nonparametric Test of Transformations on Distribution Functions with Nuisance Parameters}},
	url = {http://arxiv.org/abs/2202.11031},
	year = {2022}
}

@article{Gill1989,
	author = {Gill, Richard},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Asymptotic Theory/Gill (1989) Non- and semi-parametric maximum likelihood estimators and the von Mises method.pdf:pdf},
	journal = {Scandinavian Journal of Statistics},
	keywords = {asymptotically efficient estimation,compact differentiation,hadamard differentiation,non-parametric maximum likelihood,von mises method},
	mendeley-groups = {Nonparametric Estimation/Asymptotic Theory},
	number = {2},
	pages = {97--128},
	title = {{Non- and Semi-Parametric Maximum Likelihood Estimators and the Von Mises Method (Part 1)}},
	volume = {16},
	year = {1989}
}
@article{Ren2001,
	author = {Ren, Jian Jian and Sen, Pranab Kumar},
	doi = {10.1006/jmva.2000.1926},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Asymptotic Theory/Ren, Sen (2001) Second Order Hadamard Differentiability in Statistical Applications.pdf:pdf},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Cram{\'{e}}r-von Mises statistic,Hadamard differentiability,M-estimator,goodness of fit test,limiting distribution,linear models,statistical functionals,uniform asymptotic linearity,weighted empirical processes},
	mendeley-groups = {Nonparametric Estimation/Asymptotic Theory},
	number = {2},
	pages = {187--228},
	title = {{Second Order Hadamard Differentiability in Statistical Applications}},
	volume = {77},
	year = {2001}
}

@book{Hall1980,
	author = {Hall, Peter Gavin and Heyde, Christopher Charles},
	doi = {10.1016/C2013-0-10818-5},
	isbn = {978-0-12-319350-6},
	mendeley-groups = {Random Useful Math/Probability/Martingales},
	pages = {308},
	publisher = {Academic Press},
	title = {{Martingale Limit Theory and its Application}},
	year = {1980}
}
@article{Freitag2005,
	abstract = {Semiparametric models to describe the functional relationship between k groups of observations are broadly applied in statistical analysis, ranging from nonparametric ANOVA to proportional hazard (ph) rate models in survival analysis. In this paper we deal with the empirical assessment of the validity of such a model, which will be denoted as a "structural relationship model". To this end Hadamard differentiability of a suitable goodness-of-fit measure in the k-sample case is proved. This yields asymptotic limit laws which are applied to construct tests for various semiparametric models, including the Cox ph model. Two types of asymptotics are obtained, first when the hypothesis of the semiparametric model under investigation holds true, and second for the case when a fixed alternative is present. The latter result can be used to validate the presence of a semiparametric model instead of simply checking the null hypothesis "the model holds true". Finally, various bootstrap approximations are numerically investigated and a data example is analyzed. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
	author = {Freitag, Gudrun and Munk, Axel},
	doi = {10.1016/j.jmva.2004.03.006},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Freitag, Munk (2005) On Hadamard differentiability in  k -sample semiparametric models.pdf:pdf},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Goodness-of-fit,Hadamard differentiability,Multivariate empirical process,Nonlinear approximation,Proportional hazard rates,Quadratic differentiability,Semiparametric model,Weak convergence,k-sample problem},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	number = {1},
	pages = {123--158},
	title = {{On Hadamard Differentiability in k-Sample Semiparametric Models - With Applications to the Assessment of Structural Relationships}},
	volume = {94},
	year = {2005}
}


@book{Coleman2012,
	author = {Coleman, Rodney},
	doi = {10.1007/978-1-4614-3894-6},
	isbn = {978-1-4614-3893-9},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {xi, 249},
	publisher = {Springer New York},
	title = {{Calculus on Normed Vector Spaces}},
	year = {2012}
}

@book{Wainwright2019HighDimensionalStatisticsNonAsymptotic,
  title = {High-{{Dimensional Statistics}}: {{A Non-Asymptotic Viewpoint}}},
  author = {Wainwright, Martin J.},
  year = {2019},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781108627771},
  isbn = {978-1-108-62777-1}
}


@book{Shifrin2004,
	author = {Shifrin, Theodore},
	isbn = {978-0-471-52638-4},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {512},
	publisher = {Wiley},
	title = {{Multivariable Mathematics: Linear Algebra, Multivariable Calculus, and Manifolds}},
	year = {2004}
}
@book{Duistermaat2004,
	author = {Duistermaat, Johannes Jisse and Kolk, Johan A.C.},
	doi = {10.1017/CBO9780511616716},
	isbn = {9780511616716},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {xviii, 422},
	publisher = {Cambridge University Press},
	title = {{Multidimensional Real Analysis I: Differentiation}},
	year = {2004}
}

@book{Munkres1991,
	author = {Munkres, James R.},
	doi = {10.1201/9780429494147},
	isbn = {9780429494147},
	mendeley-groups = {Random Useful Math/Analysis},
	publisher = {CRC Press},
	title = {{Analysis On Manifolds}},
	year = {1991}
}
@book{Spivak1965,
	author = {Spivak, Michael},
	doi = {10.1201/9780429501906},
	isbn = {9780429501906},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {xii, 146},
	publisher = {CRC Press},
	title = {{Calculus On Manifolds}},
	year = {1965}
}

@book{Resnick2013,
	author = {Resnick, Sidney I.},
	doi = {10.1007/978-0-8176-8409-9},
	isbn = {978-0-8176-8408-2},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {xiv, 453},
	publisher = {Birkh{\"{a}}user Boston},
	title = {{A Probability Path}},
	year = {2013}
}

@book{Dudley2002,
	author = {Dudley, Richard M.},
	doi = {10.1017/CBO9780511755347},
	edition = {2},
	isbn = {9780511755347},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {x, 555},
	publisher = {Cambridge University Press},
	title = {{Real Analysis and Probability}},
	year = {2002}
}

@book{Rogers1994,
	author = {Rogers, Leonard Christopher Gordon and Williams, David},
	doi = {10.1017/CBO9781107590120},
	isbn = {9781107590120},
	mendeley-groups = {Random Useful Math/Probability/Martingales},
	pages = {386},
	publisher = {Cambridge University Press},
	title = {{Diffusions, Markov Processes and Martingales. Volume 1: Foundations}},
	year = {1994}
}
@book{Neveu1975,
	author = {Neveu, Jacques},
	doi = {10.1016/s0924-6509(09)x7006-6},
	isbn = {978-0-7204-2810-0},
	mendeley-groups = {Random Useful Math/Probability/Martingales},
	pages = {v, 236},
	publisher = {North-Holland},
	title = {{Discrete-Parameter Martingales}},
	year = {1975}
}


@book{Williams1991,
	author = {Williams, David},
	doi = {10.1017/CBO9780511813658},
	isbn = {9780521406055},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {265},
	publisher = {Cambridge University Press},
	title = {{Probability with Martingales}},
	year = {1991}
}
@book{Durrett2019,
	author = {Durrett, Rick},
	doi = {10.1017/9781108591034},
	isbn = {9781108591034},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {XII, 420},
	publisher = {Cambridge University Press},
	title = {{Probability: Theory and Examples}},
	year = {2019}
}

@book{Lehmann2006NonparametricsStatisticalMethods,
	title = {Nonparametrics: {{Statistical Methods Based}} on {{Ranks}}},
	shorttitle = {Nonparametrics},
	author = {Lehmann, E. L. and D'Abrera, H. J. M.},
	year = {2006},
	edition = {Rev. 1st ed},
	publisher = {Springer},
	address = {New York},
	isbn = {978-0-387-35212-1},
	lccn = {QA278.8 .L43 2006},
	keywords = {Nonparametric statistics},
	annotation = {OCLC: ocm71747543}
}

@inproceedings{Ho1995RandomDecisionForests,
  title = {Random {{Decision Forests}}},
  booktitle = {Proceedings of 3rd {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Ho, Tin Kam},
  year = {1995},
  volume = {1},
  pages = {278--282},
  publisher = {IEEE Comput. Soc. Press},
  address = {Montreal, Que., Canada},
  doi = {10.1109/ICDAR.1995.598994},
  urldate = {2025-08-03},
  isbn = {978-0-8186-7128-9}
}

@article{Neyman1937,
	author = {Neyman, Jerzy},
	doi = {10.1080/03461238.1937.10404821},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Neyman (1937) »Smooth test» for goodness of fit.pdf:pdf},
	issn = {16512030},
	journal = {Scandinavian Actuarial Journal},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	number = {3-4},
	pages = {149--199},
	title = {{Smooth Test for Goodness of Fit}},
	volume = {1937},
	year = {1937}
}
@article{Romano1988,
	author = {Romano, Joseph P.},
	doi = {10.1080/01621459.1988.10478650},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Bootstrap/Romano (1988) A Bootstrap Revival of Some Nonparametric Distance Tests.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Distance tests,Goodness of fit,Nonparametric tests,Testing equality of distributions,Testing for rotational invariance,Testing independence,Vapnik–Cervonenkis classes},
	mendeley-groups = {Resampling/Bootstrap},
	number = {403},
	pages = {698--708},
	title = {{A Bootstrap Revival of Some Nonparametric Distance Tests}},
	volume = {83},
	year = {1988}
}

@article{Thomas1979,
	author = {Thomas, David R. and Pierce, Donald A.},
	doi = {10.2307/2286351},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Thomas, Pierce (1979) Neyman's Smooth Goodness-of-Fit Test When the Hypothesis Is Composite.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	number = {366},
	pages = {441},
	title = {{Neyman's Smooth Goodness-of-Fit Test When the Hypothesis Is Composite}},
	volume = {74},
	year = {1979}
}
@article{McCulloch2013,
	author = {McCulloch, J. Huston and Percy, E. Richard},
	doi = {10.1016/j.jeconom.2012.08.018},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/McCulloch, Percy (2013). Extended Neyman smooth goodness-of-fit tests, applied to competing heavy-tailed distributions.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests and Goodness-of-Fit},
	number = {2},
	pages = {275--282},
	title = {{Extended Neyman Smooth Goodness-of-fit Tests, Applied to Competing Heavy-tailed Distributions}},
	volume = {172},
	year = {2013}
}

@book{DelBarrio2007,
	author = {del Barrio, Eustasio and Deheuvels, Paul and van de Geer, Sara},
	doi = {10.4171/027},
	isbn = {978-3-03719-027-2},
	mendeley-groups = {Random Useful Math/Probability/Stochastic Processes},
	pages = {263},
	publisher = {European Mathematical Society},
	title = {{Lectures on Empirical Processes: Theory and Statistical Applications}},
	year = {2007}
}

@book{Shorack1986,
	author = {Shorack, Galen R. and Wellner, Jon A.},
	doi = {10.1137/1.9780898719017},
	isbn = {978-0-89871-684-9},
	mendeley-groups = {Random Useful Math/Probability/Stochastic Processes},
	pages = {xxxvi + 955},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{Empirical Processes with Applications to Statistics}},
	year = {1986}
}

@book{Rayner2009,
	author = {Rayner, J.C.W. and Thas, Olivier and Best, D.J.},
	doi = {10.1002/9780470824443},
	edition = {2},
	isbn = {9780470824429},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	pages = {xvi, 281},
	publisher = {Wiley},
	title = {{Smooth Tests of Goodness of Fit: Using R}},
	year = {2009}
}

@article{Cao2005,
	author = {Cao, Ricardo and Lugosi, Gabor},
	doi = {10.1111/j.1467-9469.2005.00471.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Cao, Lugosi (2005) Goodness-of-Fit Tests Based on the Kernel Density Estimator.pdf:pdf},
	journal = {Scandinavian Journal of Statistics},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {4},
	pages = {599--616},
	title = {{Goodness-of-Fit Tests Based on the Kernel Density Estimator}},
	volume = {32},
	year = {2005}
}
@article{Braun1980,
	author = {Braun, Henry},
	doi = {10.1111/j.2517-6161.1980.tb01100.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Braun (1980) A Simple Method for Testing Goodness of Fit in the Presence of Nuisance Parameters.pdf:pdf},
	issn = {0035-9246},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	keywords = {composite hypotheses,goodness of fit,randomization},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {1},
	pages = {53--63},
	title = {{A Simple Method for Testing Goodness of Fit in the Presence of Nuisance Parameters}},
	volume = {42},
	year = {1980}
}
@article{Babu2007,
	author = {Babu, G. Jogesh and Rao, C.R.},
	doi = {10.2307/25053332},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Jogesh Babu, Rao (2004) Goodness-of-Fit Tests When Parameters Are Estimated.pdf:pdf},
	journal = {Sankhyā},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {1},
	pages = {63--74},
	title = {{Goodness-of-Fit Tests When Parameters Are Estimated}},
	volume = {66},
	year = {2007}
}

@article{Janssen1994TwoSampleGoodnessoffitTests,
  title = {Two-{{Sample Goodness-of-fit Tests When Ties Are Present}}},
  author = {Janssen, Arnold},
  year = {1994},
  journal = {Journal of Statistical Planning and Inference},
  volume = {39},
  number = {3},
  pages = {399--424},
  issn = {03783758},
  doi = {10.1016/0378-3758(94)90095-7},
  keywords = {Anderson-Darling test,asymptotic power function,Cramer-von Mises tests,Kolmogorov-Smirnov tests,permutation tests,Ties},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Density Estimation\Distribution Equality Tests\Janssen (1994) Two-sample goodness-of-fit tests when ties are present.pdf}
}


@unpublished{Moon2018,
	archivePrefix = {arXiv},
	arxivId = {1810.10987},
	author = {Moon, Hyungsik Roger and Weidner, Martin},
	eprint = {1810.10987},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Factor Models/Moon, Weidner (2023) Nuclear Norm Regularized Estimation of Panel Regression Models.pdf:pdf},
	mendeley-groups = {Factor Models,Panel Data/Factor Models and Interactive Effects},
	number = {June},
	pages = {1--69},
	title = {{Nuclear Norm Regularized Estimation of Panel Regression Models}},
	year = {2018}
}
@article{Zhou2017,
	archivePrefix = {arXiv},
	arxivId = {1509.03459},
	author = {Zhou, Wen Xin and Zheng, Chao and Zhang, Zhen},
	doi = {10.3150/15-BEJ766},
	eprint = {1509.03459},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Zhou, Zheng, Zhang (2017) Two-sample smooth tests for the equality of distributions.pdf:pdf},
	issn = {13507265},
	journal = {Bernoulli},
	keywords = {Goodness-of-fit,High-frequency alternations,Multiplier bootstrap,Neyman's smooth test,Two-sample problem},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {2},
	pages = {951--989},
	title = {{Two-Sample Smooth Tests for the Equality of Distributions}},
	volume = {23},
	year = {2017}
}
@article{Song2022,
	author = {Song, Xiaojun and Xiao, Zhijie},
	doi = {10.1017/S0266466620000596},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Song, Xiao (2021) On Smooth Tests for the Equality of Distributions.pdf:pdf},
	issn = {14694360},
	journal = {Econometric Theory},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {1},
	pages = {194--208},
	title = {{On Smooth Tests for the Equality of Distributions}},
	volume = {38},
	year = {2022}
}

@article{Bera2013,
	author = {Bera, Anil K. and Ghosh, Aurobindo and Xiao, Zhijie},
	doi = {10.1017/S0266466612000370},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Distribution Equality Tests/Bera, Ghosh, Xiao (2012) A Smooth Test for the Equality of Distributions.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {Nonparametric Estimation/Distribution Estimation/Equality Tests},
	number = {2},
	pages = {419--446},
	title = {{A Smooth Test for the Equality of Distributions}},
	volume = {29},
	year = {2013}
}

@incollection{Athey2017EconometricsRandomizedExperiments,
  title = {The {{Econometrics}} of {{Randomized Experiments}}},
  booktitle = {Handbook of {{Economic Field Experiments}}},
  author = {Athey, S. and Imbens, G.W.},
  year = {2017},
  volume = {1},
  pages = {73--140},
  publisher = {Elsevier},
  doi = {10.1016/bs.hefe.2016.10.003},
  urldate = {2024-11-14},
  abstract = {In this chapter, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments, we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as noncompliance. In the presence of noncompliance, we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider, in detail, estimation and inference for heterogenous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  isbn = {978-0-444-63324-8},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Experiments\Athey and Imbens - 2017 - The Econometrics of Randomized Experiments.pdf}
}

@article{Athey2017StateAppliedEconometrics,
  title = {The {{State}} of {{Applied Econometrics}}: {{Causality}} and {{Policy Evaluation}}},
  author = {Athey, Susan and Imbens, Guido W.},
  year = {2017},
  journal = {Journal of Economic Perspectives},
  volume = {31},
  number = {2},
  pages = {3--32},
  issn = {08953309},
  doi = {10.1257/jep.31.2.3},
  abstract = {In this paper, we discuss recent developments in econometrics that we view as important for empirical researchers working on policy evaluation questions. We focus on three main areas, in each case, highlighting recommendations for applied work. First, we discuss new research on identification strategies in program evaluation, with particular focus on synthetic control methods, regression discontinuity, external validity, and the causal interpretation of regression methods. Second, we discuss various forms of supplementary analyses, including placebo analyses as well as sensitivity and robustness analyses, intended to make the identification strategies more credible. Third, we discuss some implications of recent advances in machine learning methods for causal effects, including methods to adjust for differences between treated and control units in high-dimensional settings, and methods for identifying and estimating heterogenous treatment effects.},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Athey and Imbens - 2017 - The State of Applied Econometrics Causality and Policy Evaluation.pdf}
}

@book{Cunningham2021CausalInferenceMixtape,
  title = {Causal {{Inference}}: {{The Mixtape}}},
  author = {Cunningham, Scott},
  year = {2021},
  publisher = {Yale University Press},
  doi = {10.2307/j.ctv1c29t27},
  isbn = {978-0-300-25588-1}
}
@book{Imbens2015CausalInferenceStatistics,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  year = {2015},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781139025751},
  isbn = {978-1-139-02575-1}
}

@book{Morgan2014CounterfactualsCausalInference,
  title = {Counterfactuals and {{Causal Inference}}: {{Methods}} and {{Principles}} for {{Social Research}}},
  author = {Morgan, Stephen L. and Winship, Christopher},
  year = {2014},
  journal = {Counterfactuals and Causal Inference: Methods and Principles for Social Research},
  edition = {2},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9781107587991},
  abstract = {In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed.},
  isbn = {978-1-107-58799-1}
}

@book{Bertsekas2008IntroductionProbability,
  title = {Introduction to {{Probability}}},
  author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  year = {2008},
  series = {Optimization and Computation Series},
  edition = {2nd ed},
  publisher = {Athena scientific},
  address = {Belmont},
  isbn = {978-1-886529-23-6},
  langid = {english},
  lccn = {519.2}
}


@article{Mikusheva2022,
	abstract = {We develop a concept of weak identification in linear instrumental variable models in which the number of instruments can grow at the same rate or slower than the sample size. We propose a jackknifed version of the classical weak identification-robust Anderson–Rubin (AR) test statistic. Large-sample inference based on the jackknifed AR is valid under heteroscedasticity and weak identification. The feasible version of this statistic uses a novel variance estimator. The test has uniformly correct size and good power properties. We also develop a pre-test for weak identification that is related to the size property of a Wald test based on the Jackknife Instrumental Variable Estimator. This new pre-test is valid under heteroscedasticity and with many instruments.},
	archivePrefix = {arXiv},
	arxivId = {2004.12445},
	author = {Mikusheva, Anna and Sun, Liyang},
	doi = {10.1093/restud/rdab097},
	eprint = {2004.12445},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/IV/Weak IV/Mikusheva, Sun (2022) Inference with Many Weak Instruments.pdf:pdf},
	issn = {1467937X},
	journal = {Review of Economic Studies},
	keywords = {Dimensionality asymptotics,Instrumental variables,Weak identification},
	mendeley-groups = {IV/Weak IV},
	number = {5},
	pages = {2663--2686},
	title = {{Inference with Many Weak Instruments}},
	volume = {89},
	year = {2022}
}

@article{Mikusheva2024,
	abstract = {Linear instrumental variable regressions are widely used to estimate causal effects. Many instruments arise from the use of ‘technical' instruments and more recently from the empirical strategy of ‘judge design'. This paper surveys and summarises ideas from recent literature on estimation and statistical inferences with many instruments for a single endogenous regressor. We discuss how to assess the strength of the instruments and how to conduct weak identification robust inference under heteroscedasticity. We establish new results for a jack-knifed version of the Lagrange Multiplier test statistic. Furthermore, we extend the weak identification robust tests to settings with both many exogenous regressors and many instruments. We propose a test that properly partials out many exogenous regressors while preserving the re-centring property of the jack-knife. The proposed tests have correct size and good power properties.},
	archivePrefix = {arXiv},
	arxivId = {2308.09535},
	author = {Mikusheva, Anna and Sun, Liyang},
	doi = {10.1093/ectj/utae007},
	eprint = {2308.09535},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/IV/Weak IV/Mikusheva, Sun (2023) Weak Identification with Many Instruments.pdf:pdf},
	issn = {1368-4221},
	journal = {The Econometrics Journal},
	keywords = {instrumental variable regressions,many instruments,weak instruments},
	mendeley-groups = {IV/Weak IV},
	pages = {1--41},
	title = {{Weak identification with many instruments}},
	year = {2024}
}

@article{Huntington-Klein2020,
	abstract = {In Instrumental Variables (IV) estimation, the effect of an instrument on an endogenous variable may vary across the sample. In this case, IV produces a local average treatment effect (LATE), and if monotonicity does not hold, then no effect of interest is identified. In this paper, I calculate the weighted average of treatment effects that is identified under general first-stage effect heterogeneity, which is generally not the average treatment effect among those affected by the instrument. I then describe a simple set of data-driven approaches to modeling variation in the effect of the instrument. These approaches identify a Super-Local Average Treatment Effect (SLATE) that weights treatment effects by the corresponding instrument effect more heavily than LATE. Even when first-stage heterogeneity is poorly modeled, these approaches considerably reduce the impact of small-sample bias compared to standard IV and unbiased weak-instrument IV methods, and can also make results more robust to violations of monotonicity. In application to a published study with a strong instrument, the preferred approach reduces error by about 19{\%} in small (N ≈ 1, 000) subsamples, and by about 13{\%} in larger (N ≈ 33, 000) subsamples.},
	author = {Huntington-Klein, Nick},
	doi = {10.1515/jci-2020-0011},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/IV/Instrument Strength Heterogeneity/Huntington-Klein (2020) Instruments with Heterogeneous Effects Bias, Monotonicity, and Localness.pdf:pdf},
	issn = {21933685},
	journal = {Journal of Causal Inference},
	keywords = {Causal Inference,Observational Studies,computational methods,economics},
	mendeley-groups = {IV/Heterogeneity},
	number = {1},
	pages = {182--208},
	title = {{Instruments with Heterogeneous Effects: Bias, Monotonicity, and Localness}},
	volume = {8},
	year = {2020}
}

@unpublished{Angelini2024,
	abstract = {When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for. In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency. We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated. Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity. We introduce a novel identification strategy that appropriately combines external instruments with "informative" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation. We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes.},
	archivePrefix = {arXiv},
	arxivId = {2403.08753},
	author = {Angelini, Giovanni and Fanelli, Luca and Neri, Luca},
	eprint = {2403.08753},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/VAR and SVAR/Angelini, Fanelli, Neri (2024) Invalid proxies and volatility changes.pdf:pdf},
	keywords = {c32,c51,c52,e62,external instruments,fiscal multipliers,identification,jel classification,proxy-,shifts in volatility,structural breaks,svars,weak instruments},
	mendeley-groups = {VAR and SVAR/SVAR Identification,VAR and SVAR/SVAR Identification/Heteroskedasticity,VAR and SVAR/SVAR Identification/Instruments},
	number = {March},
	title = {{Invalid Proxies and Volatility Changes}},
	year = {2024}
}

@article{Abadie2024,
	abstract = {We propose a simple data-driven procedure that exploits heterogeneity in the first-stage correlation between an instrument and an endogenous variable to improve the asymptotic mean squared error (MSE) of instrumental variable estimators. We show that the resulting gains in asymptotic MSE can be quite large in settings where there is substantial heterogeneity in the first-stage parameters. We also show that a naive procedure used in some applied work, which consists of selecting the composition of the sample based on the value of the first-stage t-statistic, may cause substantial over-rejection of a null hypothesis on a second-stage parameter. We apply the methods to study (1) the return to schooling using the minimum school leaving age as the exogenous instrument and (2) the effect of local economic conditions on voter turnout using energy supply shocks as the source of identification.},
	author = {Abadie, Alberto and Gu, Jiaying and Shen, Shu},
	doi = {10.1016/j.jeconom.2023.02.005},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/IV/Instrument Strength Heterogeneity/Abadie, Gu, Shen (2024) Instrumental Variable Estimation with First-Stage Heterogeneity.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {2SLS,Asymptotic MSE,First-stage heterogeneity,Pre-testing},
	mendeley-groups = {IV/Heterogeneity,!Extra reading list},
	number = {2},
	pages = {105425},
	publisher = {Elsevier B.V.},
	title = {{Instrumental Variable Estimation with First-Stage Heterogeneity}},
	volume = {240},
	year = {2024}
}

@article{Stock2017,
	author = {Stock, James H. and Watson, Mark W.},
	doi = {10.1257/jep.31.2.59},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Stock, Watson (2017) Twenty Years of Time Series Econometrics in Ten Pictures.pdf:pdf},
	issn = {08953309},
	journal = {Journal of Economic Perspectives},
	mendeley-groups = {Time Series},
	number = {2},
	pages = {59--86},
	title = {{Twenty Years of Time Series Econometrics in Ten Pictures}},
	volume = {31},
	year = {2017}
}

@article{Gertler2015,
	abstract = {We provide evidence on the transmission of monetary policy shocks in a setting with both economic and financial variables. We first show that shocks identified using high frequency surprises around policy announcements as external instruments produce responses in output and inflation that are typical in monetary VAR analysis. We also find, however, that the resulting “modest” movements in short rates lead to “large” movements in credit costs, which are due mainly to the reaction of both term premia and credit spreads. Finally, we show that forward guidance is important to the overall strength of policy transmission. (JEL E31, E32, E43, E44, E52, G01)},
	author = {Gertler, Mark and Karadi, Peter},
	doi = {10.1257/mac.20130329},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Monetary/Gertler, Karadi (2015) Monetary policy surprises, credit costs, and economic activity.pdf:pdf},
	journal = {American Economic Journal: Macroeconomics},
	mendeley-groups = {Applied (By Field)/Macro/Monetary},
	number = {1},
	pages = {44--76},
	title = {{Monetary Policy Surprises, Credit Costs,and Economic Activity}},
	volume = {7},
	year = {2015}
}

@article{Chamberlain1982causality,
	author = {Chamberlain, Gary},
	doi = {10.2307/1912601},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Causality/Chamberlain (1982) The General Equivalence of Granger and Sims Causality.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Time Series/Causality},
	number = {3},
	pages = {569},
	title = {{The General Equivalence of Granger and Sims Causality}},
	volume = {50},
	year = {1982}
}
@article{Sims1972,
	author = {Sims, Christopher A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Causality/Sims (1972) Money, Income, and Causality.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Time Series/Causality},
	number = {4},
	pages = {540--552},
	title = {{Money, Income, and Causality}},
	volume = {62},
	year = {1972}
}

@article{Granger1980,
	abstract = {A general definition of causality is introduced and then specialized to become operational. By considering simple examples a number of advantages, and also difficulties, with the definition are discussed. Tests based on the definitions are then considered and the use of post-sample data emphasized, rather than relying on the same data to fit a model and use it to test causality. It is suggested that a Bayesian viewpoint should be taken in interpreting the results of these tests. Finally, the results of a study relating advertising and consumption are briefly presented.},
	author = {Granger, Clive},
	doi = {10.1016/0165-1889(80)90069-X},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Causality/Granger (1980) Testing for causality.pdf:pdf},
	journal = {Journal of Economic Dynamics and Control},
	mendeley-groups = {Time Series/Causality},
	number = {2-4},
	pages = {329--352},
	title = {{Testing for Causality: A Personal Viewpoint}},
	volume = {2},
	year = {1980}
}
@article{Granger1969,
	abstract = {No abstract is available for this item.},
	author = {Granger, Clive},
	doi = {10.2307/1912791},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Causality/Granger (1969) Investigating Causal Relations by Econometric Models and Cross-spectral Methods.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Time Series/Causality},
	number = {3},
	pages = {424},
	title = {{Investigating Causal Relations by Econometric Models and Cross-spectral Methods}},
	volume = {37},
	year = {1969}
}

@incollection{Kuersteiner2016,
	abstract = {Franchising, which is common in many advanced economies, is a contractual form of vertical integration. This article examines the economic rationale for choosing franchising over vertical integration. It also examines the influence of the franchisor's ability to maximize its own profit.},
	author = {Kuersteiner, Guido M.},
	booktitle = {The New Palgrave Dictionary of Economics},
	doi = {10.1057/978-1-349-95121-5_2095-1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Causality/Kuersteiner (2008) Granger–Sims Causality.pdf:pdf},
	isbn = {9781349951215},
	keywords = {block recursive structure,causality in eco-,conditional inde-,nomics and econometrics},
	mendeley-groups = {Time Series/Causality},
	pages = {119--134},
	publisher = {Palgrave Macmillan, London},
	title = {{Granger-Sims Causality}},
	year = {2016}
}

@article{Bojinov2019,
	abstract = {We define causal estimands for experiments on single time series, extending the potential outcome framework to dealing with temporal data. Our approach allows the estimation of a broad class of these estimands and exact randomization-based p-values for testing causal effects, without imposing stringent assumptions. We further derive a general central limit theorem that can be used to conduct conservative tests and build confidence intervals for causal effects. Finally, we provide three methods for generalizing our approach to multiple units that are receiving the same class of treatment, over time. We test our methodology on simulated “potential autoregressions,” which have a causal interpretation. Our methodology is partially inspired by data from a large number of experiments carried out by a financial company who compared the impact of two different ways of trading equity futures contracts. We use our methodology to make causal statements about their trading methods. Supplementary materials for this article are available online.},
	archivePrefix = {arXiv},
	arxivId = {1706.07840},
	author = {Bojinov, Iavor and Shephard, Neil},
	eprint = {1706.07840},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Time Series/Bojinov, Shephard (2019) Time Series Experiments and Causal Estimands Exact Randomization Tests and Trading.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Causality,Nonparametric,Potential outcomes,Trading costs},
	mendeley-groups = {Treatment Effects/Time Series},
	number = {528},
	pages = {1665--1682},
	publisher = {Informa UK Limited, trading as Taylor {\&} Francis Group},
	title = {{Time Series Experiments and Causal Estimands: Exact Randomization Tests and Trading}},
	url = {https://doi.org/10.1080/03610926.2018.1512864},
	volume = {114},
	year = {2019}
}

@article{Stock2018,
	abstract = {External sources of as-if randomness — that is, external instruments — can be used to identify the dynamic causal effects of macroeconomic shocks. One method is a one-step instrumental variables regression (local projections – IV); a more efficient two-step method involves a vector autoregression. We show that, under a restrictive instrument validity condition, the one-step method is valid even if the vector autoregression is not invertible, so comparing the two estimates provides a test of invertibility. If, however, lagged endogenous variables are needed as control variables in the one-step method, then the conditions for validity of the two methods are the same.},
	author = {Stock, James H. and Watson, Mark W.},
	doi = {10.1111/ecoj.12593},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/VAR and SVAR/Stock, Watson (2018) Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments.pdf:pdf},
	issn = {14680297},
	journal = {Economic Journal},
	mendeley-groups = {VAR and SVAR/SVAR Identification},
	number = {610},
	pages = {917--948},
	title = {{Identification and Estimation of Dynamic Causal Effects in Macroeconomics Using External Instruments}},
	volume = {128},
	year = {2018}
}

@article{Hjort2003rejoinder,
	author = {Hjort, Nils Lid and Claeskens, Gerda},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Hjort, Claeskens (2003) Rejoinder to Frequentist Model Averaging.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	number = {464},
	pages = {938--945},
	title = {{Rejoinder to the Focused Information Criterion and Frequentist Model Average Estimators}},
	volume = {98},
	year = {2003}
}
@article{Raftery2003,
	author = {Raftery, Adrian E. and Zheng, Yingye},
	doi = {10.1198/016214503000000891},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Raftery, Zheng (2003) Discussion Performance of Bayesian model averaging.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	number = {464},
	pages = {931--938},
	title = {{Discussion: Performance of Bayesian Model Averaging}},
	volume = {98},
	year = {2003}
}

@unpublished{Mohring2020,
	author = {M{\"{o}}hring, Katja and Naumann, Elias and Reifenscheid, Maximiliane and Blom, Annelies G. and Wenz, Alexander and Rettig, Tobias and Lehrer, Roni and Krieger, Ulrich and Juhl, Sebastian and Friedel, Sabine and Fikel, Marina and Cornesse, Carina},
	booktitle = {JESP European Social Policy Blog},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Unit Averaging/Papers on Germany/M{\"{o}}hring et al. (2020) Inequality in Employment During the Corona Lockdown.pdf:pdf},
	mendeley-groups = {zFor Specific Documents/Unit Averaging/AE Reply},
	number = {January},
	title = {{Inequality in Employment during the Corona Lockdown: Evidence from Germany}},
	volume = {2020},
	year = {2020}
}
@article{Adams-Prassl2020,
	abstract = {We present real time survey evidence from the UK, US and Germany showing that the immediate labor market impacts of Covid-19 differ considerably across countries. Employees in Germany, which has a well-established short-time work scheme, are substantially less likely to be affected by the crisis. Within countries, the impacts are highly unequal and exacerbate existing inequalities. Workers in alternative work arrangements and who can only do a small share of tasks from home are more likely to have lost their jobs and suffered falls in earnings. Women and less educated workers are more affected by the crisis.},
	author = {Adams-Prassl, Abi and Boneva, Teodora and Golin, Marta and Rauh, Christopher},
	doi = {10.1016/j.jpubeco.2020.104245},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Unit Averaging/Papers on Germany/Adams-Prassl et al. (2020)  Inequality in the impact of the coronavirus shock.pdf:pdf},
	issn = {00472727},
	journal = {Journal of Public Economics},
	keywords = {Coronavirus,Covid-19,Furlough,Gender gap,Inequality,Job loss,Labor market,Recessions,Short-time work,Working from home},
	mendeley-groups = {zFor Specific Documents/Unit Averaging/AE Reply},
	pages = {104245},
	publisher = {Elsevier B.V.},
	title = {{Inequality in the Impact of the Coronavirus Shock: Evidence From Real Time Surveys}},
	volume = {189},
	year = {2020}
}
@article{Casey2023,
	author = {Casey, Bernard H and Mayhew, Ken},
	doi = {10.1017/nie.2021.46},
	journal = {National Institute Economic Review},
	mendeley-groups = {zFor Specific Documents/Unit Averaging/AE Reply},
	pages = {47--60},
	publisher = {Cambridge University Press},
	title = {{Kurzarbeit/Short-Time Working: Experiences and Lessons from the COVID-Induced Downturn}},
	volume = {263},
	year = {2023}
}
@article{Aiyar2021,
	abstract = {Kurzarbeit (KA), Germany's short-time work program, is widely credited with saving jobs and supporting domestic demand during the COVID-19 recession. We quantify the impact by exploiting state-level variation in exposure to the pandemic shock and KA take-up. We construct a shift-share measure of the labor demand shock and instrument KA take-up using the pre-existing, state-specific share of workers eligible for KA. We find, first, that KA was crucial in mitigating unemployment: absent its expansion the unemployment rate would have increased by an additional 3 pp on average at the trough of the recession. Second, KA also bolstered domestic demand: the contraction in consumption could have been 2 to 3 times larger absent the program. Finally, we provide preliminary evidence on the sensitivity of the medium-run reallocation of resources to the prevalence of job- retention schemes during the Global Financial Crisis.},
	author = {Aiyar, Shekhar and Dao, Mai Chi},
	doi = {10.5089/9781513596174.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Unit Averaging/Papers on Germany/Aiyar, Dao (2021) The Effectiveness of Job-Retention Schemes COVID-19 Evidence From the German States.pdf:pdf},
	issn = {1018-5941},
	journal = {IMF Working Papers},
	keywords = {Kurzarbeit, Short-time work, Unemployment, Covid-19},
	mendeley-groups = {zFor Specific Documents/Unit Averaging/AE Reply},
	number = {242},
	pages = {1},
	title = {{The Effectiveness of Job-Retention Schemes: COVID-19 Evidence From the German States}},
	volume = {2021},
	year = {2021}
}

@article{Zhang2019b,
	abstract = {This article considers the problem of inference for nested least squares averaging estimators. We study the asymptotic behavior of the Mallows model averaging estimator (MMA; Hansen, 2007) and the jackknife model averaging estimator (JMA; Hansen and Racine, 2012) under the standard asymptotics with fixed parameters setup. We find that both MMA and JMA estimators asymptotically assign zero weight to the under-fitted models, and MMA and JMA weights of just-fitted and over-fitted models are asymptotically random. Building on the asymptotic behavior of model weights, we derive the asymptotic distributions of MMA and JMA estimators and propose a simulation-based confidence interval for the least squares averaging estimator. Monte Carlo simulations show that the coverage probabilities of proposed confidence intervals achieve the nominal level.},
	author = {Zhang, Xinyu and Liu, Chu-An},
	doi = {10.1017/S0266466618000269},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Zhang, Liu (2018)  Inference After Model Averaging in Linear Regression Models.pdf:pdf},
	issn = {14694360},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	number = {4},
	pages = {816--841},
	title = {{Inference After Model Averaging in Linear Regression Models}},
	volume = {35},
	year = {2019}
}
@article{Zhang2024,
	author = {Zhang, Xinyu and Liu, Chu-An},
	doi = {10.5705/ss.202021.0266},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Zhang, Liu (2022) A Unified Approach to Focused Information Criterion.pdf:pdf},
	issn = {10170405},
	journal = {Statistica Sinica},
	keywords = {all errors and omissions,ames 2019,and,are our own responsibility,c51,c52,discussions and suggestions,econometrics 2018,ecosta 2019,esma 2019 for their,focused information criterion,jel classification,model averaging,model selection,participants of advances in,we thank the conference},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	pages = {771--792},
	title = {{A Unified Approach to Focused Information Criterion and Plug-In Averaging Method}},
	volume = {34},
	year = {2024}
}

@book{Lehmann2022TestingStatisticalHypotheses,
  title = {Testing {{Statistical Hypotheses}}},
  author = {Lehmann, Erich L. and Romano, Joseph P.},
  year = {2022},
  edition = {4},
  publisher = {Springer Cham},
  doi = {10.1007/978-3-030-70578-7}
}


@article{Staiger1997,
	abstract = {This paper develops asymptotic distribution theory for single-equation instrumental variables regression when the partial correlations between the instruments and the endogenous variables are weak, here modeled as local to zero. Asymptotic representations are provided for various statistics, including two-stage least squares (TSLS) and limited information maximum likelihood (LIML) estimators, Wald statistics, and statistics testing overidentification and endogeneity. The asymptotic distributions are found to provide good approximations to sampling distributions with 10-20 observations per instrument. The theory suggests concrete guidelines for applied work, including using nonstandard methods for construction of confidence regions. These results are used to interpret Angrist and Krueger's (1991) estimates of the returns to education: whereas TSLS estimates with many instruments approach the OLS estimate of 6{\%}, the more reliable LIML estimates with fewer instruments fall between 8{\%} and 10{\%}, with a typical 95{\%} confidence},
	author = {Staiger, Douglas and Stock, James H.},
	doi = {10.2307/2171753},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Weak IV/Staiger, Stock (1997)  Instrumental Variables Regression with Weak Instruments.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {endogeneity tests,liml,overidentification tests,two stage least squares},
	mendeley-groups = {GMM/Weak IV},
	number = {3},
	pages = {557},
	title = {{Instrumental Variables Regression with Weak Instruments}},
	volume = {65},
	year = {1997}
}

@article{Lin2007,
	abstract = {We investigate the convergence, in norm and almost everywhere (a.e.), of weighted ergodic averages as well as weighted sums of independent identically distributed (iid) random variables. The averages are true ones, normalized by the corresponding sums of weights, which are only assumed to be non-negative. The L2-norm convergence in the mixing case is shown to rely upon very simple conditions on the weights. We show that 'quasimonotone weights' with a simple additional condition yield a.e. convergence of weighted averages for all Dunford-Schwartz contractions of probability spaces and L1-functions. For independent random variables, we look at weighted averages of centered random variables with bounded variances (or bounded moments of some order greater than 1), in particular the iid case, and obtain several sufficient conditions on the weights for almost sure convergence (weighted SLLN). For example, in Theorem 4.14 we show that if a weight sequence {\{}wk{\}} with divergent partial sums Wn satisfies supn≥1 1/W n ∑k=1n wk(log(w k+1))$\beta$ {\textless} ∞ for some $\beta$ {\textgreater} 1 then for any iid sequence in the class L(log+L)1+∈ the weighted averages converge almost surely to the expectation. {\textcopyright} 2007 Cambridge University Press.},
	author = {Lin, Michael and Weber, Michel},
	doi = {10.1017/S0143385706000769},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/LLNs/Lin, Weber (2007) Weighted ergodic theorems and strong laws of large number.pdf:pdf},
	issn = {01433857},
	journal = {Ergodic Theory and Dynamical Systems},
	mendeley-groups = {Random Useful Math/Probability/Limit Theorems},
	number = {2},
	pages = {511--543},
	title = {{Weighted Ergodic Theorems and Strong Laws of Large Numbers}},
	volume = {27},
	year = {2007}
}
@article{Rohatgi1971,
	author = {Rohatgi, Vijay K.},
	doi = {10.1017/S0305004100046685},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/LLNs/Rohatgi (1971) Convergence of weighted sums of independent random variables.pdf:pdf},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	mendeley-groups = {Random Useful Math/Probability/Limit Theorems},
	number = {2},
	pages = {305--307},
	title = {{Convergence of Weighted Sums of Independent Random Variables}},
	volume = {69},
	year = {1971}
}

@incollection{Graaff2018,
	abstract = {Persistence of high youth unemployment and dismal labour market outcomes are imminent concerns for most European economies. The relationship between demographic ageing and employment outcomes is even more worrying once the relationship is scrutinized at the regional level. We focus on modelling regional heterogeneity. We argue that an average impact across regions is often not very useful, and that—conditional on the region's characteristics—impacts may differ significantly. We advocate the use of modelling varying level and slope effects, and specifically to cluster them by the use of latent class or finite mixture models (FMMs). Moreover, in order to fully exploit the output from the FMM, we adopt self-organizing maps to understand the composition of the resulting segmentation and as a way to depict the underlying regional similarities that would otherwise be missed if a standard approach was adopted. We apply our proposed method to a case-study of Germany where we show that the regional impact of young age cohorts on the labor market is indeed very heterogeneous across regions and our results are robust against potential endogeneity bias.},
	author = {de Graaff, Thomas and Arribas-Bel, Daniel and Ozgen, Ceren},
	booktitle = {Modelling Aging and Migration Effects on Spatial Labor Markets},
	chapter = {11},
	doi = {10.1007/978-3-319-68563-2_11},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting Unemployment/Graaff et al (2018) Demographic Aging and Employment Dynamics in German Regions Modeling Regional Heterogeneity.pdf:pdf},
	isbn = {978-3-319-68563-2},
	mendeley-groups = {Applied (By Field)/Labor/Forecasting unemployment},
	pages = {211--231},
	publisher = {Springer Cham},
	title = {{Demographic Aging and Employment Dynamics in German Regions: Modeling Regional Heterogeneity}},
	year = {2018}
}

@article{Wozniak2020,
	abstract = {Interdependencies among neighboring regions appear to be important in forming the shape of local labor markets. Nevertheless, only a few studies exist which have applied spatial models to forecast over small spatial units such as cities, districts or counties. The majority of predictions are developed with quarterly or yearly time series for a country or at regional level. The paper presents the above phenomena and deals with the problem of simultaneous forecasting of the unemployment rate over 35 poviats (districts and cities) in one of the Polish provinces. Two extremely different models with spatial dependencies were developed and estimated in this paper: the Spatial Vector Autoregressions (SpVAR) and the Spatial Artificial Neural Network (SpANN). The 13-month out-of-sample forecast is based on high frequency, raw, monthly panel data extracted from 31 local labor offices. The procedure worked out here allows comparing the forecasting performance of spatial models with their non-spatial and seasonal equivalents. The inclusion of a spatial component into the models significantly improves the accuracy of forecasts; however, the overall performance of SpVAR is 30{\%} better than SpANN.},
	author = {Wozniak, Marcin},
	doi = {10.1515/snde-2016-0115},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting Unemployment/Wozniak (2018) Forecasting the unemployment rate over districts with the use of distinct methods.pdf:pdf},
	issn = {15583708},
	journal = {Studies in Nonlinear Dynamics and Econometrics},
	keywords = {Spatial Artificial Neural Network,Spatial Vector Autoregression,labor market forecast,panel data,spatial dependencies},
	mendeley-groups = {Applied (By Field)/Labor/Forecasting unemployment},
	number = {2},
	pages = {657--666},
	title = {{Forecasting the Unemployment Rate Over Districts With the Use of Distinct Methods}},
	volume = {24},
	year = {2020}
}
@article{Aaronson2022,
	abstract = {Leveraging the increasing availability of ”big data” to inform forecasts of labor market activity is an active, yet challenging, area of research. Often, the primary difficulty is finding credible ways with which to consistently identify key elasticities necessary for prediction. To illustrate, we utilize a state-level event-study focused on the costliest hurricanes to hit the U.S. mainland since 2004 in order to estimate the elasticity of initial unemployment insurance (UI) claims with respect to search intensity, as measured by Google Trends. We show that our hurricane-driven Google Trends elasticity leads to superior real-time forecasts of initial UI claims relative to other commonly used models. Our approach is also amenable to forecasting both at the state and national levels, and is shown to be well-calibrated in its assessment of the level of uncertainty for its out-of-sample predictions during the Covid-19 pandemic.},
	author = {Aaronson, Daniel and Brave, Scott A. and Butters, R. Andrew and Fogarty, Michael and Sacks, Daniel W. and Seo, Boyoung},
	doi = {10.1016/j.ijforecast.2021.04.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting Unemployment/Aaronson et al. (2022) Forecasting unemployment insurance claims in realtime with Google Trends.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Google Trends,Hurricanes,Search,Unemployment,Unemployment insurance},
	mendeley-groups = {Applied (By Field)/Labor/Forecasting unemployment},
	number = {2},
	pages = {567--581},
	publisher = {Elsevier B.V.},
	title = {{Forecasting Unemployment Insurance Claims in Realtime With Google Trends}},
	volume = {38},
	year = {2022}
}
@article{Patuelli2012,
	abstract = {The geographical distribution and persistence of regional/local unemployment rates in heterogeneous economies (such as Germany) have been, in recent years, the subject of various theoretical and empirical studies. Several researchers have shown an interest in analyzing the dynamic adjustment processes of unemployment and the average degree of dependence of the current unemployment rates or gross domestic product from the ones observed in the past. In this paper, we present a new econometric approach to the study of regional unemployment persistence, in order to account for spatial heterogeneity and/or spatial autocorrelation in both the levels and the dynamics of unemployment. First, we propose an econometric procedure suggesting the use of spatial filtering techniques as a substitute for fixed effects in a panel estimation framework. The spatial filter computed here is a proxy for spatially distributed region-specific information (e.g., the endowment of natural resources, or the size of the "home market") that is usually incorporated in the fixed effects coefficients. The advantages of our proposed procedure are that the spatial filter, by incorporating region-specific information that generates spatial autocorrelation, frees up degrees of freedom, simultaneously corrects for time-stable spatial autocorrelation in the residuals, and provides insights about the spatial patterns in regional adjustment processes. We present several experiments in order to investigate the spatial pattern of the heterogeneous autoregressive coefficients estimated for unemployment data for German NUTS-3 regions. We find widely heterogeneous but generally high persistence in regional unemployment rates. {\textcopyright} 2012, Wiley Periodicals, Inc.},
	author = {Patuelli, Roberto and Schanne, Norbert and Griffith, Daniel A. and Nijkamp, Peter},
	doi = {10.1111/j.1467-9787.2012.00759.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting Unemployment/Patuelli et al (2012) Persistence of Regional Unemployment Application of a Spatial Filtering Approach to Local Labor Markets in Germany.pdf:pdf},
	issn = {00224146},
	journal = {Journal of Regional Science},
	mendeley-groups = {Applied (By Field)/Labor/Forecasting unemployment},
	number = {2},
	pages = {300--323},
	title = {{Persistence of Regional Unemployment: Application of a Spatial Filtering Approach to Local Labor Markets in Germany}},
	volume = {52},
	year = {2012}
}

@article{Politis1994stationaryBootstrap,
	author = {Politis, Dimitris N. and Romano, Joseph P.},
	doi = {10.1080/01621459.1994.10476870},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Bootstrap/Politis, Romano (1994). The Stationary Bootstrap.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {approximate confidence limit,time series},
	mendeley-groups = {Resampling/Bootstrap},
	number = {428},
	pages = {1303--1313},
	title = {{The Stationary Bootstrap}},
	volume = {89},
	year = {1994}
}
@article{Schanne2010,
	abstract = {We forecast unemployment levels for the 176 German labour-market districts on a monthly basis. Because of their small sizes, strong spatial interdependencies exist between these regional units. To account for these, as well as for the heterogeneity in the regional development over time, we apply different versions of a univariate spatial GVAR model. When comparing the forecast precision with that of univariate time series methods, we find that the spatial model does indeed perform better, or at least as well. Hence, the spatial GVAR model provides an alternative or complementary approach to commonly used methods in regional forecasting which do not consider regional interdependencies. {\textcopyright} 2009 International Institute of Forecasters.},
	author = {Schanne, Norbert and Wapler, R{\"{u}}diger and Weyh, Antje},
	doi = {10.1016/j.ijforecast.2009.07.002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting Unemployment/Schanne et al (2010) Regional unemployment forecasts with spatial interdependencies.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Forecasting practice,Labour-market forecasting,Macroeconomic forecasting,Regional forecasting,Time series},
	mendeley-groups = {Applied (By Field)/Labor/Forecasting unemployment},
	number = {4},
	pages = {908--926},
	publisher = {Elsevier B.V.},
	title = {{Regional Unemployment Forecasts with Spatial Interdependencies}},
	volume = {26},
	year = {2010}
}

@incollection{Andersen2006,
	abstract = {Volatility has been one of the most active and successful areas of research in time series econometrics and economic forecasting in recent decades. This chapter provides a selective survey of the most important theoretical developments and empirical insights to emerge from this burgeoning literature, with a distinct focus on forecasting applications. Volatility is inherently latent, and Section 1 begins with a brief intuitive account of various key volatility concepts. Section 2 then discusses a series of different economic situations in which volatility plays a crucial role, ranging from the use of volatility forecasts in portfolio allocation to density forecasting in risk management. Sections 3-5 present a variety of alternative procedures for univariate volatility modeling and forecasting based on the GARCH, stochastic volatility and realized volatility paradigms, respectively. Section 6 extends the discussion to the multivariate problem of forecasting conditional covariances and correlations, and Section 7 discusses volatility forecast evaluation methods in both univariate and multivariate cases. Section 8 concludes briefly. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	author = {Andersen, Torben G. and Bollerslev, Tim and Christoffersen, Peter F. and Diebold, Francis X.},
	booktitle = {Handbook of Economic Forecasting},
	doi = {10.1016/S1574-0706(05)01015-3},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Financial Econometrics/Volatility Forecasting/Andersen, Bollerslev, Christoffersen, Diebold (2006) Volatility and Correlation Forecasting.pdf:pdf},
	isbn = {9780444513953},
	issn = {15740706},
	keywords = {GARCH,covariance forecasting,realized volatility,stochastic volatility,volatility modeling},
	mendeley-groups = {Financial Econometrics/Volatilty Forecasting},
	pages = {777--878},
	title = {{Volatility and Correlation Forecasting}},
	year = {2006}
}
@incollection{Hansen2012rv,
	abstract = {This article focuses on some aspects of high-frequency data and their use in volatility forecasting. High-frequency data can be used to construct volatility forecasts. The article reviews two leading approaches to this. One approach is the reduced-form forecast, where the forecast is constructed from a time series model for realized measures, or a simple regression-based approach such as the heterogeneous autoregressive model. The other is based on more traditional discrete-time volatility models that include a modeling of returns. Such models can be generalized to utilize information provided by realized measures. The article also discusses how volatility forecasts, produced by complex volatility models, can benefit from highfrequency data in an indirect manner, through the use of realized measures to facilitate and improve the estimation of complex models.},
	author = {Hansen, Peter Reinhard and Lunde, Asger},
	booktitle = {The Oxford Handbook of Economic Forecasting},
	chapter = {15},
	doi = {10.1016/j.iref.2019.10.014},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Financial Econometrics/Volatility Forecasting/Hansen, Lunde (2011) Forecasting Volatility Using High-Frequency Data.pdf:pdf},
	keywords = {Economic forecasting,High-frequency data,Volatility forecasting},
	mendeley-groups = {Financial Econometrics/Volatilty Forecasting},
	number = {April 2018},
	pages = {525--556},
	title = {{Forecasting Volatility Using High-Frequency Data}},
	year = {2012}
}
@article{Corsi2009,
	abstract = {The paper proposes an additive cascade model of volatility components defined over different time periods. This volatility cascade leads to a simple AR-type model in the realized volatility with the feature of considering different volatility components realized over different time horizons and thus termed Heterogeneous Autoregressive model of Realized Volatility (HAR-RV). In spite of the simplicity of its structure and the absence of true long-memory properties, simulation results show that the HAR-RV model successfully achieves the purpose of reproducing the main empirical features of financial returns (long memory, fat tails, and self-similarity) in a very tractable and parsimonious way. Moreover, empirical results show remarkably good forecasting performance. {\textcopyright} The Author 2009. Published by Oxford University Press. All rights reserved.},
	author = {Corsi, Fulvio},
	doi = {10.1093/jjfinec/nbp001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Financial Econometrics/Volatility Forecasting/Corsi (2008) A Simple Approximate Long-Memory Model of Realized Volatility.pdf:pdf},
	issn = {14798409},
	journal = {Journal of Financial Econometrics},
	keywords = {High-frequency data,Long-memory models,Realized volatility,Volatility forecast},
	mendeley-groups = {Financial Econometrics/Volatilty Forecasting},
	number = {2},
	pages = {174--196},
	title = {{A Simple Approximate Long-Memory Model of Realized Volatility}},
	volume = {7},
	year = {2009}
}
@article{Clements2021,
	abstract = {The standard heterogeneous autoregressive (HAR) model is perhaps the most popular benchmark model for forecasting return volatility. It is often estimated using raw realized variance (RV) and ordinary least squares (OLS). However, given the stylized facts of RV and well-known properties of OLS, this combination should be far from ideal. The aim of this paper is to investigate how the predictive accuracy of the HAR model depends on the choice of estimator, transformation, or combination scheme made by the market practitioner. In an out-of-sample study, covering the S{\&}P 500 index and 26 frequently traded NYSE stocks, it is found that simple remedies systematically outperform not only standard HAR but also state of the art HARQ forecasts.},
	author = {Clements, Adam and Preve, Daniel P.A.},
	doi = {10.1016/j.jbankfin.2021.106285},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Financial Econometrics/Volatility Forecasting/Clements, Preve (2021) A Practical Guide to Harnessing the HAR Volatility Model.pdf:pdf},
	issn = {03784266},
	journal = {Journal of Banking and Finance},
	keywords = {Box-Cox transformation,Forecast comparisons,HAR,HARQ,MSE,Model confidence set,QLIKE,Realized variance,Robust regression,VaR,Volatility forecasting,Weighted least squares},
	mendeley-groups = {Financial Econometrics/Volatilty Forecasting},
	pages = {106285},
	publisher = {Elsevier B.V.},
	title = {{A Practical Guide to Harnessing the HAR Volatility Model}},
	volume = {133},
	year = {2021}
}

@book{Stevens2020DeepLearningPyTorch,
  title = {Deep {{Learning}} with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca Pietro Giovanni and Viehmann, Thomas},
  year = {2020},
  publisher = {Manning},
  address = {Shelter Island, NY},
  isbn = {978-1-61729-526-3 978-1-63835-407-9},
  langid = {english}
}

@inproceedings{Zhu2017UnpairedImagetoImageTranslation,
  title = {Unpaired {{Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  month = oct,
  pages = {2242--2251},
  publisher = {IEEE},
  address = {Venice},
  doi = {10.1109/ICCV.2017.244},
  urldate = {2025-08-05},
  isbn = {978-1-5386-1032-9}
}

@misc{Chernozhukov2024AppliedCausalInference,
  title = {Applied {{Causal Inference Powered}} by {{ML}} and {{AI}}},
  author = {Chernozhukov, Victor and Hansen, Christian and Kallus, Nathan and Spindler, Martin and Syrgkanis, Vasilis},
  year = {2024},
  month = mar,
  number = {arXiv:2403.02467},
  eprint = {2403.02467},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.02467},
  urldate = {2025-06-25},
  abstract = {An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\moren\\Zotero\\storage\\XFAQDA9D\\Chernozhukov et al. - 2024 - Applied Causal Inference Powered by ML and AI.pdf;C\:\\Users\\moren\\Zotero\\storage\\IB5FR66F\\2403.html}
}

@book{Gaillac2025MachineLearningEconometrics,
  title = {Machine {{Learning}} for {{Econometrics}}},
  author = {Gaillac, Christophe and L'Hour, Jeremy},
  year = {2025},
  publisher = {Oxford},
  address = {Oxford University Press},
  abstract = {Machine Learning for Econometrics is a book for economists seeking to grasp modern machine learning techniques - from their predictive performance to the revolutionary handling of unstructured data - in order to establish causal relationships from data},
  isbn = {978-0-19-891882-0},
  langid = {english}
}

@book{Shalev-Shwartz2014UnderstandingMachineLearning,
  title = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  edition = {1st ed},
  publisher = {Cambridge University Press},
  address = {West Nyack},
  isbn = {978-1-107-29801-9},
  langid = {english}
}

@misc{Balduzzi2018ShatteredGradientsProblem,
  title = {The {{Shattered Gradients Problem}}: {{If Resnets Are}} the {{Answer}}, {{Then What Is}} He {{Question}}?},
  shorttitle = {The {{Shattered Gradients Problem}}},
  author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and McWilliams, Brian},
  year = {2018},
  month = jun,
  number = {arXiv:1702.08591},
  eprint = {1702.08591},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08591},
  urldate = {2025-06-27},
  abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite wellchosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new ``looks linear'' (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\moren\Zotero\storage\DRHLZ4JS\Balduzzi et al. - 2018 - The Shattered Gradients Problem If resnets are the answer, then what is the question.pdf}
}


@book{Geron2023HandsonMachineLearning,
  title = {Hands-on {{Machine Learning With Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2023},
  series = {Data Science / Machine Learning},
  edition = {Third edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {"Through a recent series of breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This best-selling book uses concrete examples, minimal theory, and production-ready Python frameworks--scikit-learn, Keras, and TensorFlow--to help you gain an intuitive understanding of the concepts and tools for building intelligent systems. With this updated third edition, author Aurelien Geron explores a range of techniques, starting with simple linear regression and progressing to deep neural networks. Numerous code examples and exercises throughout the book help you apply what you've learned. Programming experience is all you need to get started"--},
  isbn = {978-1-0981-2597-4},
  langid = {english},
  file = {C:\Users\moren\Zotero\storage\35C3ZF5M\Géron - 2023 - Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow concepts, tools, and techniques.pdf}
}

@article{Wolpert1996LackPrioriDistinctions,
  title = {The {{Lack}} of {{A Priori Distinctions Between Learning Algorithms}}},
  author = {Wolpert, David H.},
  year = {1996},
  journal = {Neural Computation},
  volume = {8},
  number = {7},
  pages = {1341--1390},
  doi = {10.1162/neco.1996.8.7.1341},
  abstract = {This is the first of two papers that use off-training set \{(OTS)\} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no \{{\textbackslash}backslashtextbackslash\}textita priori distinctions between learning algorithms. \{(The\} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are ``as many'' targets (or priors over targets) for which A has lower expected \{OTS\} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is ``anti-cross-validation'' (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the \{Vapnik-Chervonenkis\} dimension of your generalizer is small; and the training set is large, then with high probability your \{OTS\} error is small. Other implications for ``membership queries'' algorithms and ``punting'' algorithms are also discussed.},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\Learning Frameworks and Learnability\Wolpert - 1996 - The Lack of A Priori Distinctions Between Learning Algorithms.pdf}
}

@article{Halevy2009UnreasonableEffectivenessData,
  title = {The {{Unreasonable Effectiveness}} of {{Data}}},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  year = {2009},
  journal = {IEEE Intelligent Systems},
  volume = {24},
  number = {2},
  pages = {8--12},
  doi = {10.1109/MIS.2009.36},
  isbn = {9780000000002},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Machine Learning\NLP\Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf}
}

@book{Mohri2018FoundationsMachineLearning,
  title = {Foundations of {{Machine Learning}}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  publisher = {The MIT Press},
  doi = {10.5555/3360093},
  isbn = {978-0-262-03940-6}
}

@article{Burkhardt2019,
	abstract = {We estimate the effect of short-term air pollution exposure (PM2.5 and ozone) on several categories of crime, with a particular emphasis on aggressive behavior. To identify this relationship, we combine detailed daily data on crime, air pollution, and weather for an eight-year period across the United States. Our primary identification strategy employs extremely high dimensional fixed effects and we perform a series of robustness checks to address confounding variation between temperature and air pollution. We find a robust positive effect of increased air pollution on violent crimes, and specifically assaults, but no relationship between increases in air pollution and property crimes. The effects are present in and out of the home, at levels well below Ambient Air Pollution Standards, and PM2.5 effects are strongest at lower temperatures. The results suggest that a 10{\%} reduction in daily PM2.5 and ozone could save {\$}1.4 billion in crime costs per year, a previously overlooked cost associated with pollution.},
	author = {Burkhardt, Jesse and Bayham, Jude and Wilson, Ander and Carter, Ellison and Berman, Jesse D. and O'Dell, Katelyn and Ford, Bonne and Fischer, Emily V. and Pierce, Jeffrey R.},
	doi = {10.1016/j.jeem.2019.102267},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Environmental/Pollution/Burkhardt et al. (2019) The effect of pollution on crime Evidence from data on particulate matter and ozone.pdf:pdf},
	issn = {10960449},
	journal = {Journal of Environmental Economics and Management},
	keywords = {Air pollution,Crime,Wildfire smoke},
	mendeley-groups = {Economics/Environmental/Pollution},
	pages = {102267},
	publisher = {Elsevier Inc.},
	title = {{The Effect of Pollution on Crime: Evidence from Data on Particulate Matter and Ozone}},
	volume = {98},
	year = {2019}
}
@article{Kitamura2018,
	abstract = {This paper develops and implements a nonparametric test of Random Utility Models. The motivating application is to test the null hypothesis that a sample of cross-sectional demand distributions was generated by a population of rational consumers. We test a necessary and sufficient condition for this that does not rely on any restriction on unobserved heterogeneity or the number of goods. We also propose and implement a control function approach to account for endogenous expenditure. An econometric result of independent interest is a test for linear inequality constraints when these are represented as the vertices of a polyhedron rather than its faces. An empirical application to the U.K. Household Expenditure Survey illustrates computational feasibility of the method in demand problems with 5 goods.},
	archivePrefix = {arXiv},
	arxivId = {1606.04819},
	author = {Kitamura, Yuichi and Stoye, J{\"{o}}rg},
	doi = {10.3982/ecta14478},
	eprint = {1606.04819},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Demand Estimation/Kitamura, Stoye (2018. Nonparametric Analysis of Random Utility Models.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Economics/Consumption},
	number = {6},
	pages = {1883--1909},
	title = {{Nonparametric Analysis of Random Utility Models}},
	volume = {86},
	year = {2018}
}

@article{Evdokimov2012,
	abstract = {This note demonstrates that the conditions of Kotlarski's (1967, Pacific Journal of Mathematics 20(1), 69-76) lemma can be substantially relaxed. In particular, the condition that the characteristic functions of M, U 1, and U 2 are nonvanishing can be replaced with much weaker conditions: The characteristic function of U 1 can be allowed to have real zeros, as long as the derivative of its characteristic function at those points is not also zero; that of U 2 can have an isolated number of zeros; and that of M need satisfy no restrictions on its zeros. We also show that Kotlarski's lemma holds when the tails of U 1 are no thicker than exponential, regardless of the zeros of the characteristic functions of U 1, U 2, or M. {\textcopyright} Copyright Cambridge University Press 2012.},
	author = {Evdokimov, Kirill and White, Halbert},
	doi = {10.1017/S0266466611000831},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Deconvolution/Evdokimov, White (2012) Some Extensions of a Lemma of Kotlarski.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Nonparametric Estimation/Density Estimation/Deconvolution},
	number = {4},
	pages = {925--932},
	title = {{Some Extensions of a Lemma of Kotlarski}},
	volume = {28},
	year = {2012}
}

@article{Lewbel2024IdentificationTriangularTwo,
	title = {Identification of a {{Triangular Two Equation System Without Instruments}}},
	author = {Lewbel, Arthur and Schennach, Susanne M. and Zhang, Linqi},
	year = {2024},
	month = jan,
	journal = {Journal of Business \& Economic Statistics},
	volume = {42},
	number = {1},
	pages = {14--25},
	publisher = {Informa UK Limited},
	issn = {0735-0015, 1537-2707},
	doi = {10.1080/07350015.2023.2166052},
	urldate = {2025-01-14},
	langid = {english},
	keywords = {Deconvolution,Identification,Kotlarski,Returns to schooling,Triangular Systems},
	file = {C:\Users\moren\Downloads\Identification of a Triangular Two Equation System Without Instruments.pdf}
}

@article{Lewbel2022,
	abstract = {This note extends the Kotlarski (1967) Lemma to show exactly what is identified when we allow for an unknown factor loading on the common unobserved factor. That is, this note completely characterizes identification of the model Y = cV + U and X = V + W, where the joint distribution of Y and X is known, while the constant c and the mutually independent random variables V, U, and W are unobserved. Potential applications include measurement error models and panel data factor models.},
	author = {Lewbel, Arthur},
	doi = {10.1016/j.jeconom.2020.12.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Deconvolution/Lewbel (2022) Kotlarski With a Factor Loading.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Deconvolution,Factor models,Kotlarski,Measurement error},
	mendeley-groups = {Nonparametric Estimation/Density Estimation/Deconvolution},
	number = {1},
	pages = {176--179},
	title = {{Kotlarski with a Factor Loading}},
	volume = {229},
	year = {2022}
}

@unpublished{Morozov2023jmp,
	author = {Morozov, Vladislav},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Distribution Of Derivatives Via Moments/derivativeMoments.pdf:pdf},
	keywords = {heterogeneous panels,marginal effects,moment problem,nonparametric identification},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Panel},
	pages = {1--118},
	title = {{Estimating the Moments and the Distribution of Heterogeneous Marginal Effects Using Panel Data}}, 
	year = {2024}
}

@article{McFadden2005,
	abstract = {The problem of revealed stochastic preference is whether probability distributions of observed choices in a population for various choice situations are consistent with a hypothesis of maximization of preference preorders by members of the population. This is a population analog of the classical revealed preference problem in economic consumer theory. This paper synthesizes the solutions to this problem that have been obtained by Marcel K. Richter and the author, and by J. C. Falmagne, in the case of finite sets of alternatives, and utilizes unpublished research of Richter and the author to give results for the non-finite choice sets encountered in economic consumer theory.},
	author = {McFadden, Daniel L.},
	doi = {10.1007/s00199-004-0495-3},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Consumption/Theory/McFadden (2005) Revealed Stochastic Preference A Synthesis.pdf:pdf},
	issn = {09382259},
	journal = {Economic Theory},
	keywords = {Choice,Random utility maximization,Revealed preference,Stochastic preference},
	mendeley-groups = {Economics/Consumption/Theory},
	number = {2},
	pages = {245--264},
	title = {{Revealed Stochastic Preference: A Synthesis}},
	volume = {26},
	year = {2005}
}

@unpublished{Morozov2023,
	archivePrefix = {arXiv},
	arxivId = {2210.08524},
	author = {Morozov, Vladislav},
	eprint = {2210.08524},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Extreme Quantiles of Unobservables/Text and Slides/Text Version 3/noisyExtremeMS.pdf:pdf},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	pages = {1--60},
	title = {{Inference on Extreme Quantiles of Unobserved Individual Heterogeneity}},
	year = {2023}
}

@unpublished{Morozov2023deconvolution,
	archivePrefix = {arXiv},
	arxivId = {2210.08524},
	author = {Morozov, Vladislav},
	eprint = {2210.08524},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Extreme Quantiles of Unobservables/Text and Slides/Text Version 3/noisyExtremeMS.pdf:pdf},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	pages = {1--60},
	title = {{Deconvolution Estimation and Inference on the Distribution of Heterogeneous Marginal Effects Using Panel Data}},
	year = { }
}


@unpublished{MorozovSy2023,
	archivePrefix = {arXiv},
	arxivId = {2210.08524},
	author = {Morozov, Vladislav and Sy, Andrea},
	eprint = {2210.08524},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Notes/Extreme Quantiles of Unobservables/Text and Slides/Text Version 3/noisyExtremeMS.pdf:pdf},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	pages = {1--60},
	title = {{Distribution Equality Tests with Noisy Observations}},
	year = { }
}

@article{Pritsker1997,
	author = {Pritsker, Igor E. and Varga, Richard S.},
	doi = {10.1090/S0002-9947-97-01889-8},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Complex Analysis/Pritsker, Varga (1997) The Szego curve, zero distribution and weighted approximation.pdf:pdf},
	journal = {Transactions of the Americal Mathematical Society},
	mendeley-groups = {Random Useful Math/Complex Analysis},
	number = {10},
	pages = {4085--4105},
	title = {{Zero Distribution, the Szego Curve, and Weighted Polynomial Approximation in the Complex Plane}},
	volume = {349},
	year = {1997}
}

@article{Wahba1990,
	abstract = {We investigate the behavior of the optimal regularization parameter in the method of regularization for solving first kind integral equations with noisy data, under a range of definitions of “optimal”, varying from mean square error in higher derivatives of the solution, to mean square error in the predicted data. We study how the optimal regularization parameter changes when the optimality criteria changes, under a broad range of smoothness assumptions on the solution, the kernel of the integral operator, and the penalty functional. Although some of the calculations we present have been given elsewhere, we organize the results with a specific goal in mind. That is, we study a certain class of problems within which we can identify conditions on the solution, the kernel of the operator and the penalty functional for which the rate at which the optimal regularization parameter goes to zero is the same for both predictive mean square error and solution mean square error optimality criteria, and for which it is different. The former circumstances are of interest because then data based estimates of the regularization parameter such as generalized cross-validation, which are known to be optimal for predictive mean square error, will also go to zero at the optimal rate for solution mean square error. {\textcopyright} 1990, Taylor {\&} Francis Group, LLC. All rights reserved.},
	author = {Wahba, Grace and Wang, Yonghua},
	doi = {10.1080/03610929008830285},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Smoothing Parameter Choice/Wahba, Wang (1990) When is the optimal regularization parameter insensitive to the choice of the loss function.pdf:pdf},
	issn = {1532415X},
	journal = {Communications in Statistics - Theory and Methods},
	keywords = {convergence rates,decon-,method of regularization,optimal smoothing parameter,volution},
	mendeley-groups = {Nonparametric Estimation/Smoothing Parameter Choice},
	number = {5},
	pages = {1685--1700},
	title = {{When is the Optimal Regularization Parameter Insensitive to the Choice of the Loss Function ?}},
	volume = {19},
	year = {1990}
}

@article{Blundell1988,
	author = {Blundell, Richard W.},
	doi = {10.2307/2233510},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Consumption/Blundell (1988) Consumer Behaviour Theory and Empirical Evidence A Survey.pdf:pdf},
	journal = {The Economic Journal},
	mendeley-groups = {Applied (By Field)/Consumption},
	number = {389},
	pages = {16--65},
	title = {{Consumer Behaviour: Theory and Empirical Evidence -- A Survey}},
	volume = {98},
	year = {1988}
}
 
@article{Horowitz2011AppliedNonparametricInstrumental,
  title = {Applied {{Nonparametric Instrumental Variables Estimation}}},
  author = {Horowitz, Joel L.},
  year = {2011},
  journal = {Econometrica},
  volume = {79},
  number = {2},
  pages = {347--394},
  issn = {0012-9682},
  doi = {10.3982/ecta8662},
  abstract = {Instrumental variables are widely used in applied econometrics to achieve identification and carry out estimation and inference in models that contain endogenous explanatory variables. In most applications, the function of interest (e.g., an Engel curve or demand function) is assumed to be known up to finitely many parameters (e.g., a linear model), and instrumental variables are used to identify and estimate these parameters. However, linear and other finite-dimensional parametric models make strong assumptions about the population being modeled that are rarely if ever justified by economic theory or other a priori reasoning and can lead to seriously erroneous conclusions if they are incorrect. This paper explores what can be learned when the function of interest is identified through an instrumental variable but is not assumed to be known up to finitely many parameters. The paper explains the differences between parametric and nonparametric estimators that are important for applied research, describes an easily implemented nonparametric instrumental variables estimator, and presents empirical examples in which nonparametric methods lead to substantive conclusions that are quite different from those obtained using standard, parametric estimators. {\copyright} 2011 The Econometric Society.},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Endogeneity\Horowitz (2011) Applied Nonparametric Instrumental Variables Estimation.pdf}
}


@article{Charnigo2015,
	abstract = {We propose a new multivariate generalized Cp (MGCp) criterion for tuning parameter selection in nonparametric regression, applicable when there are multiple covariates whose values may be irregularly spaced. Apart from an asymptotically negligible remainder, the MGCp criterion has expected value equal to the sum of squared errors of a fitted derivative (rather than of a fitted mean response). Thus, unlike traditional criteria for tuning parameter selection, MGCp is not prone to undersmoothed derivative estimation. We illustrate a scientific application in a case study that explores the relationship among three measures of liver function. Since recent technological developments hold promise for assessing two of these measures outside of medical and laboratory facilities, better understanding of the aforementioned relationship may allow enhanced monitoring of liver function, especially in developing countries and among persons for whom access to medical and laboratory facilities is limited.},
	author = {Charnigo, Richard and Srinivasan, Cidambi},
	doi = {10.1093/biostatistics/kxu042},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Smoothing Parameter Choice/Charnigo, Srinivasan (2011) A multivariate generalized Cp and surface estimation.pdf:pdf},
	issn = {14684357},
	journal = {Biostatistics},
	keywords = {Bilirubin,Compound estimation,Curve estimation,Functional data analysis,Liver disease,Nonparametric regression,Tuning parameter},
	mendeley-groups = {Nonparametric Estimation/Smoothing Parameter Choice},
	number = {2},
	pages = {311--325},
	pmid = {25187530},
	title = {{A Multivariate Generalized Cp and Surface Estimation}},
	volume = {16},
	year = {2015}
}

@article{Masry1996,
	abstract = {Local high-order polynomial fitting is employed for the estimation of the multivariate regression function m(x1, . . ., xd) = E{\{}$\psi$(Yd)|X1 = x1, . . ., Xd = xd{\}}, and of its partial derivatives, for stationary random processes {\{}Yi, Xi{\}}. The function $\psi$ may be selected to yield estimates of the conditional mean, conditional moments and conditional distributions. Uniform strong consistency over compact subsets of Rd, along with rates, are established for the regression function and its partial derivatives for strongly mixing processes.},
	author = {Masry, Elias},
	doi = {10.1111/j.1467-9892.1996.tb00294.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/LP/Masry (1996) MULTIVARIATE LOCAL POLYNOMIAL REGRESSION FOR TIME SERIES UNIFORM STRONG CONSISTENCY AND RATES.pdf:pdf},
	journal = {Journal of Time Series Analysis},
	keywords = {Local polynomial fitting,Mixing processes,Multivariate regression estimation,Rates of convergence,Uniform strong consistency},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression/LP},
	number = {6},
	pages = {571--599},
	title = {{Multivariate Local Polynomial Regression for Time Series: Uniform Strong Consistency and Rates}},
	volume = {17},
	year = {1996}
}
@article{Masry1996b,
	author = {Masry, Elias},
	doi = {10.1016/S0304-4149(96)00095-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/LP/Masry (1996) Multivariate regression estimation Local polynomial fitting for time series.pdf:pdf},
	journal = {Stochastic Processes and their Applications},
	keywords = {Joint asymptotic normality,Local polynomial fitting of arbitrary order,Mixing processes,Multivariate regression estimation,Uniform rates of almost sure convergence},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression/LP},
	number = {1},
	pages = {3575--3581},
	title = {{Multivariate Regression Estimation: Local Polynomial Fitting for Time Series}},
	volume = {65},
	year = {1996}
}


@article{Palma2020,
	abstract = {The simulated choice probabilities in mixed logit models are usually approximated numerically using Halton or random draws from a multivariate mixing distribution for the random parameters. Theoretically, the order in which the estimated variables enter the model should not matter. However, in practice, simulation “noise” inherent in the numerical procedure leads to differences in the magnitude of the estimated coefficients depending on the arbitrary order in which the random variables are estimated. The problem is exacerbated when a low number of draws are used or if correlation among coefficients is allowed. In particular, the Cholesky factorization procedure, which is used to incorporate correlation into the model, propagates simulation noise in the estimate of one coefficient to estimates of all subsequent coefficients in the model. Ignoring the potential ordering effects in simulated maximum likelihood estimation methods may seriously compromise the ability of replicating the results and can inadvertently influence policy recommendations. We find that better estimation accuracy is achieved with Halton draws using small prime numbers as it is the case for small integrating dimensions; but random draws provide better accuracy than Halton draws from large prime numbers as it is normally the case in high integrating dimensions. With correlation, the standard deviations have very large fluctuations depending on the order of the variables, affecting the conclusions regarding heterogeneity of preferences.},
	author = {Palma, Marco A. and Vedenov, Dmitry V. and Bessler, David},
	doi = {10.1007/s00181-018-1609-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Biases/Palma et al. (2020) The order of variables, simulation noise, and accuracy.pdf:pdf},
	issn = {14358921},
	journal = {Empirical Economics},
	keywords = {C25,C63,Cholesky,Halton draws,Maximum simulated likelihood,Random draws,Simulated noise},
	mendeley-groups = {Discrete Choice/Biases},
	number = {5},
	pages = {2049--2083},
	publisher = {Springer Berlin Heidelberg},
	title = {{The Order of Variables, Simulation Noise, and Accuracy of Mixed Logit Estimates}},
	volume = {58},
	year = {2020}
}

@article{Hahn2022,
	abstract = {We propose to reduce asymptotic biases of simulated maximum likelihood estimators (SMLE) by using a jackknife method similar to Dhaene and Jochmans (2015). Because the jackknife method does not require an explicit characterization of the bias, it may be a practically attractive alternative to Lee's (1995) estimator.},
	author = {Hahn, Jinyong and Liu, Xueyuan},
	doi = {10.1016/j.econlet.2022.110784},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Biases/Hahn, Lee (2022) Jackknife bias reduction for simulated maximum likelihood estimator.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {Simulated maximum likelihood estimator,Split sample jackknife},
	mendeley-groups = {Discrete Choice/Biases},
	pages = {110784},
	publisher = {Elsevier B.V.},
	title = {{Jackknife Bias Reduction for Simulated Maximum Likelihood Estimator of Discrete Choice Models}},
	volume = {219},
	year = {2022}
}
@article{Bastin2010,
	abstract = {Maximum simulated likelihood (MSL) procedure is generally adopted in discrete choice analysis to solve complex models without closed mathematical formulation. This procedure differs from the maximum likelihood simply because simulated probabilities are inserted into the Log-Likelihood (LL) function. The LL function to be maximized is the sum of the logarithm of the expected choice probabilities; since the logarithmic operation is a nonlinear transformation bias is then introduced. The simulation bias depends on the number of draws that are used in the simulation and on the sample size. Although the asymptotic properties of the MSL estimator are well known, the question is how simulation bias affects parameters estimation and therefore the main outcomes of choice models (for instance value of travel time and market shares). In this paper, we estimate explicitly the simulation bias in mixed logit parameter estimation, using Taylor expansion and we correct the log-likelihood objective function during the maximization process. The method is developed in the context of Monte Carlo simulation. We report significant error reduction on the final objective value but also on the optimal parameters. The method could be extended to randomized quasi-Monte Carlo techniques as long as standard deviations of simulated choice probabilities are calculated. Computation costs can be neglected when using Monte Carlo draws and even when advanced strategies such as adaptive sampling methodology are in use.},
	author = {Bastin, Fabian and Cirillo, Cinzia},
	doi = {10.1016/S1755-5345(13)70036-8},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Biases/Bastin, Cirillo (2010) Reducing simulation bias in mixed logit model estimation.pdf:pdf},
	issn = {17555345},
	journal = {Journal of Choice Modelling},
	keywords = {Mixed logit,Optimisation bias,Simulation bias},
	mendeley-groups = {Discrete Choice/Biases},
	number = {2},
	pages = {71--88}, 
	title = {{Reducing Simulation Bias in Mixed Logit Model Estimation}},
	volume = {3},
	year = {2010}
}

@article{Kneip2015,
	abstract = {Frontier estimation appears in productivity analysis. Firm's performance is measured by the distance between its output and an optimal production frontier. Frontier estimation becomes difficult if outputs are measured with noise and most approaches rely on restrictive parametric assumptions. This paper contributes to nonparametric approaches, with unknown frontier and unknown variance of a normally distributed error. We propose a nonparametric method identifying and estimating both quantities simultaneously. Consistency and rate of convergence of our estimators are established, and simulations verify the performance of the estimators for small samples. We illustrate our method with data on American electricity companies.},
	author = {Kneip, Alois and Simar, L{\'{e}}opold and {Van Keilegom}, Ingrid},
	doi = {10.1016/j.jeconom.2014.09.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Production Frontier/Kneip, Simar, Van Keleigom (2015) Estimating a Changepoint, Boundary, or Frontier in the Presence of Observation Error.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Deconvolution,Nonparametric estimation,Penalized likelihood,Stochastic frontier estimation},
	mendeley-groups = {Applied (By Econometrics)/Production Frontier Estimation},
	number = {2},
	pages = {379--393},
	publisher = {Elsevier B.V.},
	title = {{Frontier Estimation in the Presence of Measurement error with Unknown Variance}},
	volume = {184},
	year = {2015}
}

@incollection{Kumbhakar2020-2,
	author = {Kumbhakar, Subal C. and Parmeter, Christopher F. and Zelenyuk, Valentin},
	booktitle = {Handbook of Production Economics},
	doi = {10.1007/978-981-10-3450-3_11-1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Production Frontier/Kumbhakar, Parmeter, Zelenyuk (2020) Stochastic Frontier Analysis Foundations and Advances II.pdf:pdf},
	mendeley-groups = {Applied (By Econometrics)/Production Frontier Estimation},
	pages = {1--38},
	title = {{Stochastic Frontier Analysis: Foundations and Advances II}},
	year = {2020}
}
@incollection{Kumbhakar2020-1,
	abstract = {This chapter reviews some of the most important developments in the econo-metric estimation of productivity and efficiency surrounding the stochastic frontier model. We highlight endogeneity issues, recent advances in generalized panel data stochastic fron-tier models, nonparametric estimation of the frontier, quantile estimation and distribution free methods. An emphasis is placed on highlighting recent research and providing broad coverage, while details are left for further reading in the abundant (although not limited to) list of references provided.},
	author = {Kumbhakar, Subal C. and Parmeter, Christopher F. and Zelenyuk, Valentin},
	booktitle = {Handbook of Production Economics},
	doi = {10.1007/978-981-10-3450-3_9-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Production Frontier/Kumbhakar, Parmeter, Zelenyuk (2020) Stochastic Frontier Analysis Foundations and Advances I.pdf:pdf},
	mendeley-groups = {Applied (By Econometrics)/Production Frontier Estimation},
	pages = {1--40},
	title = {{Stochastic Frontier Analysis: Foundations and Advances I}},
	year = {2020}
}

@article{Fan1995,
	author = {Fan, Jianqing and Gijbels, Irene},
	doi = {10.1111/j.2517-6161.1995.tb02034.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Smoothing Parameter Choice/Fan, Gijbels (1995) Data-Driven Bandwidth Selection in Local Polynomial Fitting Variable Bandwidth and Spatial Adaptation.pdf:pdf},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	mendeley-groups = {Nonparametric Estimation/Smoothing Parameter Choice},
	number = {2},
	pages = {371--394},
	title = {{Data-Driven Bandwidth Selection in Local Polynomial Fitting: Variable Bandwidth and Spatial Adaptation}},
	volume = {57},
	year = {1995}
}
@article{Girard2021,
	abstract = {Expectiles define a least squares analogue of quantiles. They have been the focus of a substantial quantity of research in the context of actuarial and financial risk assessment over the last decade. The behaviour and estimation of unconditional extreme expectiles using independent and identically distributed heavy-tailed observations have been investigated in a recent series of papers. We build here a general theory for the estimation of extreme conditional expectiles in heteroscedastic regression models with heavy-tailed noise; our approach is supported by general results of independent interest on residual-based extreme value estimators in heavy-tailed regression models, and is intended to cope with covariates having a large but fixed dimension. We demonstrate how our results can be applied to a wide class of important examples, among which are linear models, single-index models as well as ARMA and GARCH time series models. Our estimators are showcased on a numerical simulation study and on real sets of actuarial and financial data.},
	author = {Girard, St{\'{e}}phane and Stupfler, Gilles and Usseglio-Carleve, Antoine},
	doi = {10.1214/21-AOS2087},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Expectiles/Girard et al. (2021) Extreme conditional expectile estimation in heavy-tailed heteroscedastic regression models.pdf:pdf},
	issn = {21688966},
	journal = {Annals of Statistics},
	keywords = {Expectiles,Extreme value analysis,Heavy-tailed distribution,Heteroscedasticity,Regression models,Residual-based estimators,Single-index model,Tail empirical process of residuals},
	mendeley-groups = {Quantile/Extreme Value/Expectiles},
	number = {6},
	pages = {3358--3382},
	title = {{Extreme Conditional Expectile Estimation In Heavy-Tailed Heteroscedastic Regression Models}},
	volume = {49},
	year = {2021}
}

@unpublished{Goncalves2022,
	abstract = {Many empirical studies estimate impulse response functions that depend on the state of the economy. Most of these studies rely on a variant of the local projection (LP) approach to estimate the state-dependent impulse response functions. Despite its widespread application, the asymptotic validity of the LP approach to estimating state-dependent impulse responses has not been established to date. We formally derive this result for a structural state-dependent vector autoregressive process. The model only requires the structural shock of interest to be identi{\ldots}ed. A su¢ cient condition for the consistency of the state-dependent LP estimator of the response function is that the {\ldots}rst-and second-order conditional moments of the structural shocks are independent of current and future states, given the information available at the time the shock is realized. This rules out models in which the state of the economy is a function of current or future realizations of the outcome variable of interest, as is often the case in applied work. Even when the state is a function of past values of this variable only, consistency may hold only at short horizons.},
	author = {Gon{\c{c}}alves, S{\'{i}}lvia and Herrera, Ana Mar{\'{i}}a and Kilian, Lutz and Pesavento, Elena},
	booktitle = {Federal Reserve Bank of Dallas, Working Papers},
	doi = {10.24149/wp2205},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Local Projections/Gon{\c{c}}alves et al. (2022) When do state-dependent local projections work.pdf:pdf},
	mendeley-groups = {Time Series/Local Projections},
	title = {{When Do State-Dependent Local Projections Work?}},
	year = {2022}
}

@article{Powell2020,
	abstract = {This paper proposes a method to estimate unconditional quantile treatment effects (QTEs) given one or more treatment variables, which may be discrete or continuous, even when it is necessary to condition on covariates. The estimator, generalized quantile regression (GQR), is developed in an instrumental variable framework for generality to permit estimation of unconditional QTEs for endogenous policy variables, but it is also applicable in the conditionally exogenous case. The framework includes simultaneous equations models with nonadditive disturbances, which are functions of both unobserved and observed factors. Quantile regression and instrumental variable quantile regression are special cases of GQR and available in this framework.},
	author = {Powell, David},
	doi = {10.1162/rest_a_00858},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Regression/Powell (2020) Quantile Treatment effects in the presence of covariates.pdf:pdf},
	issn = {15309142},
	journal = {Review of Economics and Statistics},
	mendeley-groups = {Quantile/Fixed Quantile Regression},
	number = {5},
	pages = {994--1005},
	title = {{Quantile Treatment Effects in the Presence of Covariates}},
	volume = {102},
	year = {2020}
}

@unpublished{Wojciechowski2023,
	author = {Wojciechowski, Robert},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Local Projections/Wojciechowski (2023) Understanding the State-Dependent Impact of Financial Shocks on Growth via Unconditional Quantile Impulse Responses.pdf:pdf},
	mendeley-groups = {Time Series/Local Projections},
	pages = {1--21},
	title = {{Understanding the State-Dependent Impact of Financial Shocks on Growth via Unconditional Quantile Impulse Responses}},
	year = {2023}
}

@article{Blundell2003NonparametricEngelCurves,
  title = {Nonparametric {{Engel Curves}} and {{Revealed}} Preference},
  author = {Blundell, Richard W. and Browning, Martin and Crawford, Ian A.},
  year = {2003},
  journal = {Econometrica},
  volume = {71},
  number = {1},
  pages = {205--240},
  issn = {00129682},
  doi = {10.1111/1468-0262.00394},
  abstract = {This paper applies revealed preference theory to the nonparametric statistical analysis of consumer demand. Knowledge of expansion paths is shown to improve the power of non-parametric tests of revealed preference. The tightest bounds on indifference surfaces and welfare measures are derived using an algorithm for which revealed preference conditions are shown to guarantee convergence. Nonparametric Engel curves are used to estimate expansion paths and provide a stochastic structure within which to examine the consistency of household level data and revealed preference theory. An application is made to a long time series of repeated cross-sections from the Family Expenditure Survey for Britain. The consistency of these data with revealed preference theory is examined. For periods of consistency with revealed preference, tight bounds are placed on true cost of living indices.},
  keywords = {Consumer demands,Nonparametric regression,Revealed preference},
  file = {D:\Academic Things\Econometrics\Articles\Applied\Engel Curves\Blundell, Browning,  Crawford (2003) Nonparametric Engel Curves and Revealed Preference.pdf}
}

@article{Blundell2007SemiNonparametricIVEstimation,
  title = {Semi-{{Nonparametric IV Estimation}} of {{Shape-Invariant Engel Curves}}},
  author = {Blundell, Richard W. and Chen, Xiaohong and Kristensen, Dennis},
  year = {2007},
  journal = {Econometrica},
  volume = {75},
  number = {6},
  pages = {1613--1669},
  issn = {00129682},
  doi = {10.1111/j.1468-0262.2007.00808.x},
  abstract = {This paper studies a shape-invariant Engel curve system with endogenous total expenditure, in which the shape-invariant specification involves a common shift parameter for each demographic group in a pooled system of nonparametric Engel curves. We focus on the identification and estimation of both the nonparametric shapes of the Engel curves and the parametric specification of the demographic scaling parameters. The identification condition relates to the bounded completeness and the estimation procedure applies the sieve minimum distance estimation of conditional moment restrictions, allowing for endogeneity. We establish a new root mean squared convergence rate for the nonparametric instrumental variable regression when the endogenous regressor could have unbounded support. Root-n asymptotic normality and semiparametric efficiency of the parametric components are also given under a set of "low-level" sufficient conditions. Our empirical application using the U.K. Family Expenditure Survey shows the importance of adjusting for endogeneity in terms of both the nonparametric curvatures and the demographic parameters of systems of Engel curves. {\copyright} The Econometric Society 2007.},
  keywords = {Bounded completeness,Consumer demands,Nonparametric convergence rate,Nonparametric IV,Root-n semiparametric efficiency,Sieve measure of ill-posedness,Sieve minimum distance},
  file = {D:\Academic Things\Econometrics\Articles\Applied\Engel Curves\Blundell, Chen, Kristensen (2007) Semi-Nonparametric IV Estimation of Shape-Invariant Engel Curves.pdf}
}

@article{Newey2001FlexibleSimulatedMoment,
  title = {Flexible {{Simulated Moment Estimation}} of {{Nonlinear Errors-in-Variables Models}}},
  author = {Newey, Whitney K.},
  year = {2001},
  journal = {The Review of Economics and Statistics},
  volume = {83},
  number = {4},
  pages = {616--627},
  doi = {10.1162/003465301753237704},
  abstract = {Nonlinear regression with measurement error is important for estimation from microeconomic data. One approach to identification and estimation is a causal model, in which the unobserved true variable is predicted by observable variables. This paper details the estimation of such a model using simulated moments and a flexible disturbance distribution. An estimator of the asymptotic variance is given for parametric models. Also, a semiparametric consistency result is given. The value of the estimator is demonstrated in a Monte Carlo study and an application to estimating Engel Curves.},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Measurement Error\Simulation\Newey - 2001 - Flexible Simulated Moment Estimation of Nonlinear Errors-in-Variables Models.pdf}
}


@article{Hausman1991,
	abstract = {Methods of estimation of regression coefficients are proposed when the regression function includes a polynomial in a 'true' regressor which is measured with error. Two sources of additional information concerning the unobservable regressor are considered: either an additional indicator of the regressor (itself measured with error) or instrumental variables which characterize the systematic variation in the true regressor. In both cases, estimators are constructed by relating moments involving the unobserved variables to moments of observables; these relations lead to recursion formulae for computation of the regression coefficients and nuisance parameters (e.g., moments of the measurement error). Consistency and asymptotic normality of the estimated coefficients is demonstrated, and consistent estimators of the asymptotic covariant matrices are provided. {\textcopyright} 1991.},
	author = {Hausman, Jerry A. and Newey, Whitney K. and Ichimura, Hidehiko and Powell, James L.},
	doi = {10.1016/0304-4076(91)90022-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Errors in Variables/Hausman, Newey, Ichimura, Powell (1991) Identification and estimation of polynomial errors-in-variables models.pdf:pdf},
	journal = {Journal of Econometrics},
	mendeley-groups = {Errors in Variables/Nonlinear},
	number = {3},
	pages = {273--295},
	title = {{Identification and Estimation of Polynomial Errors-in-Variables Models}},
	volume = {50},
	year = {1991}
}

@article{Hausman1995b,
	abstract = {The most common solution to the errors in variables problem for the linear regression model is the use of instrumental variable estimation. However, this methodology cannot be applied in the nonlinear regression framework. In this paper we develop consistent estimators for nonlinear regression specifications when errors in variables are present. We apply our methodology to estimation of Engel curves on household data. First, we find that the 'Lesser-Working' specification of budget shares regressed on the log of income or expenditure should be generalized to higher-order terms in log income. Also, we find that errors in variables in either reported income or expenditure should be accounted for. Lastly and perhaps most interesting, we find rather strong support for the Gorman rank restriction on the matrix of coefficients for the polynomial terms in income. {\textcopyright} 1995.},
	author = {Hausman, J. A. and Newey, W. K. and Powell, J. L.},
	doi = {10.1016/0304-4076(94)01602-V},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Errors in Variables/Hausman, Newey, Powell (1995) Nonlinear errors in variables Estimation of some Engel curves.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Engel curves,Errors in variables},
	mendeley-groups = {Errors in Variables/Nonlinear,Applied (By Field)/Consumption/Engel Curves},
	number = {1},
	pages = {205--233},
	title = {{Nonlinear Errors in Variables Estimation of Some Engel Curves}},
	volume = {65},
	year = {1995}
}
@article{Almas2012,
	author = {Alm{\aa}s, Ingvild},
	doi = {10.1257/aer.102.2.1093},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Engel Curves/Alm{\aa}s (2012) International Income Inequality Measuring PPP Bias by Estimating Engel Curves for Food.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {Applied (By Field)/Consumption/Engel Curves,!Extra reading list},
	number = {2},
	pages = {1093--1117},
	title = {{International Income Inequality: Measuring PPP Bias by Estimating Engel Curves for Food}},
	volume = {102},
	year = {2012}
}


@unpublished{Almas2018,
	author = {Alm{\aa}s, Ingvild and Beatty, Timothy K. M. and Crossley, Thomas F.},
	doi = {10.2139/ssrn.3156848},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Engel Curves/Almas, Beatty, Crossley (2018) Lost in Translation What Do Engel Curves Tell Us about the Cost of Living.pdf:pdf},
	mendeley-groups = {Applied (By Field)/Consumption/Engel Curves},
	pages = {1--56},
	title = {{Lost in Translation: What Do Engel Curves Tell Us About the Cost of Living?}},
	year = {2018}
}
@article{Lopez-laborda2021,
	author = {L{\'{o}}pez-laborda, Julio and Mar{\'{i}}n-Gonz{\'{a}}lez, Carmen and Onrubia, Jorge},
	doi = {10.1080/02664763.2020.1796933},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Engel Curves/Lopez-Laborda (2017) Estimating Engel curves A new way to improve the SILC-HBS.pdf:pdf},
	journal = {Journal of Applied Statistics},
	mendeley-groups = {Applied (By Field)/Consumption/Engel Curves},
	number = {16},
	pages = {3233--3250},
	title = {{Estimating Engel Curves: a New Way to Improve the SILC-HBS Matching Process Using GLM Methods}},
	volume = {48},
	year = {2021}
}
@article{Chai2010,
	author = {Chai, Andreas and Moneta, Alessio},
	doi = {10.1257/jep.24.1.225},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Engel Curves/Chai, Moneta (2010) Engel Curves Retrospectives.pdf:pdf},
	journal = {The Journal of Economic Perspectives},
	mendeley-groups = {Applied (By Field)/Consumption/Engel Curves},
	number = {1},
	pages = {225--240},
	title = {{Retrospectives: Engel Curves}},
	volume = {24},
	year = {2010}
}
@incollection{Lewbel2008,
	author = {Lewbel, Arthur},
	booktitle = {The New Palgrave Dictionary of Economics},
	doi = {10.1057/978-1-349-95121-5_525-1},
	editor = {Durlauf, Steven N. and Blume, Lawrence E.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Engel Curves/Lewbel (2006) Engel Curves Dictionary.pdf:pdf},
	mendeley-groups = {Applied (By Field)/Consumption/Engel Curves},
	pages = {1--7},
	publisher = {Palgrave Macmillan, London},
	title = {{Engel Curve}},
	year = {2008}
}

@article{McFadden2000,
	author = {McFadden, Daniel and Train, Kenneth},
	doi = {10.1002/1099-1255(200009/10)15:5<447::AID-JAE570>3.0.CO;2-1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Multiple/McFadden, Train (2000) Mixed MNL models for discrete response.pdf:pdf},
	journal = {Journal of Applied Econometrics},
	mendeley-groups = {Discrete Choice/Multinomial},
	number = {5},
	pages = {447--470},
	title = {{Mixed MNL Models for Discrete Response}},
	volume = {15},
	year = {2000}
}

@article{Banks1997QuadraticEngelCurve,
  title = {Quadratic {{Engel Curve}} and {{Consumer Demand}}},
  author = {Banks, James and Blundell, Richard W. and Lewbel, Arthur},
  year = {1997},
  journal = {Review of Economics and Statistics},
  volume = {79},
  number = {4},
  pages = {527--539},
  doi = {10.1162/003465397557015},
  file = {D:\Academic Things\Econometrics\Articles\Applied\Engel Curves\Banks, Blundell, Lewbel (1997) Quadratic Engel Curves and Consumer Demand.pdf}
}


@article{Wendel1961,
	author = {Wendel, J. G.},
	doi = {10.1214/aoms/1177705164},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Characteristic Functions/Wendel (1961) The Non-Absolute Convergence of Gil-Pelaez Inversion Integral.pdf:pdf},
	issn = {0003-4851},
	journal = {The Annals of Mathematical Statistics},
	mendeley-groups = {Random Useful Math/Probability/Characteristic Functions},
	number = {1},
	pages = {338--339},
	title = {{The Non-Absolute Convergence of Gil-Pelaez' Inversion Integral}},
	volume = {32},
	year = {1961}
}

@article{Gil-Pelaez1951,
	author = {Gil-Pelaez, J.},
	doi = {10.1093/biomet/38.3-4.481},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Characteristic Functions/Gil-Pelaez (1951) Note on the inversion theorem.pdf:pdf},
	issn = {14645211},
	journal = {Biometrika},
	mendeley-groups = {Random Useful Math/Probability/Characteristic Functions},
	number = {3-4},
	pages = {481--482},
	title = {{Note on the Inversion Theorem}},
	volume = {38},
	year = {1951}
}

@book{Hall1992,
	author = {Hall, Peter},
	doi = {10.1007/978-1-4612-4384-7},
	isbn = {978-0-387-97720-1},
	mendeley-groups = {Resampling/Bootstrap},
	pages = {XIV, 354},
	publisher = {Springer New York},
	title = {{The Bootstrap and Edgeworth Expansion}},
	year = {1992}
}
@article{Hastie1993,
	author = {Hastie, Trevor and Loader, Clive},
	doi = {10.1214/ss/1177011002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Kernel/Hastie, Loader (1993) Local Regression Automatic Kernel Carpentry.pdf:pdf},
	journal = {Statistical Science},
	keywords = {boundary effects,derivative estimation,kernel,local regression,smoothing},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression/Kernel,Nonparametric Estimation/Nonparametric Regression/LP},
	number = {2},
	pages = {120--129},
	title = {{Local Regression: Automatic Kernel Carpentry}},
	volume = {8},
	year = {1993}
}
 @book{Fan1996,
 	author = {Fan, Jianqing and Gijbels, Irene},
 	isbn = {9780412983214},
 	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression/LP},
 	pages = {XV, 341},
 	publisher = {Springer New York},
 	title = {{Local Polynomial Modelling and Its Applications}},
 	year = {1996}
 }
 @article{Cheng2019,
 	abstract = {In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estima-tion) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was recently employed by Calonico et al. (2018b) to construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas of using the debiased estimator and further propose a bootstrap approach for constructing simultaneous confidence bands. This modified method has an advantage that we can easily choose the smoothing bandwidth from conventional bandwidth selectors and the confidence band will be asymptotically valid. We prove the validity of the bootstrap confidence band and generalize it to density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets. We apply our approach to an Astronomy dataset to show its applicability.},
 	archivePrefix = {arXiv},
 	arxivId = {1702.07027},
 	author = {Cheng, Gang and Chen, Yen Chi},
 	doi = {10.1214/19-EJS1575},
 	eprint = {1702.07027},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Inference/Cheng, Chen (2019) Nonparametric inference via bootstrapping the debiased estimator.pdf:pdf},
 	issn = {19357524},
 	journal = {Electronic Journal of Statistics},
 	keywords = {Bootstrap,Confidence set,Inverse regression,Kernel density estimator,Level set,Local polynomial regression},
 	mendeley-groups = {Nonparametric Estimation/Inference},
 	number = {1},
 	pages = {2194--2256},
 	title = {{Nonparametric Inference via Bootstrapping the Debiased Estimator}},
 	volume = {13},
 	year = {2019}
 }
 @article{Gu2015,
 	abstract = {Masry (1996b) provides estimation bias and variance expression for a general local polynomial kernel estimator in a general multivariate regression framework. Under smoother conditions on the unknown regression function and by including more refined approximation terms than that in Masry (1996b), we extend the result of Masry (1996b) to obtain explicit leading bias terms for the whole vector of the local polynomial estimator. Specifically, we derive the leading bias and leading variance terms of nonparametric local polynomial kernel estimator in a general nonparametric multivariate regression model framework. The results can be used to obtain optimal smoothing parameters in local polynomial estimation of the unknown conditional mean function and its derivative functions.},
 	author = {Gu, Jingping and Li, Qi and Yang, Jui Chung},
 	doi = {10.1080/07474938.2014.956615},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/LP/Gu, Li, Yang (2014) Multivariate Local Polynomial Kernel Estimators Leading Bias and Asymptotic Distribution.pdf:pdf},
 	issn = {15324168},
 	journal = {Econometric Reviews},
 	keywords = {Kernel estimation,Leading bias,Local polynomial method},
 	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression/LP},
 	number = {6-10},
 	pages = {979--1010},
 	title = {{Multivariate Local Polynomial Kernel Estimators: Leading Bias and Asymptotic Distribution}},
 	volume = {34},
 	year = {2015}
 }
 
@article{Abowd1999,
	author = {Abowd, John M and Kramarz, Francis and Margolis, David N and Abowd, B Y John M},
	doi = {10.1111/1468-0262.00020},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Abowd, Kramarz, Margolis (1999) High Wage Workers and High Wage Firms.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Applied (By Field)/Labor},
	number = {2},
	pages = {251--333},
	title = {{High Wage Workers and High Wage Firms}},
	volume = {67},
	year = {1999}
}

@article{Bonhomme2023,
	abstract = {Many studies use matched employer-employee data to estimate a statistical model of earnings determination where log-earnings are expressed as the sum of worker effects, firm effects, covariates, and idiosyncratic error terms. Estimates based on this model have produced two influential yet controversial conclusions. First, firm effects typically explain around 20{\%} of the variance of log-earnings, pointing to the importance of firm-specific wage-setting for earnings inequality. Second, the correlation between firm and worker effects is often small and sometimes negative, indicating little if any sorting of high-wage workers to high-paying firms. The objective of this paper is to assess the sensitivity of these conclusions to the biases that arise because of limited mobility of workers across firms. We use employer-employee data from the US and several European countries while taking advantage of both fixed-effects and random-effects methods for bias-correction. We find that limited mobility bias is severe and that bias-correction is important. Once one corrects for limited mobility bias, firm effects matter less for earnings inequality and worker sorting becomes always positive and typically strong.},
	author = {Bonhomme, Stephane and Holzheu, Kerstin and Lamadon, Thibaut and Manresa, Elena and Mogstad, Magne and Setzler, Bradley},
	doi = {10.1086/720009},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Bonhomme et al. (2023) How Much Should We Trust Estimates of Firm Effects and Worker Sorting.pdf:pdf},
	journal = {Journal of Labor Economics},
	mendeley-groups = {Applied (By Field)/Labor},
	title = {{How Much Should We Trust Estimates of Firm Effects and Worker Sorting?}},
	year = {2023}
}

@article{Melo2009,
	abstract = {Although the productivity gains of urban agglomeration economies are generally found to be positive, there is a great deal of variability in the magnitude of reported estimates. This paper undertakes a quantitative review of the empirical literature on agglomeration through a meta-analysis of 729 elasticities taken from 34 different studies. The objective is to make sense of the range of values for agglomeration economies found in the literature by identifying some key characteristics that affect the magnitude of the results obtained. Our analysis confirms that study characteristics do matter. In particular, we find that country specific effects, the industrial coverage, the specification of agglomeration economies, and the presence of controls for both unobserved cross-sectional heterogeneity and differences in time-variant labor quality can give rise to large differences in the results reported in the literature. In contrast, correcting for reverse causality of agglomeration does not seem to produce noticeable changes in the size of urban agglomeration estimates. We also test for publication bias and find some evidence supporting the presence of positive reporting bias. The findings support the intuition that agglomeration estimates for any particular empirical context may have little relevance elsewhere. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Melo, Patricia C. and Graham, Daniel J. and Noland, Robert B.},
	doi = {10.1016/j.regsciurbeco.2008.12.002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Urban and Geography/Melo, Graham, Noland (2009) A meta-analysis of estimates of urban agglomeration economies.pdf:pdf},
	issn = {01660462},
	journal = {Regional Science and Urban Economics},
	keywords = {Agglomeration,Elasticity,Meta-analysis,Productivity},
	mendeley-groups = {Economics/Urban and Geography},
	number = {3},
	pages = {332--342},
	publisher = {Elsevier B.V.},
	title = {{A Meta-Analysis of Estimates of Urban Agglomeration Economies}},
	volume = {39},
	year = {2009}
}

@article{Asplund2006,
	abstract = {This paper is motivated by the empirical regularity that industries differ greatly in the level of firm turnover and that entry and exit rates are positively correlated across industries. Our objective is to investigate the effect of fixed costs and, in particular, market size on entry and exit rates and hence on the age distribution of firms. We analyse a stochastic dynamic model of a monopolistically competitive industry. Each firm's efficiency is assumed to follow a Markov process. We show existence and uniqueness of a stationary equilibrium with simultaneous entry and exit: efficient firms survive, while inefficient ones leave the market and are replaced by new entrants. We perform comparative dynamics with respect to the level of fixed costs: entry costs are negatively related and fixed production costs positively related to entry and exit rates. A central empirical prediction of the model is that the level of firm turnover is increasing in market size. In larger markets, competition is endogenously more intense than in smaller markets, and so price-cost margins are smaller. This price competition effect implies that the marginal surviving firm has to be more efficient than in smaller markets. Hence, in larger markets, the expected lifespan of firms is shorter, and the age distribution of firms is first-order stochastically dominated by that in smaller markets. In the empirical part, the prediction on market size and firm turnover is tested on an industry where firms compete in well-defined geographical markets of different sizes. Using data on hair salons in Sweden, we show that an increase in market size or fixed costs shifts the age distribution of firms towards younger firms, as predicted by the model. {\textcopyright} 2006 The Review of Economic Studies Limited.},
	author = {Asplund, Marcus and Nocke, Volker},
	doi = {10.1111/j.1467-937X.2006.00377.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/IO/Asplund, Nocke (2006) Firm Turnover in Imperfectly Competitive Markets.pdf:pdf},
	issn = {00346527},
	journal = {Review of Economic Studies},
	mendeley-groups = {Economics/IO},
	number = {2},
	pages = {295--327},
	title = {{Firm Turnover in Imperfectly Competitive Markets}},
	volume = {73},
	year = {2006}
}

@article{Ormoneit1999,
	abstract = {We describe an algorithm to efficiently compute maximum entropy densities, i.e. densities maximizing the Shannon entropy - (Formula presented.) under a set of constraints (Formula presented.). Our method is based on an algorithm by Zellner and Highfield, which has been found not to converge under a variety of circumstances. To demonstrate that our method overcomes these difficulties, we conduct numerous experiments for the special case g i(x) = x i, n = 4. An extensive table of results for this case and computer code are available on the World Wide Web. {\textcopyright} 1999, Taylor {\&} Francis Group, LLC.},
	author = {Ormoneit, D. and White, H.},
	doi = {10.1080/07474939908800436},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Moments/Ormoneit, White (1999) An efficient algorithm to compute maximum entropy densities.pdf:pdf},
	issn = {15324168},
	journal = {Econometric Reviews},
	keywords = {Density estimation,Maximum entropy principle,Shannon entropy},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems,Nonparametric Estimation/Density Estimation/From Moments},
	number = {2},
	pages = {127--140},
	title = {{An Efficient Algorithm to Compute Maximum Entropy Densities}},
	volume = {18},
	year = {1999}
}
@article{Wu2003,
	abstract = {The maximum entropy approach is a flexible and powerful tool for density approximation. This paper proposes a sequential updating method to calculate the maximum entropy density subject to known moment constraints. Instead of imposing the moment constraints simultaneously, the sequential updating method incorporates the moment constraints into the calculation from lower to higher moments and updates the density estimates sequentially. The proposed method is employed to approximate the size distribution of U.S. family income. Empirical evidence demonstrates the efficiency of this method. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
	author = {Wu, Ximing},
	doi = {10.1016/S0304-4076(03)00114-3},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Moments/Wu (2003) Calculation of maximum entropy densities with application to income distribution.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Density estimation,Income distribution,Maximum entropy,Sequential updating},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems,Nonparametric Estimation/Density Estimation/From Moments},
	number = {2},
	pages = {347--354},
	title = {{Calculation of Maximum Entropy Densities with Application to Income Distribution}},
	volume = {115},
	year = {2003}
}

@article{Chen2005,
	abstract = {In this article we consider identification and estimation of a censored nonparametric location scale-model. We first show that in the case where the location function is strictly less than the (fixed) censoring point for all values in the support of the explanatory variables, the location function is not identified anywhere. In contrast, when the location function is greater or equal to the censoring point with positive probability, the location function is identified on the entire support, including the region where the location function is below the censoring point. In the latter case we propose a simple estimation procedure based on combining conditional quantile estimators for various higher quantiles. The new estimator is shown to converge at the optimal nonparametric rate with a limiting normal distribution. A small-scale simulation study indicates that the proposed estimation procedure performs well in finite samples. We also present an empirical illustration on unemployment insurance duration using administrative-level data from New Jersey. {\textcopyright} 2005 American Statistical Association.},
	author = {Chen, Songnian and Dahl, Gordon B. and Khan, Shakeeb},
	doi = {10.1198/016214504000000836},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Chen et al (2005) Nonparametric Identification and Estimation of a Censored Location-Scale Regression.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Censored regression,Nonparametric quantile regression,Unemployment insurance},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {469},
	pages = {212--221},
	title = {{Nonparametric Identification and Estimation of a Censored Location-Scale Regression Model}},
	volume = {100},
	year = {2005}
}
@article{Sasaki2022,
	abstract = {We develop a new extreme value theory for repeated cross-sectional and longitudinal/panel data to construct asymptotically valid confidence intervals (CIs) for conditional extremal quantiles from a fixed number k of nearest-neighbor tail observations. As a by-product, we also construct CIs for extremal quantiles of coefficients in linear random coefficient models. For any fixed k, the CIs are uniformly valid without parametric assumptions over a set of nonparametric data generating processes associated with various tail indices. Simulation studies show that our CIs exhibit superior small-sample coverage and length properties than alternative nonparametric methods based on asymptotic normality. Applying the proposed method to Natality Vital Statistics, we study factors of extremely low birth weights. We find that signs of major effects are the same as those found in preceding studies based on parametric models, but with different magnitudes.},
	archivePrefix = {arXiv},
	arxivId = {1909.00294},
	author = {Sasaki, Yuya and Wang, Yulong},
	doi = {10.1080/07350015.2020.1870985},
	eprint = {1909.00294},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Conditional/Sasaki, Wang (2022) Fixed-k Inference for Conditional Extremal Quantiles.pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Conditional extremal quantile,Confidence interval,Extreme value theory,Fixed k,Random coefficient},
	mendeley-groups = {Quantile/Extreme Value},
	number = {2},
	pages = {829--837},
	publisher = {Taylor {\&} Francis},
	title = {{Fixed-k Inference for Conditional Extremal Quantiles}}, 
	volume = {40},
	year = {2022}
}

@incollection{Blundell2007,
	abstract = {This chapter is concerned with the identification and estimation of models of labor supply. The focus is on the key issues that arise from unobserved heterogeneity, nonparticipation and dynamics. We examine the simple "static" labor supply model with proportional taxes and highlight the problems surrounding nonparticipation and missing wages. The difference-in-differences approach to estimation and identification is developed within the context of the labor supply model. We also consider the impact of incorporating nonlinear taxation and welfare program participation. Family labor supply is looked at from both the unitary and collective perspectives. Finally we consider intertemporal models focusing on the difficulties that arise with participation and heterogeneity. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Blundell, Richard and MaCurdy, Thomas and Meghir, Costas},
	booktitle = {Handbook of Econometrics},
	chapter = {65},
	doi = {10.1016/S1573-4412(07)06069-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Blundell, MaCurdy, Meghir (2007)  Labor Supply Models Unobserved Heterogeneity Nonparticipation and Dynamics.pdf:pdf},
	isbn = {9780444506313},
	keywords = {consumption,labor supply,microeconometrics,taxation},
	mendeley-groups = {Economics/Labor},
	pages = {4667--4775},
	publisher = {North-Holland},
	title = {{Labor Supply Models: Unobserved Heterogeneity, Nonparticipation and Dynamics}},
	volume = {6},
	year = {2007}
}

@article{Saez2012,
	abstract = {This paper critically surveys the large and growing literature estimating the elasticity of taxable income with respect to marginal tax rates using tax return data. First, we provide a theoretical framework showing under what assumptions this elasticity can be used as a sufficient statistic for efficiency and optimal tax analysis. We discuss what other parameters should be estimated when the elasticity is not a sufficient statistic. Second, we discuss conceptually the key issues that arise in the empirical estimation of the elasticity of taxable income using the example of the 1993 top individual income tax rate increase in the United States to illustrate those issues. Third, we provide a critical discussion of selected empirical analyses of the elasticity of taxable income in light of the theoretical and empirical framework we laid out. Finally, we discuss avenues for future research. {\textcopyright} 2012 AEA.},
	author = {Saez, Emmanuel and Slemrod, Joel and Giertz, Seth H.},
	doi = {10.1257/jel.50.1.3},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Saez, Slemrod, Giertz (2012) The Elasticity of Taxable Income with Respect to Marginal Tax Rates A Critical Review.pdf:pdf},
	issn = {00220515},
	journal = {Journal of Economic Literature},
	mendeley-groups = {Economics/Labor},
	number = {1},
	pages = {3--50},
	title = {{The Elasticity of Taxable Income with Respect to Marginal Tax Rates: A Critical Review}},
	volume = {50},
	year = {2012}
}
@article{Fan1991,
	author = {Fan, Jianqing},
	doi = {10.1214/aos/1176348248},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Deconvolution/Fan (1991) On the Optimal Rates of Convergence for Nonparametric Deconvolution Problems.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Nonparametric Estimation/Deconvolution},
	number = {3},
	pages = {1257--1272},
	title = {{On the Optimal Rates of Convergence for Nonparametric Deconvolution Problems}},
	volume = {19},
	year = {1991}
}

@article{Abrevaya2021,
	abstract = {Nonlinearity and heterogeneity are known to cause difficulties in estimating and interpreting partial effects. This paper provides a systematic characterization of the various partial effects in nonlinear panel data models that might be of interest to empirical researchers. The interpretation of the partial effects depends upon (i) whether the distribution of unobserved heterogeneity is treated as fixed or allowed to vary with covariates, and (ii) whether one is interested in particular covariate values or an average over such values. The characterization covers partial-effects concepts already in the literature but also includes new concepts for partial effects. A simple panel probit design highlights that the different partial effects can be quantitatively very different.},
	author = {Abrevaya, Jason and Hsu, Yu Chin},
	doi = {10.1093/ectj/utab004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Abrevaya, Hsu (2021) Partial effects in non-linear panel data models with correlated random effects.pdf:pdf},
	issn = {1368423X},
	journal = {Econometrics Journal},
	keywords = {Non-linear panel data models,correlated random effects,partial effects},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {3},
	pages = {519--535},
	title = {{Partial Effects in Non-linear Panel Data Models with Correlated Random Effects}},
	volume = {24},
	year = {2021}
}

@article{Meister2006,
	abstract = {This note deals with the problem of estimating the support of a distribution function when the empirical data are contaminated. An easily computable estimation method based on moment estimation is introduced. Unlike earlier approaches, these estimators also cover situations in which the sharpness of the target distribution at the support boundaries is arbitrarily low and in which the distribution is discontinuous at several points within the support. General consistency results for the estimators are achieved and rates of convergence are studied in the case of normal measurement error. The practical performance of the estimation procedure is studied in a simulation section.},
	author = {Meister, Alexander},
	doi = {10.1080/02331880600723101},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Meister (2006) Support estimation via moment estimation in presence of noise.pdf:pdf},
	isbn = {0233188060072},
	issn = {02331888},
	journal = {Statistics},
	keywords = {Deconvolution,Errors-in-variables,Moment estimation,Support estimation},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {3},
	pages = {259--275},
	title = {{Support Estimation via Moment Estimation in Presence of Noise}},
	volume = {40},
	year = {2006}
}

@book{Akhiezer1965,
	author = {Akhiezer, Naum I.},
	doi = {10.1137/1.9781611976397},
	isbn = {978-1-61197-638-0},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	pages = {xiv + 253},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{The Classical Moment Problem and Some Related Questions in Analysis}},
	year = {1965}
}

@misc{Brownlees2024UnitAveragingHeterogeneous,
  title = {Unit {{Averaging}} for {{Heterogeneous Panels}}},
  author = {Brownlees, Christian and Morozov, Vladislav},
  year = {2024},
  month = may,
  number = {arXiv:2210.14205},
  eprint = {2210.14205},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.14205},
  urldate = {2025-09-16},
  abstract = {In this work we introduce a unit averaging procedure to efficiently recover unit-specific parameters in a heterogeneous panel model. The procedure consists in estimating the parameter of a given unit using a weighted average of all the unit-specific parameter estimators in the panel. The weights of the average are determined by minimizing an MSE criterion we derive. We analyze the properties of the resulting minimum MSE unit averaging estimator in a local heterogeneity framework inspired by the literature on frequentist model averaging, and we derive the local asymptotic distribution of the estimator and the corresponding weights. The benefits of the procedure are showcased with an application to forecasting unemployment rates for a panel of German regions.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
}

@article{Segers2005,
	abstract = {The Pickands estimator for the extreme value index is generalized in a way that includes all of its previously known variants. A detailed study of the asymptotic behavior of the estimators in the family serves to determine its optimally performing members. These are given by simple, explicit formulas, have the same asymptotic variance as the maximum likelihood estimator in the generalized Pareto model, and are robust to departures from the limiting generalized Pareto model in case the convergence of the excess distribution to its limit is slow. A simulation study involving a wide range of distributions shows the new estimators to compare favorably with the maximum likelihood estimator. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
	author = {Segers, Johan},
	doi = {10.1016/j.jspi.2003.11.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Extreme Value Index Estimation/Segers (2005) Generalized Pickands estimators for the extreme value index.pdf:pdf},
	issn = {03783758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Extreme value index,Generalized Pareto distribution,Pickands estimator,Second-order regular variation,Tail quantile function,Threshold exceedances},
	mendeley-groups = {Quantile/Extreme Value/EVI Estimation},
	number = {2},
	pages = {381--396},
	title = {{Generalized Pickands Estimators for the Extreme Value Index}},
	volume = {128},
	year = {2005}
}

@article{Hill1975,
	author = {Hill, Bruce M.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Extreme Value Index Estimation/Hill (1975) A Simple General Approach to Inference About the Tail of a Distribution.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Quantile/Extreme Value/EVI Estimation},
	number = {5},
	pages = {1163--1174},
	title = {{A Simple General Approach to Inference About the Tail of a Distribution}},
	volume = {3},
	year = {1975}
}

@book{Polak1997,
	author = {Polak, Elijah},
	doi = {10.1007/978-1-4612-0663-7},
	isbn = {978-0-387-94971-0},
	mendeley-groups = {Random Useful Math/Optimization},
	pages = {XX, 782},
	publisher = {Springer New York},
	title = {{Optimization: Algorithms and Consistent Approximations}},
	year = {1997}
}

@article{Stone1982,
	author = {Stone, Charles J.},
	doi = {10.1214/aos/1176345969},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Asymptotic Theory/Stone (1982) Optimal Global Rates of Convergence for Nonparametric Regression.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {6},
	pages = {1348--1360},
	title = {{Optimal Rates of Convergence for Nonparametric Estimators}},
	volume = {8},
	year = {1982}
}

@misc{NIST:DLMF,
	key = "{\relax DLMF}",
	title = "{\it NIST Digital Library of Mathematical Functions}",
	howpublished = "http://dlmf.nist.gov/, Release 1.1.8 of 2022-12-15",
	url = "http://dlmf.nist.gov/",
	note = "F.~W.~J. Olver, A.~B. {Olde Daalhuis}, D.~W. Lozier, B.~I. Schneider,
	R.~F. Boisvert, C.~W. Clark, B.~R. Miller, B.~V. Saunders,
	H.~S. Cohl, and M.~A. McClain, eds."}

@article{Wang1996,
	author = {Wang, Jinde},
	doi = {10.1214/aos/1032526971},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Wang (1996) Asymptotics of Least-Squares Estimators for Constrained Nonlinear Regression.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Limit Theory,!Extra reading list},
	number = {3},
	pages = {1316--1326},
	title = {{Asymptotics of Least-Squares Estimators for Constrained Nonlinear Regression}},
	volume = {24},
	year = {1996}
}
@article{Andrews1999,
	author = {Andrews, Donald W.K.},
	doi = {10.1111/1468-0262.00082},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Andrews (1999) Estimation When a Parameter is on a Boundary.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Limit Theory/Parameter on Boundary,!Extra reading list},
	number = {6},
	pages = {1341--1383},
	title = {{Estimation When a Parameter is on a Boundary}},
	volume = {67},
	year = {1999}
}

@book{Shohat1943,
	author = {Shohat, James Alexander and Tamarkin, Jacob David},
	isbn = {9780821815014},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	pages = {140},
	publisher = {American Mathematical Society},
	title = {{The Problem of Moments}},
	year = {1943}
}

 @article{Lindsay1989,
 	author = {Lindsay, Bruce G.},
 	doi = {10.1214/aos/1176347138},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Mixtures/Lindsay (1989) Moment Matrices Applications in Mixtures.pdf:pdf},
 	journal = {The Annals of Statistics},
 	mendeley-groups = {Mixtures/Method of Moments},
 	number = {2},
 	pages = {722--740},
 	title = {{Moment Matrices: Applications in Mixtures}},
 	volume = {17},
 	year = {1989}
 }
 @article{Wu2020,
 	abstract = {The method of moments (Philos. Trans. R. Soc. Lond. Ser. A 185 (1894) 71–110) is one of the most widely used methods in statistics for parameter estimation, by means of solving the system of equations that match the population and estimated moments. However, in practice and especially for the important case of mixture models, one frequently needs to contend with the difficulties of non-existence or nonuniqueness of statistically meaningful solutions, as well as the high computational cost of solving large polynomial systems. Moreover, theoretical analyses of the method of moments are mainly confined to asymptotic normality style of results established under strong assumptions. This paper considers estimating a k-component Gaussian location mixture with a common (possibly unknown) variance parameter. To overcome the aforementioned theoretic and algorithmic hurdles, a crucial step is to denoise the moment estimates by projecting to the truncated moment space (via semidefinite programming) before solving the method of moments equations. Not only does this regularization ensure existence and uniqueness of solutions, it also yields fast solvers by means of Gauss quadrature. Furthermore, by proving new moment comparison theorems in the Wasserstein distance via polynomial interpolation and majorization techniques, we establish the statistical guarantees and adaptive optimality of the proposed procedure, as well as oracle inequality in misspecified models. These results can also be viewed as provable algorithms for generalized method of moments (Econometrica 50 (1982) 1029–1054) which involves nonconvex optimization and lacks theoretical guarantees.},
 	archivePrefix = {arXiv},
 	arxivId = {1807.07237},
 	author = {Wu, Yihong and Yang, Pengkun},
 	doi = {10.1214/19-AOS1873},
 	eprint = {1807.07237},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Mixtures/Wu, Yang (2020) Optimal estimation of Gaussian mixtures via denoised method of.pdf:pdf},
 	issn = {21688966},
 	journal = {The Annals of Statistics},
 	keywords = {Deconvolution,Finite mixture model,Gauss quadrature,Gaussian mixture,Method of moments,Minimax optimality,Moment space,Semidefinite programming,Wasserstein distance},
 	mendeley-groups = {Mixtures/Method of Moments},
 	number = {4},
 	pages = {1981--2007},
 	title = {{Optimal Estimation of Gaussian Mixtures via Denoised Method of Moments}},
 	volume = {48},
 	year = {2020}
 }
 

@article{Lindsay1993,
	abstract = {A longstanding difficulty in multivariate statistics is identifying and evaluating nonnormal data structures in high dimensions with high statistical efficiency and low search effort. Here the possibilities of using sample moments to identify mixtures of multivariate normals are investigated. A particular system of moment equations is devised and then shown to be one that identifies the true mixing distribution, with some limitations (indicated in the text), and thus provides consistent estimates. Moreover, the estimates are shown to be quickly calculated in any dimension and to be highly efficient in the sense of being close to the values of the parameters that maximize the likelihood function. This is shown by simulation and the application of the method to Fisher's iris data. While establishing these results, we discuss certain limitations associated with moment methods with regard to uniqueness and equivariance and explain how we addressed these problems. {\textcopyright} 1993 Taylor {\&} Francis Group, LLC.},
	author = {Lindsay, Bruce G. and Basak, Prasanta},
	doi = {10.1080/01621459.1993.10476297},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Mixtures/Lindsay, Basak (1993) Multivariate Normal Mixtures A Fast Consistent Method of Moments.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {EM algorithm,Identifiability,Initial values,Maximum likelihood,Multivariate moments,Unbiased estimates},
	mendeley-groups = {Mixtures/Mixtures of Normals},
	number = {422},
	pages = {468--476},
	title = {{Multivariate Normal Mixtures: A Fast Consistent Method of Moments}},
	volume = {88},
	year = {1993}
}

@article{Barron1991b,
	author = {Barron, Andrew R. and Sheu, Chyong-Hwa},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Barron, Sheu (1991) Approximation of Density Functions by Sequences of Exponential Families.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Nonparametric Estimation/Density Estimation},
	number = {3},
	pages = {1347--1369},
	title = {{Approximation of Density Functions by Sequences of Exponential Families}},
	volume = {19},
	year = {1991}
}

@article{Zellinger2021,
	abstract = {We study the problem of approximating the recovery of a probability distribution on the unit interval from its first k moments. As main result we obtain an upper bound on the L1 reconstruction error under the regularity assumption that the log-density function has square-integrable derivatives up to some natural order r{\textgreater}1. Our bound is of order O(k−r). A comparative study relates our findings to alternative conditions on the distributions.},
	author = {Zellinger, Werner and Moser, Bernhard A.},
	doi = {10.1016/j.amc.2021.126057},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Zellinger, Moser (2021) On the truncated Hausdorff moment problem under Sobolev.pdf:pdf},
	issn = {00963003},
	journal = {Applied Mathematics and Computation},
	keywords = {Maximum entropy,Moment-based distribution approximation,Total variation distance,Truncated Hausdorff moment problem},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	pages = {126057},
	publisher = {Elsevier Inc.},
	title = {{On the Truncated Hausdorff Moment Problem Under Sobolev Regularity Conditions}},
	volume = {400},
	year = {2021}
}

@book{Bustamante2017,
	author = {Bustamante, Jorge},
	doi = {10.1007/978-3-319-55402-0},
	isbn = {978-3-319-55401-3},
	mendeley-groups = {Random Useful Math/Approximation Theory},
	pages = {1--420},
	publisher = {Birkh{\"{a}}user Cham},
	title = {{Bernstein Operators and Their Properties}},
	year = {2017}
}

@article{Dragomir2000,
	abstract = {An Ostrowski type integral inequality for the Riemann-Stieltjes Integral R ba f (t) du (t) , where f is assumed to be of bounded variation on [a, b] and u is of r ? H?H{\"{o}}lder type on the same interval, is given. Applications to the approximation problem of the Riemann-Stieltjes integral in terms of Riemann-Stieltjes sums are also pointed out. {\textcopyright} 2000 Korean Society for Computational {\&} Applied Mathematics and Korean SIGCAM.},
	author = {Dragomir, Silvestru S.},
	doi = {10.1007/bf03012272},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Analysis/Integration/Dragomir (2000) On the Ostrowski's inequality for Riemann-Stieltjes integral and applications.pdf:pdf},
	issn = {15985865},
	journal = {Journal of Applied Mathematics and Computing},
	keywords = {Ostrowski Inequality,Riemann-Stieltjes Integral},
	mendeley-groups = {Random Useful Math/Analysis/Integration},
	number = {3},
	pages = {611--627},
	title = {{On the Ostrowski's Inequality for Riemann-Stieltjes Integral and Applications}},
	volume = {7},
	year = {2000}
}

@article{Norets2010,
	abstract = {This paper shows that large nonparametric classes of conditional multivariate densities can be approximated in the Kullback-Leibler distance by different specifications of finite mixtures of normal regressions in which normal means and variances and mixing probabilities can depend on variables in the conditioning set (covariates). These models are a special case of models known as "mixtures of experts" in statistics and computer science literature. Flexible specifications include models in which only mixing probabilities, modeled by multinomial logit, depend on the covariates and, in the univariate case, models in which only means of the mixed normals depend flexibly on the covariates. Modeling the variance of the mixed normals by flexible functions of the covariates can weaken restrictions on the class of the approximable densities. Obtained results can be generalized to mixtures of general location scale densities. Rates of convergence and easy to interpret bounds are also obtained for different model specifications. These approximation results can be useful for proving consistency of Bayesian and maximum likelihood density estimators based on these models. The results also have interesting implications for applied researchers. {\textcopyright} 2010 Institute of Mathematical Statistics.},
	author = {Norets, Andriy},
	doi = {10.1214/09-AOS765},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Norets (2010) Approximation of conditional densities by smooth mixtures of regressions.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Bayesian conditional density estimation,Finite mixtures of normal distributions,Mixtures of experts,Smoothly mixing regressions},
	mendeley-groups = {Nonparametric Estimation/Density Estimation/Conditional Density Estimation},
	number = {3},
	pages = {1733--1766},
	title = {{Approximation of Conditional Densities by Smooth Mixtures of Regressions}},
	volume = {38},
	year = {2010}
}
@inproceedings{Li1999,
	abstract = {Gaussian mixtures (or so-called radial basis function networks) for density estimation provide a natural counterpart to sigmoidal neural networks for function fitting and approximation. In both cases, it is possible to give simple expressions for the iterative improvement of performance as components of the network are introduced one at a time. In particular, for mixture density estimation we show that a k-component mixture estimated by maximum likelihood (or by an iterative likelihood improvement that we introduce) achieves log-likelihood within order 1/$\kappa$ of the log-likelihood achievable by any convex combination. Consequences for approximation and estimation using Kullback-Leibler risk are also given. A Minimum Description Length principle selects the optimal number of compo-minimizes $\kappa$ that minimizes the risk bound.},
	author = {Li, Jonathan Q. and Barron, Andrew R.},
	booktitle = {Advances in Neural Information Processing Systems 12 (NIPS 1999)},
	editor = {Solla, Sara A. and Leen, Todd and M{\"{u}}ller, Klaus-Robert},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Neural Networks/Li, Barron (1999) Mixture Density Estimation.pdf:pdf},
	isbn = {0262194503},
	issn = {10495258},
	mendeley-groups = {Nonparametric Estimation/Density Estimation/Conditional Density Estimation},
	pages = {279--285},
	title = {{Mixture Density Estimation}},
	year = {1999}
}

@article{Barron1991,
	abstract = {The minimum complexity or minimum description-length criterion developed by Kolmogorov, Rissanen, Wallace, Sorkin, and others leads to consistent probability density estimators. These density estimators are defined to achieve the best compromise between likelihood and simplicity. A related issue is the compromise between accuracy of approximations and complexity relative to the sample size. An index of resolvability is studied which is shown to bound the statistical accuracy of the density estimators, as well as the information-theoretic redundancy. {\textcopyright} 1991 IEEE},
	author = {Barron, Andrew R. and Cover, Thomas M.},
	doi = {10.1109/18.86996},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Neural Networks/Barron, Cover (1991) Minimum Complexity Density Estimation.pdf:pdf},
	isbn = {0001489518},
	issn = {15579654},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Kolmogorov complexity,bounds on redundancy,consistency,density estimation,discovery of probability laws,minimum description-length criterion,model selection,resolvability of functions,statistical convergence rates,universal data compression},
	mendeley-groups = {Nonparametric Estimation/Density Estimation},
	number = {4},
	pages = {1034--1054},
	title = {{Minimum Complexity Density Estimation}},
	volume = {37},
	year = {1991}
}

@article{Barron1993,
	author = {Barron, Andrew R.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Neural Networks/Barron (1993) Universal approximation bounds for superpositions of a sigmoidal function.pdf:pdf},
	journal = {IEEE Transactions on Information Theory},
	mendeley-groups = {Neural Networks,Nonparametric Estimation/Sieve methods,Random Useful Math/Approximation Theory},
	number = {3},
	pages = {930--945},
	title = {{Universal Approximation Bounds for Superpositions of a Sigmoidal Function}},
	volume = {39},
	year = {1993}
}

@article{Zeevi1997,
	author = {Zeevi, Assaf J. and Meir, Ronny},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Zeevi, Meir (1997) Density Estimation Through Convex Combinations of Densities.pdf:pdf},
	journal = {Neural Networks},
	keywords = {-density},
	mendeley-groups = {Nonparametric Estimation/Density Estimation},
	number = {1},
	pages = {99--109},
	title = {{Density Estimation Through Convex Combinations of Densities: Approximation and Estimation Bounds}},
	volume = {10},
	year = {1997}
}

@article{Masse1999,
	author = {M{\^{a}}sse, Beno{\^{i}}t R. and Truong, Young K.},
	doi = {10.2307/3316133},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Sieve Estimation/Masse, Truong (1999) Conditional Logspline Density Estimation.pdf:pdf},
	journal = {The Canadian Journal of Statistics},
	keywords = {akaike information c,ams 1991 subject classifications,and phrases,conditional density estimation,estimation,maximum likelihood,primary 62g,r,stepwise knot deletion},
	mendeley-groups = {Nonparametric Estimation/Density Estimation/Conditional Density Estimation,Nonparametric Estimation/Sieve methods},
	number = {4},
	pages = {819--832},
	title = {{Conditional Logspline Density Estimation}},
	volume = {27},
	year = {1999}
}

@article{Stone1994,
	author = {Stone, Charles J.},
	doi = {10.1214/aos/1176325361},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Sieve Estimation/Stone (1994) The Use of Polynomial Splines and Their Tensor Products in Multivariate Function.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Nonparametric Estimation/Sieve methods},
	number = {1},
	pages = {118--171},
	title = {{The Use of Polynomial Splines and Their Tensor Products in Multivariate Function Estimation}},
	volume = {22},
	year = {1994}
}

@article{Diebold1995,
	author = {Diebold, Francis X. and Mariano, Roberto S.},
	doi = {10.2307/1392185},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Diebold, Mariano (1995) Comparing Predictive Accuracy.pdf:pdf},
	issn = {07350015},
	journal = {Journal of Business {\&} Economic Statistics},
	mendeley-groups = {Forecasting},
	number = {3},
	pages = {253},
	title = {{Comparing Predictive Accuracy}},
	volume = {13},
	year = {1995}
}

@unpublished{Hong2018,
	abstract = {We consider a class of nonparametric time series regression models in which the regressor takes values in a sequence space and the data are stationary and weakly dependent. We propose an infinite dimensional Nadaraya-Watson type estimator with a bandwidth sequence that shrinks the effects of long lags. We investigate its asymptotic properties in detail under both static and dynamic regressions contexts. First we show pointwise consistency of the estimator under a set of mild regularity conditions. We establish a CLT for the estimator at a point under stronger conditions as well as for a feasibly studentized version of the estimator, thereby allowing pointwise inference to be conducted. We establish the uniform consistency over a compact set of logarithmically increasing dimension. We specify the explicit rates of convergence in terms of the Lambert W function, and show that the optimal rate that balances the squared bias and variance is of logarithmic order, the precise rate depending on the smoothness of the regression function and the dependence of the data in a non-trivial way.},
	archivePrefix = {arXiv},
	arxivId = {1604.06380},
	author = {Hong, Seok Young and Linton, Oliver B.},
	eprint = {1604.06380},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Hong, Linton (2018) Asymptotic properties of a Nadaraya-Watson type estimator for regression functions of infinite order.pdf:pdf},
	keywords = {2000 mathematics subject classification,curse of infinite,dimensionality,functional regression,nadaraya-watson estimator,near epoch dependence,primary 62g05,secondary 62g08},
	mendeley-groups = {Nonparametric Estimation/Time Series and Dependent Data,Nonparametric Estimation/Nonparametric Regression,Time Series/Nonparametric},
	title = {{Asymptotic Properties of a Nadaraya-Watson Type Estimator for Regression Functions of Infinite Order}},
	year = {2018}
}

@article{Heckman2001,
	author = {Heckman, James J.},
	doi = {10.1086/322086},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Cross sections/Heckman (2001) Micro Data, Heterogeneity, and the Evaluation of Public Policy Nobel Lecture.pdf:pdf},
	issn = {23281235},
	journal = {Journal of Political Economy}, 
	number = {4},
	pages = {673--748},
	title = {{Micro Data, Heterogeneity and the Evaluation of Public Policy: Nobel Lecture}},
	volume = {109},
	year = {2001}
}
@article{Browning2010,
	abstract = {We consider dynamic discrete choice models with heterogeneity in both the levels parameter and the state dependence parameter. We first present an empirical analysis that motivates the theoretical analysis which follows. The theoretical analysis considers a simple two-state, first-order Markov chain model without covariates in which both transition probabilities are heterogeneous. Using such a model we are able to derive exact small sample results for bias and mean squared error (MSE). We discuss the maximum likelihood approach and derive two novel estimators. The first is a bias corrected version of the Maximum Likelihood Estimator (MLE) although the second, which we term MIMSE, minimizes the integrated mean square error. The MIMSE estimator is always well defined, has a closed-form expression and inherits the desirable large sample properties of the MLE. Our main finding is that in almost all short panel contexts the MIMSE significantly outperforms the other two estimators in terms of MSE. A final section extends the MIMSE estimator to allow for exogenous covariates. {\textcopyright} The Author(s). Journal compilation {\textcopyright} Royal Economic Society 2010.},
	author = {Browning, Martin and Carro, Jesus M.},
	doi = {10.1111/j.1368-423X.2009.00301.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Heterogeneity/Browning, Carro (2010) Heterogeneity in dynamic discrete choice models.pdf:pdf},
	issn = {13684221},
	journal = {The Econometrics Journal},
	keywords = {Binary choice,Fixed effects,Heterogeneous slopes,Panel data,Unobserved heterogeneity},
	mendeley-groups = {Discrete Choice/Heterogeneity},
	number = {1},
	pages = {1--39},
	title = {{Heterogeneity in dynamic discrete choice models}},
	volume = {13},
	year = {2010}
}
@article{Browning2007,
	abstract = {INTRODUCTION There is general agreement that there is a good deal of heterogeneity in observed behavior. Heckman in his Nobel lecture (Heckman, 2001) states: “the most important discovery [from the widespread use of micro-data is] the evidence on the pervasiveness of heterogeneity and diversity in economic life.” This is true but to see it in print as a “discovery” is a surprise since we have internalized it so thoroughly and it is now second nature for anyone working with microdata to consider heterogeneity. We have been unable to find a consensus definition of heterogeneity. A definition we suggest (which derives from Cunha, Heckman, and Navarro, 2005) is that heterogeneity is the dispersion in factors that are relevant and known to individual agents when making a particular decision. Latent heterogeneity would then be those relevant factors that are known to the agent but not to the researcher. The heterogeneity could be differences in tastes, beliefs, abilities, skills, or constraints. Note that this definition does not impose that heterogeneity is constant over time for a given individual nor that because something is fixed and varies across the population that it is necessarily heterogeneous. Examples of the former would be changing information sets and an example of the latter would be, say, some genetic factor which impacts on outcomes but which is unobserved by any agent. Thus a “fixed effect” in an econometric model may or may not be consistent with heterogeneity, as defined here. Our definition of heterogeneity distinguishes it clearly from uncertainty, measurement error, and model misspecification that are other candidates for the variation we see around the predictions of a given deterministic model.},
	author = {Browning, Martin and Carro, Jesus M.},
	doi = {10.1017/CBO9780511607547.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Cross sections/Browning, Carro (2007) Heterogeneity and Microeconometrics Modeling.pdf:pdf},
	isbn = {9780511607547},
	journal = {Advances in Economics and Econometrics: Theory and Applications, Ninth World Congress, Volume III},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {47--74},
	title = {{Heterogeneity and Microeconometrics Modeling}},
	year = {2007}
}
@article{Efron2011,
	author = {Efron, Bradley},
	doi = {10.1198/jasa.2011.tm11181},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Shrinkage/Efron (2013) Tweedie's Formula and Selection Bias.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	keywords = {bayesian relevance,empirical bayes information,false discovery rates,james,regret,stein},
	mendeley-groups = {Shrinkage},
	number = {496},
	pages = {1602--1614},
	title = {{Tweedie's Formula and Selection Bias}},
	volume = {106},
	year = {2011}
}
@article{Weinstein2018,
	abstract = {The problem of estimating the mean of a normal vector with known but unequal variances introduces substantial difficulties that impair the adequacy of traditional empirical Bayes estimators. By taking a different approach that treats the known variances as part of the random observations, we restore symmetry and thus the effectiveness of such methods. We suggest a group-linear empirical Bayes estimator, which collects observations with similar variances and applies a spherically symmetric estimator to each group separately. The proposed estimator is motivated by a new oracle rule which is stronger than the best linear rule, and thus provides a more ambitious benchmark than that considered in the previous literature. Our estimator asymptotically achieves the new oracle risk (under appropriate conditions) and at the same time is minimax. The group-linear estimator is particularly advantageous in situations where the true means and observed variances are empirically dependent. To demonstrate the merits of the proposed methods in real applications, we analyze the baseball data used by Brown (2008), where the group-linear methods achieved the prediction error of the best nonparametric estimates that have been applied to the dataset, and significantly lower error than other parametric and semiparametric empirical Bayes estimators.},
	author = {Weinstein, Asaf and Ma, Zhuang and Brown, Lawrence D. and Zhang, Cun Hui},
	doi = {10.1080/01621459.2017.1280406},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Shrinkage/Weinstein et al. (2017) Group Linear Empirical Bayes Estimates for a Heteroscedastic Normal Mean.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Asymptotic optimality,Compound decision,Empirical Bayes,Heteroscedasticity,Shrinkage estimator},
	mendeley-groups = {Shrinkage},
	number = {522},
	pages = {698--710},
	publisher = {Taylor {\&} Francis},
	title = {{Group-Linear Empirical Bayes Estimates for a Heteroscedastic Normal Mean}},
	volume = {113},
	year = {2018}
}
 
@article{Nagashima2019,
	abstract = {Prediction intervals are commonly used in meta-analysis with random-effects models. One widely used method, the Higgins–Thompson–Spiegelhalter prediction interval, replaces the heterogeneity parameter with its point estimate, but its validity strongly depends on a large sample approximation. This is a weakness in meta-analyses with few studies. We propose an alternative based on bootstrap and show by simulations that its coverage is close to the nominal level, unlike the Higgins–Thompson–Spiegelhalter method and its extensions. The proposed method was applied in three meta-analyses.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1804.01054v4},
	author = {Nagashima, Kengo and Noma, Hisashi and Furukawa, Toshi A.},
	doi = {10.1177/0962280218773520},
	eprint = {arXiv:1804.01054v4},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Nagashima, Noma, Furukawa (2018) Prediction Intervals for RE Meta-Analysis Confidence Distribution Approach.pdf:pdf},
	issn = {14770334},
	journal = {Statistical Methods in Medical Research},
	keywords = {Confidence distributions,coverage properties,meta-analysis,prediction intervals,random-effects models},
	mendeley-groups = {Meta-Analysis},
	number = {6},
	pages = {1689--1702},
	pmid = {29745296},
	title = {{Prediction Intervals for Random-Effects Meta-Analysis: A Confidence Distribution Approach}},
	volume = {28},
	year = {2019}
}


@article{Muller2017,
	abstract = {We consider inference about tail properties of a distribution from an iid sample, based on extreme value theory. All of the numerous previous suggestions rely on asymptotics where eventually, an infinite number of observations from the tail behave as predicted by extreme value theory, enabling the consistent estimation of the key tail index, and the construction of confidence intervals using the delta method or other classic approaches. In small samples, however, extreme value theory might well provide good approximations for only a relatively small number of tail observations. To accommodate this concern, we develop asymptotically valid confidence intervals for high quantile and tail conditional expectations that only require extreme value theory to hold for the largest k observations, for a given and fixed k. Small-sample simulations show that these “fixed-k” intervals have excellent small-sample coverage properties, and we illustrate their use with mainland U.S. hurricane data. In addition, we provide an analytical result about the additional asymptotic robustness of the fixed-k approach compared to kn → ∞ inference.},
	author = {M{\"{u}}ller, Ulrich K. and Wang, Yulong},
	doi = {10.1080/01621459.2016.1215990},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Muller, Wang (2017) Fixed k Asymptotic Inference About Tail Properties.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Extreme quantiles,extreme value distribution,tail conditional expectations},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	number = {519},
	pages = {1334--1343},
	title = {{Fixed-k Asymptotic Inference About Tail Properties}},
	volume = {112},
	year = {2017}
}


@article{Einmahl2000,
	abstract = {We use general empirical process methods to determine under mild regularity conditions exact rates of uniform strong consistency of kernel-type function estimators. In the process a useful new bound on the expectation of the supremum of the empirical process is obtained.},
	author = {Einmahl, Uwe and Mason, David M.},
	doi = {10.1023/A:1007769924157},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Einmahl, Mason (2000) An Empirical Process Approach to the Uniform Consistency of Kernel-Type Function Estimators.pdf:pdf},
	issn = {08949840},
	journal = {Journal of Theoretical Probability},
	keywords = {Empirical process,Estimators,Strongly consistent},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {1},
	pages = {1--37},
	title = {{An Empirical Process Approach to the Uniform Consistency of Kernel-Type Function Estimators}},
	volume = {13},
	year = {2000}
}
@book{Folland1999,
	author = {Folland, Gerald B.},
	isbn = {978-0-471-31716-6},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {416},
	publisher = {Wiley},
	title = {{Real Analysis: Modern Techniques and Their Applications}},
	year = {1999}
}


@book{Horn2012,
	author = {Horn, Roger A. and Johnson, Charles R.},
	doi = {10.1017/CBO9781139020411},
	edition = {2},
	isbn = {9780521548236},
	mendeley-groups = {Random Useful Math/Linear Algebra},
	publisher = {Cambridge University Press},
	title = {{Matrix Analysis}},
	year = {2012}
}

@article{Mack1982,
	abstract = {We study the estimation of a regression function by the kernel method. Under mild conditions on the "window", the "bandwidth" and the underlying distribution of the bivariate observations {\{}(Xi, Yi){\}}, we obtain the weak and strong uniform convergence rates on a bounded interval. These results parallel those of Silverman (1978) on density estimation and extend those of Schuster and Yakowitz (1979) and Collomb (1979) on regression estimation. {\textcopyright} 1982 Springer-Verlag.},
	author = {Mack, Y. P. and Silverman, Bernard W.},
	doi = {10.1007/BF00539840},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Mack, Silverman (1982) Weak and strong uniform consistency of kernel regression estimates.pdf:pdf},
	issn = {00443719},
	journal = {Zeitschrift f{\"{u}}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {3},
	pages = {405--415},
	title = {{Weak and Strong Uniform Consistency of Kernel Regression Estimates}},
	volume = {61},
	year = {1982}
}
@article{Hansen2008rates,
	abstract = {This paper presents a set of rate of uniform consistency results for kernel estimators of density functions and regressions functions. We generalize the existing literature by allowing for stationary strong mixing multivariate data with infinite support, kernels with unbounded support, and general bandwidth sequences. These results are useful for semiparametric estimation based on a first-stage nonparametric estimator. {\textcopyright} 2008 Cambridge University Press.},
	author = {Hansen, Bruce E.},
	doi = {10.1017/S0266466608080304},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Hansen (2008) Uniform Convergence Rates for Kernel Estimation With Dependent Data.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {3},
	pages = {726--748},
	title = {{Uniform Convergence Rates for Kernel Estimation with Dependent Data}},
	volume = {24},
	year = {2008}
}

@article{Tardella2001,
	abstract = {The k-truncated moment class $\Gamma$(mk)={\{}$\pi$∈P: mi=∫Ixi$\pi$(dx),i=1,...,k{\}} of all probability distributions $\pi$ on a compact interval I of the real line which have the same first k moments is considered. This paper derives some remarkable properties of the ranges of (k+h)th moments which allow to provide bounds for the diameter of $\Gamma$(mk) for a suitable probability metric. {\textcopyright} 2001 Elsevier Science B.V.},
	author = {Tardella, Luca},
	doi = {10.1016/S0167-7152(00)00205-4},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Tardella (2000) A note on estimating the diameter of a truncated moment class.pdf:pdf},
	issn = {01677152},
	journal = {Statistics and Probability Letters},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {2},
	pages = {115--124},
	title = {{A Note on Estimating the Diameter of a Truncated Moment Class}},
	volume = {54},
	year = {2001}
}

@unpublished{Arras2017,
	abstract = {In this paper, we propose a general means of estimating the rate at which convergences in law occur. Our approach, which is an extension of the classical Stein-Tikhomirov method, rests on a new pair of linear operators acting on characteristic functions. In principle, this method is admissible for any approximating sequence and any target, although obviously the conjunction of several favorable factors is necessary in order for the resulting bounds to be of interest. As we briefly discuss, our approach is particularly promising whenever some version of Stein's method applies. We apply our approach to two examples. The first application concerns convergence in law towards targets {\$}F{\_}\backslashinfty{\$} which belong to the second Wiener chaos (i.e. {\$}F{\_}{\{}\backslashinfty{\}}{\$} is a linear combination of independent centered chi-squared rvs). We detail an application to {\$}U{\$}-statistics. The second application concerns convergence towards targets belonging to the generalized Dickman family of distributions. We detail an application to a theorem from number theory. In both cases our method produces bounds of the correct order (up to a logarithmic loss) in terms of quantities which occur naturally in Stein's method.},
	archivePrefix = {arXiv},
	arxivId = {1605.06819},
	author = {Arras, Benjamin and Mijoule, Guillaume and Poly, Guillaume and Swan, Yvik},
	eprint = {1605.06819},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Weak Convergence/Arras et al. (2016) A new approach to the Stein-Tikhomirov method.pdf:pdf},
	keywords = {60e07,60e10,60f05,60g50,bias transformations,characteristic function,dickman dis-,limit theorems,msc 2010,stable law,tribution,wiener chaos},
	mendeley-groups = {Random Useful Math/Probability/Weak Convergence},
	pages = {1--45},
	title = {{A New Approach to the Stein-Tikhomirov Method: with Applications to the Second Wiener Chaos and Dickman Convergence}},
	year = {2017}
}

@book{Ibragimov1971,
	author = {Ibragimov, Ildar A. and Linnik, Yury V.},
	isbn = {9789001418854},
	mendeley-groups = {Random Useful Math/Probability/Weak Convergence},
	publisher = {Wolters-Noordhoff},
	title = {{Independent and Stationary Sequences of Random Variables}},
	year = {1971}
}

@book{Ushakov2011,
	author = {Ushakov, Nikolai G.},
	doi = {10.1515/9783110935981},
	isbn = {9783110935981},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination},
	publisher = {De Gruyter},
	title = {{Selected Topics in Characteristic Functions}},
	year = {2011}
}

@article{Gibbs2002,
	abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
	archivePrefix = {arXiv},
	arxivId = {math/0209021},
	author = {Gibbs, Alison L. and Su, Francis Edward},
	doi = {10.1111/j.1751-5823.2002.tb00178.x},
	eprint = {0209021},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Weak Convergence/Gibbs, Su (2002) On Choosing and Bounding Probability Metrics.pdf:pdf},
	issn = {03067734},
	journal = {International Statistical Review},
	keywords = {Discrepancy,Hellinger distance,Probability metrics,Prokhorov metric,Rates of convergence,Relative entropy,Wasserstein distance},
	mendeley-groups = {Random Useful Math/Probability/Weak Convergence},
	number = {3},
	pages = {419--435},
	primaryClass = {math},
	title = {{On Choosing and Bounding Probability Metrics}},
	volume = {70},
	year = {2002}
}

@book{Klebanov2006,
	author = {Klebanov, Lev Borisovich and Kozubowski, Tomasz J and Rachev, Svetlozar Todorov},
	isbn = {160021262X},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination},
	publisher = {Nova Publishers},
	title = {{Ill-Posed Problems in Probability and Stability of Random Sums}},
	year = {2006}
}

@book{Dudley1999,
	abstract = {This book shows how the central limit theorem for independent, identically distributed random variables with values in general, multidimensional spaces, holds uniformly over some large classes of functions. The author, an acknowledged expert, gives a thorough treatment of the subject, including several topics not found in any previous book, such as the Fernique-Talagrand majorizing measure theorem for Gaussian processes, an extended treatment of Vapnik-Chervonenkis combinatorics, the Ossiander L2 bracketing central limit theorem, the Gin{\'{e}}-Zinn bootstrap central limit theorem in probability, the Bronstein theorem on approximation of convex sets, and the Shor theorem on rates of convergence over lower layers. Other results of Talagrand and others are surveyed without proofs in separate sections. Problems are included at the end of each chapter so the book can be used as an advanced text. The book will interest mathematicians working in probability, mathematical statisticians and computer scientists working in computer learning theory.},
	address = {Cambridge},
	author = {Dudley, Richard M.},
	booktitle = {Cambridge Studies in Advanced Mathematics},
	doi = {10.1017/CBO9780511665622},
	isbn = {9780511665622},
	mendeley-groups = {Random Useful Math/Probability/Stochastic Processes,Random Useful Math/Probability/Limit Theorems},
	publisher = {Cambridge University Press},
	title = {{Uniform Central Limit Theorems}},
	year = {1999}
}

@article{Frontini2011,
	abstract = {Different existence conditions of the Maximum Entropy solution to finite Hausdorff moment problem have been formulated in literature. Through a counterexample we prove that the most cited one is uncorrect. We do not bound ourselves to a crude counterexample, as we think that a detailed explanation is of interest by itself. It clarifies the difference existing between the finite and infinite Hausdorff moment problem existence conditions. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
	author = {Frontini, M. and Tagliani, A.},
	doi = {10.1016/j.amc.2011.05.082},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Frontini, Tagliani (2011) Hausdorff moment problem and Maximum Entropy On the existence conditions.pdf:pdf},
	issn = {00963003},
	journal = {Applied Mathematics and Computation},
	keywords = {Completely monotonic sequence,Hankel matrices,Hausdorff moment problem,Maximum Entropy,Moments space},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {2},
	pages = {430--433},
	publisher = {Elsevier Inc.},
	title = {{Hausdorff Moment Problem and Maximum Entropy: On the Existence Conditions}},
	volume = {218},
	year = {2011}
}

@article{Mead1984,
	abstract = {The maximum-entropy approach to the solution of underdetermined inverse problems is studied in detail in the context of the classical moment problem. In important special cases, such as the Hausdorff moment problem, we establish necessary and sufficient conditions for the existence of a maximum-entropy solution and examine the convergence of the resulting sequence of approximations. A number of explicit illustrations are presented. In addition to some elementary examples, we analyze the maximum-entropy reconstruction of the density of states in harmonic solids and of dynamic correlation functions in quantum spin systems. We also briefly indicate possible applications to the Lee-Yang theory of Ising models, to the summation of divergent series, and so on. The general conclusion is that maximum entropy provides a valuable approximation scheme, a serious competitor of traditional Pad{\'{e}}-like procedures. {\textcopyright} 1984 American Institute of Physics.},
	author = {Mead, Lawrence R. and Papanicolaou, N.},
	doi = {10.1063/1.526446},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Mead, Papanicolaou (1984) Maximum entropy in the problem of moments.pdf:pdf},
	issn = {00222488},
	journal = {Journal of Mathematical Physics},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems,Random Useful Math/Information Theory/Maximum Entropy},
	number = {8},
	pages = {2404--2417},
	title = {{Maximum Entropy in the Problem of Moments}},
	volume = {25},
	year = {1984}
}

@article{Mnatsakanov2008b,
	abstract = {The problem of approximation of the moment-determinate cumulative distribution function (cdf) from its moments is studied. This method of recovering an unknown distribution is natural in certain incomplete models like multiplicative-censoring or biased sampling when the moments of unobserved distributions are related in a simple way to the moments of an observed distribution. In this article some properties of the proposed construction are derived. The uniform and L1-rates of convergence of the approximated cdf to the target distribution are obtained. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Mnatsakanov, Robert M.},
	doi = {10.1016/j.spl.2008.01.011},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Mnatsakanov (2008) Hausdorff moment problem Reconstruction of distributions.pdf:pdf},
	issn = {01677152},
	journal = {Statistics and Probability Letters},
	keywords = {Hausdorff moment problem,L1-rate of approximation,Moment-recovered distribution,Uniform rate of approximation},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {12},
	pages = {1612--1618},
	title = {{Hausdorff Moment Problem: Reconstruction of Distributions}},
	volume = {78},
	year = {2008}
}

@article{Viano1991,
	abstract = {We present a new method for solving the Hausdorff moment problem which makes use of Pollaczek polynomials. This problem is ill-posed in the sense of Hadamard and therefore it is unstable from a numerical viewpoint. We shall prove that the expansion, which we obtain, is asymptotically convergent in the L2-norm when the data (i.e., the moments) are perturbed by noise and finite in number. This result is largely used in the numerical analysis of the problem. {\textcopyright} 1991.},
	author = {Viano, Giovanni Alberto},
	doi = {10.1016/0022-247X(91)90406-P},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Viano (1991) Solution of the Hausdorff moment problem by the use of Pollaczek polynomials.pdf:pdf},
	issn = {10960813},
	journal = {Journal of Mathematical Analysis and Applications},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {2},
	pages = {410--427},
	title = {{Solution of the Hausdorff Moment Problem by the Use of Pollaczek Polynomials}},
	volume = {156},
	year = {1991}
}


@book{Schmudgen2017,
	author = {Schm{\"{u}}dgen, Konrad},
	doi = {10.1007/978-3-319-64546-9},
	isbn = {978-3-319-64545-2},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	publisher = {Springer},
	title = {{The Moment Problem}},
	year = {2017}
}
 
@article{Chen2012EstimationNonparametricConditional,
  title = {Estimation of {{Nonparametric Conditional Moment Models With Possibly Nonsmooth Generalized Residuals}}},
  author = {Chen, Xiaohong and Pouzo, Demian},
  year = {2012},
  journal = {Econometrica},
  volume = {80},
  number = {1},
  pages = {277--321},
  issn = {0012-9682},
  doi = {10.3982/ecta7888},
  abstract = {This paper studies nonparametric estimation of conditional moment restrictions in which the generalized residual functions can be nonsmooth in the unknown functions of endogenous variables. This is a nonparametric nonlinear instrumental variables (IV) problem. We propose a class of penalized sieve minimum distance (PSMD) estimators, which are minimizers of a penalized empirical minimum distance criterion over a collection of sieve spaces that are dense in the infinite-dimensional function parameter space. Some of the PSMD procedures use slowly growing finite-dimensional sieves with flexible penalties or without any penalty; others use large dimensional sieves with lower semicompact and/or convex penalties. We establish their consistency and the convergence rates in Banach space norms (such as a sup-norm or a root mean squared norm), allowing for possibly noncompact infinite-dimensional parameter spaces. For both mildly and severely ill-posed nonlinear inverse problems, our convergence rates in Hilbert space norms (such as a root mean squared norm) achieve the known minimax optimal rate for the nonparametric mean IV regression. We illustrate the theory with a nonparametric additive quantile IV regression. We present a simulation study and an empirical application of estimating nonparametric quantile IV Engel curves. {\copyright} 2012 The Econometric Society.},
  file = {D:\Academic Things\Econometrics\Articles\Nonparametric Estimation\Sieve Estimation\Chen, Pouzo (2012) Estimation of Nonparametric Conditional Moment Models With Possibly Nonsmooth Generalized.pdf}
}

@article{Fox2011SimpleEstimatorDistribution,
  title = {A {{Simple Estimator}} for the {{Distribution}} of {{Random Coefficients}}},
  author = {Fox, Jeremy T. and {il Kim}, Kyoo and Ryan, Stephen P. and Bajari, Patrick},
  year = {2011},
  journal = {Quantitative Economics},
  volume = {2},
  number = {3},
  pages = {381--418},
  issn = {17597323},
  doi = {10.3982/qe49},
  abstract = {We propose a simple mixtures estimator for recovering the joint distribution of parameter heterogeneity in economic models, such as the random coefficients logit. The estimator is based on linear regression subject to linear inequality con-straints, and is robust, easy to program, and computationally attractive compared to alternative estimators for random coefficient models. For complex structural models, one does not need to nest a solution to the economic model during op-timization. We present a Monte Carlo study and an empirical application to dy-namic programming discrete choice with a serially correlated unobserved state variable.},
  file = {D:\Academic Things\Econometrics\Articles\Discrete Choice\Multiple\Fox, Kim, Ryan, Bajari (2011) A simple estimator for the distribution of random coefficients.pdf}
}

@article{Fox2012RandomCoefficientsLogit,
  title = {The {{Random Coefficients Logit Model}} Is {{Identified}}},
  author = {Fox, Jeremy T. and Kim, Kyoo Il and Ryan, Stephen P. and Bajari, Patrick},
  year = {2012},
  journal = {Journal of Econometrics},
  volume = {166},
  number = {2},
  pages = {204--212},
  publisher = {Elsevier B.V.},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2011.09.002},
  abstract = {The random coefficients multinomial choice logit model, also known as the mixed logit, has been widely used in empirical choice analysis for the last thirty years. We prove that the distribution of random coefficients in the multinomial logit model is nonparametrically identified. Our approach requires variation in product characteristics only locally and does not rely on the special regressors with large supports used in related papers. One of our two identification arguments is constructive. Both approaches may be applied to other choice models with random coefficients. {\copyright} 2011 Elsevier B.V. All rights reserved.},
  file = {D:\Academic Things\Econometrics\Articles\Discrete Choice\Multiple\Fox, Kim, Ryan, Bajari (2011) The random coefficients logit model is identified.pdf}
}

@article{Fox2016SimpleNonparametricApproach,
  title = {A {{Simple Nonparametric Approach}} to {{Estimating}} the {{Distribution}} of {{Random Coefficients}} in {{Structural Models}}},
  author = {Fox, Jeremy T. and {il Kim}, Kyoo and Yang, Chenyu},
  year = {2016},
  journal = {Journal of Econometrics},
  volume = {195},
  number = {2},
  pages = {236--254},
  publisher = {Elsevier B.V.},
  issn = {18726895},
  doi = {10.1016/j.jeconom.2016.05.018},
  abstract = {We explore least squares and likelihood nonparametric mixtures estimators of the joint distribution of random coefficients in structural models. The estimators fix a grid of heterogeneous parameters and estimate only the weights on the grid points, an approach that is computationally attractive compared to alternative nonparametric estimators. We provide conditions under which the estimated distribution function converges to the true distribution in the weak topology on the space of distributions. We verify most of the consistency conditions for three discrete choice models. We also derive the convergence rates of the least squares nonparametric mixtures estimator under additional restrictions. We perform a Monte Carlo study on a dynamic programming model.},
  keywords = {Discrete choices,Dynamic programming,Mixtures,Random coefficients,Sieve estimation},
  file = {D:\Academic Things\Econometrics\Articles\Panel Data\Heterogeneous Panels\Cross sections\Fox, Kim, Yang (2016)  A simple nonparametric approach to estimating the distribution of random coefficients in structural models.pdf}
}

@article{Wallis1986,
	author = {Wallis, Kenneth F.},
	doi = {10.1002/for.3980050102},
	journal = {Journal of Forecasting},
	mendeley-groups = {Forecasting/Nowcasting},
	number = {1},
	pages = {1--13},
	title = {{Forecasting with an Econometric Model: The ‘Ragged Edge' Problem}},
	volume = {5},
	year = {1986}
}

@article{Beran1996,
	abstract = {An experiment records stimulus and response for a random sample of cases. The relationship between response and stimulus is thought to be linear, the values of the slope and intercept varying by case. From such data, we construct a consistent, asymptotically normal, nonparametric estimator for the joint density of the slope and intercept. Our methodology incorporates the radial projection-slice theorem for the Radon transform, a technique for locally linear nonparametric regression and a tapered Fourier inversion. Computationally, the new density estimator is more feasible than competing nonparametric estimators, one of which is based on moments and the other on minimum distance considerations.},
	author = {Beran, Rudolf and Feuerverger, Andrey and Hall, Peter},
	doi = {10.1214/aos/1032181170},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Cross sections/Beran, Feuerverger, Hall (1996) On nonparametric estimation of intercept and slope distributions in random coefficient regression.pdf:pdf},
	issn = {00905364},
	journal = {The Annals of Statistics},
	keywords = {Characteristic function,Computerized tomography,Local linear regression,Projection-slice theorem,Radon transform,Tapered fourier inversion},
	mendeley-groups = {Random Coefficients/Cross-Section},
	number = {6},
	pages = {2569--2592},
	title = {{On Nonparametric Estimation of Intercept and Slope Distributions in Random Coefficient Regression}},
	volume = {24},
	year = {1996}
}

@book{Koenker2005,
	author = {Koenker, Roger},
	doi = {10.1017/CBO9780511754098},
	isbn = {9780511754098},
	mendeley-groups = {Quantile/Fixed Quantile Regression},
	publisher = {Cambridge University Press},
	title = {{Quantile Regression}},
	year = {2005}
}

@article{Kazemi2017,
	abstract = {In this work the following problem is discussed: Unknown are two objects, a probability density (corresponding to a distribution, or to a probability measure) f and its support supp{\{}f{\}} = [L, U] which is assumed to be bounded/compact. Available is the sequence of all integer order moments of f and the goal is to recover the support [L, U] or both the support [L, U] and the density f. This paper provides the sequences involving integer order moments that approximate the compact support of a probability measure. The proposed sequences are based on the ratios of subsequent moments and they converge faster than the previously suggested sequences representing the n-th roots of n-th moments. As a result, the approximation of a probability density function with unknown support is proposed. The convergence of newly defined approximations of the boundaries and the densities are demonstrated through various examples, including the case of noisy data.},
	author = {Kazemi, Amir and Shahdoosti, Hamid Reza and Mnatsakanov, Robert M.},
	doi = {10.1515/jiip-2016-0022},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Kazemi, Shahdoosti, Mnatsakanov (2017) Hausdorff moment problem Recovery of an unknown support for a probability density function.pdf:pdf},
	issn = {09280219},
	journal = {Journal of Inverse and Ill-Posed Problems},
	keywords = {Inverse Hausdorff moment problem,compact support,contaminated sample,density estimator,moment-determinate probability measure},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {6},
	pages = {719--731},
	title = {{Hausdorff moment problem: Recovery of an unknown support for a probability density function}},
	volume = {25},
	year = {2017}
}
@article{Mnatsakanov2003,
	author = {Mnatsakanov, Robert M. and Ruymgaart, Frits H.},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Mnatsakanov, Ruymgaart (2003) Some properties of moment-empirical cdf's with application to some inverse estimation problems.pdf:pdf},
	journal = {Mathematical Methods of Statistics},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {4},
	pages = {478--495},
	title = {{Some Properties of Moment-Empirical CDFs with Application to Some Inverse Estimation Problems}},
	volume = {12},
	year = {2003}
}
@incollection{Mnatsakanov2009,
	abstract = {The problem of recovering a cumulative distribution function (cdf) and corresponding density function from its moments is studied. This problem is a special case of the classical moment problem. The results obtained within the moment problem can be applied in many indirect models, e.g., those based on convolutions, mixtures, multiplicative censoring, and right-censoring, where the moments of unobserved distribution of actual interest can be easily estimated from the transformed moments of the observed distributions. Nonparametric estimation of a quantile function via moments of a target distribution represents another very interesting area where the moment problem arises. In all such models one can apply the present results to recover a function via its moments. In this article some properties of the proposed constructions are derived. The uniform rates of convergence of the approximation of cdf, its density function, quantile and quantile density function are obtained as well.},
	author = {Mnatsakanov, Robert M. and Hakobyan, Artak S.},
	booktitle = {IMS Lecture Notes - Monograph Series, Volume 57: Optimality: The Third Erich L. Lehmann Symposium},
	doi = {10.1214/09-lnms5715},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Mnatsakanov, Hakobyan (2009) Recovery of Distributions via Moments.pdf:pdf},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	pages = {252--265},
	title = {{Recovery of Distributions via Moments}},
	volume = {57},
	year = {2009}
}
@article{Mnatsakanov2008,
	abstract = {The problem of recovering a moment-determinate probability density function (pdf) from its moments is studied. The proposed construction provides a method for recovery of different pdfs via simple transformations of the moment sequences. Uniform and L1-rates of convergence of moment-recovered pdfs are obtained. Finally, some applications and examples are briefly discussed. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Mnatsakanov, Robert M.},
	doi = {10.1016/j.spl.2008.01.054},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Mnatsakanov (2008) Hausdorff Moment Problem Reconstruction of Pdfs.pdf:pdf},
	issn = {01677152},
	journal = {Statistics and Probability Letters},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {13},
	pages = {1869--1877},
	title = {{Hausdorff Moment Problem: Reconstruction of Probability Density Functions}},
	volume = {78},
	year = {2008}
}
@unpublished{Ponomareva2010,
	author = {Ponomareva, Maria},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Panel QR/Ponomareva (2010) Quantile Regression for Panel Data Models with Fixed.pdf:pdf},
	mendeley-groups = {Panel Data/Panel QR,Quantile/Fixed Quantile Regression,Quantile/Panel QR},
	pages = {1--19},
	title = {{Quantile Regression for Panel Data Models with Fixed Effects and Small T: Identification and Estimation}},
	year = {2010}
}

@book{Aliprantis2006,
	author = {Aliprantis, Charalambos D. and Border, Kim C.},
	doi = {10.1007/3-540-29587-9},
	edition = {3},
	isbn = {978-3-540-29586-0},
	mendeley-groups = {Random Useful Math/Operator Theory and Functional Analysis},
	publisher = {Springer Berlin Heidelberg},
	title = {{Infinite Dimensional Analysis: A Hitchhiker's Guide}},
	year = {2006}
}

@unpublished{Tjur1975,
	author = {Tjur, Tue},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Tjur (1970) A Constructive Definition of Conditional Probabilities.pdf:pdf},
	mendeley-groups = {Random Useful Math/Probability/Conditional Expectations},
	pages = {1--21},
	title = {{A Constructive Definition of Conditional Distributions}},
	year = {1975}
}
@unpublished{Wacker2020,
	abstract = {It is often of interest to condition on a singular event given by a random variable, e.g. {\$}\backslash{\{}Y=y\backslash{\}}{\$} for a continuous random variable {\$}Y{\$}. Conditional measures with respect to this event are usually derived as a special case of the conditional expectation with respect to the random variables generating sigma algebra. The existence of the latter is usually proven via a non-constructive measure-theoretic argument which yields an only almost-everywhere defined quantity. In particular, the quantity {\$}\backslashmathbb E[f|Y]{\$} is initially only defined almost everywhere and conditioning on {\$}Y=y{\$} corresponds to evaluating {\$}\backslashmathbb E[f|Y=y] = \backslashmathbb E[f|Y]{\{}Y=y{\}}{\$}, which is not meaningful because of {\$}\backslashmathbb E[f|Y]{\$} not being well-defined on such singular sets. This problem is not addressed by the introduction of regular conditional distributions, either. On the other hand it can be shown that the naively computed conditional density {\$}f{\_}{\{}Z|Y=y{\}}(z){\$} (which is given by the ratio of joint and marginal densities) is a version of the conditional distribution, i.e. {\$}\backslashmathbb E[\backslash{\{}Z\backslashin B\backslash{\}}|Y=y] = \backslashint{\_}B f{\_}{\{}Z|Y=y{\}}(z) dz{\$} and this density can indeed be evaluated pointwise in {\$}y{\$}. This mismatch between mathematical theory (which generates an object which cannot produce what we need from it) and practical computation via the conditional density is an unfortunate fact. Furthermore, the classical approach does not allow a pointwise definition of conditional expectations of the form {\$}\backslashmathbb E[f|Y=y]{\$}, only of conditional distributions {\$}\backslashmathbb E[\backslash{\{}Z\backslashin B\backslash{\}}|Y=y]{\$}. We propose a (as far as the author is aware) little known approach to obtaining a pointwise defined version of conditional expectation by use of the Lebesgue-Besicovich lemma without the need of additional topological arguments which are necessary in the usual derivation.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:2007.01635v1},
	author = {Wacker, Philipp},
	eprint = {arXiv:2007.01635v1},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Wacker (2020) Pointwise defined version of conditional expectation.pdf:pdf},
	mendeley-groups = {Random Useful Math/Probability/Conditional Expectations},
	pages = {1--15},
	title = {{Pointwise Defined Version of Conditional Expectation with Respect to a Random Variable.}},
	year = {2020}
}

@article{Sasaki2021,
	abstract = {Panel data often contain stayers (units with no within-variations) and slow movers (units with little within-variations). In the presence of many slow movers, conventional econometric methods can fail to work. We propose a novel method of robust inference for the average partial effects in correlated random coefficient models robustly across various distributions of within-variations, including the cases with many stayers and/or many slow movers in a unified manner. In addition to this robustness property, our proposed method entails smaller biases and hence improves accuracy in inference compared to existing alternatives. Simulation studies demonstrate our theoretical claims about these properties: the conventional 95{\%} confidence interval covers the true parameter value with 37-93{\%} frequencies, whereas our proposed one achieves 93-96{\%} coverage frequencies.},
	archivePrefix = {arXiv},
	arxivId = {2110.12041},
	author = {Sasaki, Yuya and Ura, Takuya},
	eprint = {2110.12041},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Sasaki, Ura (2022) Slow Movers in Panel Data.pdf:pdf},
	keywords = {bias reduction,c14,c23,c33,correlated random coefficient,jel classification codes,panel data,robustness,slow movers},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {1--40},
	title = {{Slow Movers in Panel Data}},
	year = {2021}
}

@article{Gabaix2016,
	author = {Gabaix, Xavier},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Power Laws/Gabaix (2016) Power Laws in Economics An Introduction.pdf:pdf},
	journal = {Journal of Economic Perspectives},
	mendeley-groups = {Applied (By Econometrics)/Power Laws},
	number = {1},
	pages = {185--205},
	title = {{Power Laws in Economics: An Introduction}},
	volume = {30},
	year = {2016}
}
@article{Gabaix2009,
	author = {Gabaix, Xavier},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Power Laws/Gabaix (2009) Power Laws in Economics and Finance.pdf:pdf},
	journal = {Annual Review of Economics},
	keywords = {crashes,fat tails,scaling,superstars},
	mendeley-groups = {Applied (By Econometrics)/Power Laws},
	pages = {255--293},
	title = {{Power Laws in Economics and Finance}},
	volume = {1},
	year = {2009}
}

@article{Veroniki2016,
	abstract = {Meta-analyses are typically used to estimate the overall/mean of an outcome of interest. However, inference about between-study variability, which is typically modelled using a between-study variance parameter, is usually an additional aim. The DerSimonian and Laird method, currently widely used by default to estimate the between-study variance, has been long challenged. Our aim is to identify known methods for estimation of the between-study variance and its corresponding uncertainty, and to summarise the simulation and empirical evidence that compares them. We identified 16 estimators for the between-study variance, seven methods to calculate confidence intervals, and several comparative studies. Simulation studies suggest that for both dichotomous and continuous data the estimator proposed by Paule and Mandel and for continuous data the restricted maximum likelihood estimator are better alternatives to estimate the between-study variance. Based on the scenarios and results presented in the published studies, we recommend the Q-profile method and the alternative approach based on a 'generalised Cochran between-study variance statistic' to compute corresponding confidence intervals around the resulting estimates. Our recommendations are based on a qualitative evaluation of the existing literature and expert consensus. Evidence-based recommendations require an extensive simulation study where all methods would be compared under the same scenarios.},
	author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and Higgins, Julian Pt and Langan, Dean and Salanti, Georgia},
	doi = {10.1002/jrsm.1164},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Veroniki et al. (2015). Methods to estimate the between-study variance and its uncertainty in meta-analysis.pdf:pdf},
	issn = {17592887},
	journal = {Research Synthesis Methods},
	keywords = {Bias,Confidence interval,Coverage probability,Heterogeneity,Mean squared error},
	mendeley-groups = {Meta-Analysis},
	number = {1},
	pages = {55--79},
	pmid = {26332144},
	title = {{Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis}},
	volume = {7},
	year = {2016}
}
@article{Card2001,
	abstract = {This paper reviews a set of recent studies that have attempted to measure the causal effect of education on labor market earnings by using institutional features of the supply side of the education system as exogenous determinants of schooling outcomes. A simple theoretical model that highlights the role of comparative advantage in the optimal schooling decision is presented and used to motivate an extended discussion of econometric issues, including the properties of ordinary least squares and instrumental variables estimators. A review of studies that have used compulsory schooling laws, differences in the accessibility of schools, and similar features as instrumental variables for completed education, reveals that the resulting estimates of the return to schooling are typically as big or bigger than the corresponding ordinary least squares estimates. One interpretation of this finding is that marginal returns to education among the low-education subgroups typically affected by supply-side innovations tend to be relatively high, reflecting their high marginal costs of schooling, rather than low ability that limits their return to education.},
	author = {Card, David},
	doi = {10.1111/1468-0262.00237},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Education/Card (2001) Estimating the Return to Schooling Progress on Some Persistent Econometric.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Ability bias,Random coefficients,Returns to education},
	mendeley-groups = {Applied (By Field)/Education,Applied (By Econometrics)/Heterogeneous Coefficients},
	number = {5},
	pages = {1127--1160},
	title = {{Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems}},
	volume = {69},
	year = {2001}
}

@article{Borenstein2017,
	abstract = {When we speak about heterogeneity in a meta-analysis, our intent is usually to understand the substantive implications of the heterogeneity. If an intervention yields a mean effect size of 50 points, we want to know if the effect size in different populations varies from 40 to 60, or from 10 to 90, because this speaks to the potential utility of the intervention. While there is a common belief that the I2 statistic provides this information, it actually does not. In this example, if we are told that I2 is 50{\%}, we have no way of knowing if the effects range from 40 to 60, or from 10 to 90, or across some other range. Rather, if we want to communicate the predicted range of effects, then we should simply report this range. This gives readers the information they think is being captured by I2 and does so in a way that is concise and unambiguous. Copyright {\textcopyright} 2017 John Wiley {\&} Sons, Ltd.},
	author = {Borenstein, Michael and Higgins, Julian P.T. and Hedges, Larry V. and Rothstein, Hannah R.},
	doi = {10.1002/jrsm.1230},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Borenstein et al. (2017) Basics of meta-analysis I2is not an absolute measure of heterogeneity.pdf:pdf},
	issn = {17592887},
	journal = {Research Synthesis Methods},
	keywords = {I-squared,I2,heterogeneity,inconsistency,meta-analysis,prediction intervals},
	mendeley-groups = {Meta-Analysis},
	number = {1},
	pages = {5--18},
	pmid = {28058794},
	title = {{Basics of Meta-Analysis: $I^2$ Is Not An Absolute Measure of Heterogeneity}},
	volume = {8},
	year = {2017}
}

@article{Viechtbauer2007,
	abstract = {Effect size estimates to be combined in a systematic review are often found to be more variable than one would expect based on sampling differences alone. This is usually interpreted as evidence that the effect sizes are heterogeneous. A random-effects model is then often used to account for the heterogeneity in the effect sizes. A novel method for constructing confidence intervals for the amount of heterogeneity in the effect sizes is proposed that guarantees nominal coverage probabilities even in small samples when model assumptions are satisfied. A variety of existing approaches for constructing such confidence intervals are summarized and the various methods are applied to an example to illustrate their use. A simulation study reveals that the newly proposed method yields the most accurate coverage probabilities under conditions more analogous to practice, where assumptions about normally distributed effect size estimates and known sampling variances only hold asymptotically. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	author = {Viechtbauer, Wolfgang},
	doi = {10.1002/sim.2514},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Viechtbauer (2006) Confidence intervals for the amount of heterogeneity in meta-analysis.pdf:pdf},
	issn = {02776715},
	journal = {Statistics in Medicine},
	keywords = {Confidence intervals,Heterogeneity,Meta-analysis,Random-effects model},
	mendeley-groups = {Meta-Analysis},
	number = {1},
	pages = {37--52},
	pmid = {16463355},
	title = {{Confidence Intervals for the Amount of Heterogeneity in Meta-Analysis}},
	volume = {26},
	year = {2007}
}

@article{Higgins2002,
	abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the X2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of H that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity. Copyright {\textcopyright} 2002 John Wiley {\&} Sons, Ltd.},
	author = {Higgins, Julian P.T. and Thompson, Simon G.},
	doi = {10.1002/sim.1186},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Higgins, Thompson (2002) Quantifying heterogeneity in a meta-analysis.pdf:pdf},
	issn = {02776715},
	journal = {Statistics in Medicine},
	keywords = {Heterogeneity,Meta-analysis},
	mendeley-groups = {Meta-Analysis},
	number = {11},
	pages = {1539--1558},
	pmid = {12111919},
	title = {{Quantifying Heterogeneity in a Meta-Analysis}},
	volume = {21},
	year = {2002}
}
@book{Pollard1984,
	author = {Pollard, David},
	doi = {10.1007/978-1-4612-5254-2},
	isbn = {978-1-4612-9758-1},
	mendeley-groups = {Random Useful Math/Probability/Stochastic Processes},
	publisher = {Springer New York},
	title = {{Convergence of Stochastic Processes}},
	year = {1984}
}

@article{Higgins2009,
	author = {Higgins, Julian P.T. and Thompson, Simon G. and Spiegelhalter, David J.},
	doi = {10.1111/j.1467-985x.2008.00552.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Meta-Analysis/RE/Higgins, Thompson,  Spiegelhalter  (2009) A re-evaluation of random-effects meta-analysis.pdf:pdf},
	journal = {Journal of the Royal Statistical Society: Series A},
	keywords = {meta-analysis,prediction,random-effects models,systematic reviews},
	mendeley-groups = {Meta-Analysis},
	number = {1},
	pages = {137--159},
	pmid = {19381330},
	title = {{A Re-Evaluation of Random-Effects Meta-Analysis}},
	volume = {172},
	year = {2009}
}

@book{Kallenberg2021,
	author = {Kallenberg, Olav},
	doi = {10.1007/978-3-030-61871-1},
	edition = {3},
	isbn = {978-3-030-61870-4},
	pages = {1--946},
	publisher = {Springer Cham},
	title = {{Foundations of Modern Probability}},
	year = {2021}
}

@article{Davis2011,
	author = {Davis, Lucas W. and Kilian, Lutz},
	doi = {10.1002/jae.1156},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Energy/Gasoline/Davis, Kilian (2010). Estimating the effect of a gasoline tax on carbon emissions.pdf:pdf},
	journal = {Journal of Applied Business and Economics},
	mendeley-groups = {Climate Enviro and Energy/Energy Demand},
	number = {7},
	pages = {1187--1214},
	title = {{Estimating the Effect of a Gasoline Tax on Carbon Emissions}},
	volume = {26},
	year = {2011}
}

@article{Blundell2012b,
	author = {Blundell, Richard and Horowitz, Joel L. and Parey, Matthias},
	doi = {10.3982/qe91},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Demand Estimation/Blundell, Horowtz, Parey (2012) Measuring the price responsiveness of gasoline demand Economic shape restrictions.pdf:pdf},
	issn = {17597323},
	journal = {Quantitative Economics},
	keywords = {C140,Consumer demand,D120,H310,deadweight loss,gasoline demand,nonparametric estimation},
	mendeley-groups = {Demand Estimation},
	number = {1},
	pages = {29--51},
	title = {{Measuring The Price Responsiveness of Gasoline Demand: Economic Shape Restrictions and Nonparametric Demand Estimation}},
	volume = {3},
	year = {2012}
}

@article{Dennig2015,
	abstract = {Integrated assessment models of climate and the economy provide estimates of the social cost of carbon and inform climate policy. We create a variant of the Regional Integrated model of Climate and the Economy (RICE)-a regionally disaggregated version of the Dynamic Integrated model of Climate and the Economy (DICE)-in which we introduce a more fine-grained representation of economic inequalities within the model's regions. This allows us to model the common observation that climate change impacts are not evenly distributed within regions and that poorer people are more vulnerable than the rest of the population. Our results suggest that this is important to the social cost of carbon-as significant, potentially, for the optimal carbon price as the debate between Stern and Nordhaus on discounting.},
	author = {Dennig, Francis and Budolfson, Mark B. and Fleurbaey, Marc and Siebert, Asher and Socolow, Robert H.},
	doi = {10.1073/pnas.1513967112},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Climate Change/Dennig (2015) Inequality, climate impacts on the future poor,and carbon prices.pdf:pdf},
	issn = {10916490},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	keywords = {Climate change,Damage distribution,Inequality,RICE,Social cost of carbon},
	mendeley-groups = {Climate Enviro and Energy/Climate Change},
	number = {52},
	pages = {15827--15832},
	pmid = {26644560},
	title = {{Inequality, Climate Impacts on the Future Poor, and Carbon Prices}},
	volume = {112},
	year = {2015}
}

@book{Nordhaus2013,
	abstract = {This survey examines the history and current practice in integrated assessment models (IAMs) of the economics of climate change. It begins with a review of the emerging problem of climate change. The next section provides a brief sketch of the rise of IAMs in the 1970s and beyond. The subsequent section is an extended exposition of one IAM e the DICE/RICE family of models. The purpose of this description is to provide readers an example of how such a model is developed and what the major components are. The final section discusses major important open questions that continue to occupy IAM modelers. These involve issues such as the discount rate, uncertainty, the social cost of carbon, the potential for catastrophic climate change, algorithms and fat-tailed distributions. These issues are the ones that pose both deep intellectual challenges as well as important policy implications for climate change and climate change policy.},
	author = {Nordhaus, William},
	booktitle = {Handbook of Computable General Equilibrium Modeling},
	doi = {10.1016/B978-0-444-59568-3.00016-X},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Climate Change/Nordhaus (2013) Integrated Economic and Climate Modeling.pdf:pdf},
	isbn = {9780444595683},
	issn = {22116885},
	keywords = {Climate change,Economics of climate change,Energy models,Integrated assessment models,Large-scale economic modeling,Social cost of carbon},
	mendeley-groups = {Climate Enviro and Energy/Climate Change},
	pages = {1069--1131},
	publisher = {Elsevier},
	title = {{Integrated Economic and Climate Modeling}},
	volume = {1},
	year = {2013}
}

@article{Pesaran2006,
	abstract = {This paper presents a new approach to estimation and inference in panel data models with a general multifactor error structure. The unobserved factors and the individual-specific errors are allowed to follow arbitrary stationary processes, and the number of unobserved factors need not be estimated. The basic idea is to filter the individual-specific regressors by means of cross-section averages such that asymptotically as the cross-section dimension (N) tends to infinity, the differential effects of unobserved common factors are eliminated. The estimation procedure has the advantage that it can be computed by least squares applied to auxiliary regressions where the observed regressors are augmented with cross-sectional averages of the dependent variable and the individual-specific regressors. A number of estimators (referred to as common correlated effects (CCE) estimators) are proposed and their asymptotic distributions are derived. The small sample properties of mean group and pooled CCE estimators are investigated by Monte Carlo experiments, showing that the CCE estimators have satisfactory small sample properties even under a substantial degree of heterogeneity and dynamics, and for relatively small values of N and T.},
	author = {Pesaran, M. Hashem},
	doi = {10.1111/j.1468-0262.2006.00692.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Factor Models/Pesaran (2006) Estimation and Inference in Large Heterogeneous Panels with a Multifactor Error Structure.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Common correlated effects,Cross-section dependence,Estimation and inference,Heterogeneity,Large panels},
	mendeley-groups = {Panel Data/Factor Models and Interactive Effects},
	number = {4},
	pages = {967--1012},
	title = {{Estimation and Inference in Large Heterogeneous Panels with a Multifactor Error Structure}},
	volume = {74},
	year = {2006}
}

@article{Dietz2019,
	abstract = {We exploit recent advances in climate science to develop a physically consistent, yet surprisingly simple, model of climate policy. It seems that key economic models have greatly overestimated the delay between carbon emissions and warming, and ignored the saturation of carbon sinks that takes place when the atmospheric concentration of carbon dioxide rises. This has important implications for climate policy. If carbon emissions are abated, damages are avoided almost immediately. Therefore it is optimal to reduce emissions significantly in the near term and bring about a slow transition to optimal peak warming, even if optimal steady-state/peak warming is high. The optimal carbon price should start relatively high and grow relatively fast.},
	author = {Dietz, Simon and Venmans, Frank},
	doi = {10.1016/j.jeem.2019.04.003},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Climate Change/Dietz, Venmans (2019) Cumulative carbon emissions and economic policy In search of general principles.pdf:pdf},
	issn = {10960449},
	journal = {Journal of Environmental Economics and Management},
	keywords = {Carbon price,Climate change,Cumulative emissions,Peak warming,Social cost of carbon},
	mendeley-groups = {Climate Enviro and Energy/Climate Change},
	pages = {108--129},
	publisher = {Elsevier Inc.},
	title = {{Cumulative Carbon Emissions and Economic Policy: In Search of General Principles}},
	volume = {96},
	year = {2019}
}

@article{Golosov2014,
	abstract = {1. Macroeconomics and Monetary Economics - General Aggregative Models - Neoclassical 2. Macroeconomics and Monetary Economics - General Aggregative Models - Forecasting and Simulation 3. Macroeconomics and Monetary Economics - Macroeconomic Policy, Macroeconomic Aspects of Public Finance, and 4. Public Economics - Taxation, Subsidies, and Revenue - Externalities; Redistributive Effects; Environmental Taxes 5. Agricultural and Natural Resource Economics; Environmental and Ecological - Renewable Resources and Conservation - Government Policy 6. Agricultural and Natural Resource Economics; Environmental and Ecological - Energy 7. Agricultural and Natural Resource Economics; Environmental and Ecological - Environmental Economics},
	author = {Golosov, Mikhail and Hassler, John and Krusell, Per and Tsyvinski, Aleh},
	doi = {10.3982/ecta10217},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Optimal Fiscal Policy/Golosov et al. (2014) Optimal Taxes on Fossil Fuel in General Equilibrium.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Economics/Fiscal Policy/Energy,Climate Enviro and Energy/Fiscal Policy},
	number = {1},
	pages = {41--88},
	title = {{Optimal Taxes on Fossil Fuel in General Equilibrium}},
	volume = {82},
	year = {2014}
}

@article{Li2014,
	abstract = {Gasoline taxes can be employed to correct externalities from automobile use and to raise government revenue. Our understanding of the optimal gasoline tax and the efficacy of existing taxes is largely based on empirical analysis of consumer responses to gasoline price changes. In this paper, we examine directly how gasoline taxes affect gasoline consumption as distinct from tax-inclusive retail gasoline prices. We find robust evidence that consumers respond more strongly to gasoline tax changes under a variety of model specifications. We discuss two potential reasons for our main findings as well as their implications.},
	author = {Li, Shanjun and Linn, Joshua and Muehlegger, Erich},
	doi = {10.1257/pol.6.4.302},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Energy/Gasoline/Li, Linn, Muehlegger (2014) Gasoline Taxes and Consumer Behavior.pdf:pdf},
	issn = {1945774X},
	journal = {American Economic Journal: Economic Policy},
	mendeley-groups = {Climate Enviro and Energy/Taxes/Gasoline},
	number = {4},
	pages = {302--342},
	title = {{Gasoline Taxes and Consumer Behavior}},
	volume = {6},
	year = {2014}
}

@article{Fang2022,
	abstract = {Theoretical results of frequentist model averaging mainly focus on asymptotic optimality and asymptotic distribution of the model averaging estimator. However, even for basic least squares model averaging, many theoretical problems have not been well addressed yet. This article discusses asymptotic properties of a class of least squares model averaging methods with nested candidate models that includes the Mallows model averaging (MMA) of Hansen (2007, Econometrica 75, 1175-1189) as a special case. Two scenarios are considered: (i) all candidate models are under-fitted; and (ii) the true model is included in the candidate models. We find that in the first scenario, the least squares model averaging method asymptotically assigns weight one to the largest candidate model and the resulting model averaging estimator is asymptotically normal. In the second scenario with a slightly special weight space, if the penalty factor in the weight selection criterion is diverging with certain order, the model averaging estimator is asymptotically optimal by putting weight one to the true model. However, MMA with fixed model dimensions is not asymptotically optimal since it puts nonnegligible weights to over-fitted models. The theoretical results are clearly summarized with their restrictions, and some critical implications are discussed. Monte Carlo simulations confirm our theoretical results.},
	author = {Fang, Fang and Yuan, Chaoxia and Tian, Wenling},
	doi = {10.1017/S0266466622000032},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Fang, Yuan, Tian (2022) AN ASYMPTOTIC THEORY FOR LEAST SQUARES MODEL AVERAGING WITH NESTED MODELS.pdf:pdf},
	issn = {14694360},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	pages = {1--30},
	title = {{An Asymptotic Theory for Least Squares Model Averaging with Nested Models}},
	year = {2022}
}

@article{Hsiang2018,
	author = {Hsiang, Solomon and Kopp, Robert E.},
	doi = {10.1257/jep.32.4.3},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Climate Change/Hsiang, Copp (2019) An Economist's Guide to Climate Change Science.pdf:pdf},
	issn = {08953309},
	journal = {Journal of Economic Perspectives},
	mendeley-groups = {Climate Enviro and Energy/Climate Change},
	number = {4},
	pages = {3--32},
	title = {{An Economist's Guide to Climate Change Science}},
	volume = {32},
	year = {2018}
}

@book{CAEF2009,
	abstract = {For multi-user PDF licensing, please contact customer service. Energy touches our lives in countless ways and its costs are felt when we fill up at the gas pump, pay our home heating bills, and keep businesses both large and small running. There are long-term costs as well: to the environment, as natural resources are depleted and pollution contributes to global climate change, and to national security and independence, as many of the world's current energy sources are increasingly concentrated in geopolitically unstable regions. The country's challenge is to develop an energy portfolio that addresses these concerns while still providing sufficient, affordable energy reserves for the nation. The United States has enormous resources to put behind solutions to this energy challenge; the dilemma is to identify which solutions are the right ones. Before deciding which energy technologies to develop, and on what timeline, we need to understand them better. America's Energy Future analyzes the potential of a wide range of technologies for generation, distribution, and conservation of energy. This book considers technologies to increase energy efficiency, coal-fired power generation, nuclear power, renewable energy, oil and natural gas, and alternative transportation fuels. It offers a detailed assessment of the associated impacts and projected costs of implementing each technology and categorizes them into three time frames for implementation.},
	address = {Washington, DC},
	author = {{Committee on America's Energy Future}, .},
	doi = {10.17226/12091},
	isbn = {978-0-309-11602-2},
	keywords = {Energy and Energy Conservation,Environment and Environmental Studies},
	language = {English},
	mendeley-groups = {Climate Enviro and Energy/Policy Books},
	publisher = {The National Academies Press},
	title = {{America's Energy Future: Technology and Transformation}},
	year = {2009}
}


@article{Szekely2000,
	author = {Sz{\'{e}}kely, G{\'{a}}bor J. and Rao, C.R.},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Szekely, Rao (2000) Identifiability of Distributions of Independent Random Variables by Linear.pdf:pdf},
	journal = {Sankhyā},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {2},
	pages = {193--202},
	title = {{Identifiability of Distributions of Independent Random Variables by Linear Combinations and Moments}},
	volume = {62},
	year = {2000}
}

@article{Lindsay2000,
	author = {Lindsay, Bruce G. and Basak, Prasanta},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Moment Problem/Lundsay, Basak (2000) Moments Determine the Tail of a Distribution (But Not Much Else).pdf:pdf},
	journal = {The American Statistician},
	keywords = {1,c,distribution bounds,l,m7-lvp,moment matrix,moments,vvi,where mp is the,wp,x},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Moment Problems},
	number = {4},
	pages = {248--251},
	title = {{Moments Determine the Tail of a Distribution (But Not Much Else)}},
	volume = {54},
	year = {2000}
}

@article{Blundell2012,
	abstract = {The optimal design of low-income support is examined using a structural labour supply model. The approach incorporates unobserved heterogeneity, fixed costs of work, childcare costs and the detailed non-convexities of the tax and transfer system. The analysis considers purely Pareto improving reforms and also optimal design under social welfare functions with different degrees of inequality aversion. We explore the gains from tagging and also examine the case for the use of hours-contingent payments. Using the tax schedule for lone parents in the U.K. as our policy environment, the results point to a reformed non-linear tax schedule with tax credits only optimal for low earners. The results also suggest a welfare improving role for tagging according to child age and for hours-contingent payments, although the case for the latter is mitigated when hours cannot be monitored or recorded accurately by the tax authorities. {\textcopyright} The Author 2011. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
	author = {Blundell, Richard and Shephard, Andrew},
	doi = {10.1093/restud/rdr034},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Blundell, Shephard (2012) Employment, Hours of Work and the Optimal Taxation of Low-Income Families.pdf:pdf},
	issn = {1467937X},
	journal = {Review of Economic Studies},
	keywords = {Discrete choice,Labour supply,Optimal taxation},
	mendeley-groups = {Economics/Labor},
	number = {2},
	pages = {481--510},
	title = {{Employment, Hours of Work and the Optimal Taxation of Low-Income Families}},
	volume = {79},
	year = {2012}
}

@book{Krantz2002,
	author = {Krantz, Steven G. and Parks, Harold R.},
	doi = {10.1007/978-0-8176-8134-0},
	isbn = {9780817642648},
	mendeley-groups = {Random Useful Math/Analysis},
	pages = {209},
	publisher = {Birkh{\"{a}}user Boston},
	series = {Birkh{\"{a}}user Advanced Texts},
	title = {{A Primer of Real Analytic Functions}},
	year = {2002}
}

@article{Ran2012,
	abstract = {The finite section method for infinite Vandermonde matrices is the focus of this paper. In particular, it is shown that for a large class of infinite Vandermonde matrices the finite section method converges in l1 sense if the right hand side of the equation is in a suitably weighted l1 ($\alpha$) space. Some explicit results are obtained for a wide class of examples. {\textcopyright} 2012 Royal Dutch Mathematical Society (KWG).},
	author = {Ran, Andr{\'{e}} C.M. and Ser{\'{e}}ny, Andr{\'{a}}s},
	doi = {10.1016/j.indag.2012.05.009},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Functional Analysis and Operator Theory/Infinite Systems of Equations/Ran, Ser{\'{e}}ny, A. (2012) The finite section method for infinite Vandermonde matrices.pdf:pdf},
	issn = {00193577},
	journal = {Indagationes Mathematicae},
	keywords = {Finite section method,Infinite Vandermonde matrix,Infinite systems of equations},
	mendeley-groups = {Random Useful Math/Operator Theory and Functional Analysis/Infinite Systems of Linear Equations},
	number = {4},
	pages = {884--899},
	title = {{The Finite Section Method for Infinite Vandermonde Matrices}},
	volume = {23},
	year = {2012}
}
@article{Arkhangelsky2022,
	archivePrefix = {arXiv},
	arxivId = {1909.09412v3},
	author = {Arkhangelsky, Dmitry and Imbens, Guido W.},
	doi = {10.1093/ectj/utac019},
	eprint = {1909.09412v3},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Panel/Arkhangelsky, Imbens (2022) Double-Robust Identification for Causal Panel.pdf:pdf},
	journal = {The Econometrics Journal},
	keywords = {causal effects,clustering,cross-section data,fixed effects,treatment effects,uncon-},
	mendeley-groups = {Treatment Effects/Panel},
	pages = {1--50},
	title = {{Double-Robust Identification for Causal Panel Data Models}},
	year = {2022}
}

@article{Belisle1997,
	author = {Belisle, Claude and Masse, Jean-Claude and Ransford, Thomas},
	doi = {10.1214/aop/1024404418},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Cramer-Wold theorems/Belisle, Masse, Ransford (1997) When is a Probability Measure Determined by Infinitely Many Projections.pdf:pdf},
	journal = {The Annals of Probability},
	mendeley-groups = {Random Useful Math/Probability/Distribution Determination/Cramer-Wold},
	number = {2},
	pages = {767--786},
	title = {{When is a Probability Measure Determined by Infinitely Many Projections?}},
	volume = {25},
	year = {1997}
}

@article{Masten2018,
	abstract = {This article considers a classical linear simultaneous equations model with random coefficients on the endogenous variables. Simultaneous equations models are used to study social interactions, strategic interactions between firms, and market equilibrium. Random coefficient models allow for heterogeneous marginal effects. I show that random coefficient seemingly unrelated regression models with common regressors are not point identified, which implies random coefficient simultaneous equations models are not point identified. Important features of these models, however, can be identified. For two-equation systems, I give two sets of sufficient conditions for point identification of the coefficients' marginal distributions conditional on exogenous covariates. The first allows for small support continuous instruments under tail restrictions on the distributions of unobservables which are necessary for point identification. The second requires full support instruments, but allows for nearly arbitrary distributions of unobservables. I discuss how to generalize these results to many equation systems, where I focus on linear-in-means models with heterogeneous endogenous social interaction effects. I give sufficient conditions for point identification of the distributions of these endogenous social effects. I propose a consistent nonparametric kernel estimator for these distributions based on the identification arguments. I apply my results to the Add Health data to analyse peer effects in education.},
	author = {Masten, Matthew A.},
	doi = {10.1093/restud/rdx047},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Cross sections/Masten (2017) Random Coefficients on Endogenous Variables in Simultaneous Equations Models.pdf:pdf},
	issn = {1467937X},
	journal = {Review of Economic Studies},
	keywords = {Instrumental variables,Non-parametric identification,Random coefficients,Simultaneous equations models,Social interactions},
	mendeley-groups = {Random Coefficients,Random Coefficients/Cross-Section},
	number = {2},
	pages = {1193--1250},
	title = {{Random Coefficients on Endogenous Variables in Simultaneous Equations Models}},
	volume = {85},
	year = {2018}
}

@article{Beran1992,
	author = {Beran, Rudolf and Hall, Peter},
	doi = {10.1214/aos/1176348898},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Cross sections/Beran, Hall (1992)  Estimating coefficient distributions in random coefficient regressions.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Random Coefficients/Cross-Section},
	number = {4},
	pages = {1970--1984},
	title = {{Estimating Coefficient Distributions in Random Coefficient Regressions}},
	volume = {20},
	year = {1992}
}

@article{Ito2014,
	abstract = {Nonlinear pricing and taxation complicate economic decisions by creating multiple marginal prices for the same good. This paper provides a framework to uncover consumers' perceived price of nonlinear price schedules. I exploit price variation at spatial discontinuities in electricity service areas, where households in the same city experience substantially different nonlinear pricing. Using household-level panel data from administrative records, I find strong evidence that consumers respond to average price rather than marginal or expected marginal price. This suboptimizing behavior makes nonlinear pricing unsuccessful in achieving its policy goal of energy conservation and critically changes the welfare implications of nonlinear pricing. Copyright {\textcopyright} 2014 by the American Economic Association.},
	author = {Ito, Koichiro},
	doi = {10.1257/aer.104.2.537},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Energy/Ito (2014) Do Consumers Respond to Marginal or Average Price.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {Economics/Energy,Climate Enviro and Energy/Energy Demand},
	number = {2},
	pages = {537--563},
	title = {{Do Consumers Respond to Marginal or Average Price? Evidence From Nonlinear Electricity Pricing}},
	volume = {104},
	year = {2014}
}

@article{Hausman2017,
	abstract = {Exact consumer's surplus and deadweight loss are the most widely used welfare and economic efficiency measures. These measures can be computed from demand functions in straightforward ways. Nonparametric estimation can be used to estimate the welfare measures. In doing so, it seems important to account correctly for unobserved heterogeneity, given the high degree of unexplained demand variation often found in applications. This review surveys work on nonparametric welfare analysis, focusing on work that allows for general heterogeneity in demand, such as that of Hausman {\&} Newey (2016).},
	author = {Hausman, Jerry A. and Newey, Whitney K.},
	doi = {10.1146/annurev-economics-080315-015107},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Welfare/Hausman, Newey (2017). Nonparametric Welfare Analysis.pdf:pdf},
	issn = {19411391},
	journal = {Annual Review of Economics},
	keywords = {Consumer surplus,Deadweight loss,Identification,Quantiles},
	mendeley-groups = {Economics/Welfare},
	number = {2016},
	pages = {521--546},
	title = {{Nonparametric Welfare Analysis}},
	volume = {9},
	year = {2017}
}
@article{Kitagawa2018,
	abstract = {One of the main objectives of empirical analysis of experiments and quasi-experiments is to inform policy decisions that determine how treatments are allocated to individuals with different observable covariates. We propose the Empirical Welfare Maximization (EWM) method, which estimates a treatment assignment policy by maximizing the sample analog of average social welfare over a class of candidate treatment policies. We show that, when propensity score is known, the average social welfare attained by EWM rules converges at least at n{\{}{\^{}}{\}}{\{}{\{}{\}}-1/2{\{}{\}}{\}} rate to the maximum obtainable welfare. This holds uniformly over a minimally constrained class of data distributions, and this uniform convergence rate is minimax optimal. In comparison with this benchmark rate, we examine how the uniform convergence rate of the average welfare improves or deteriorates depending on the richness of the class of candidate decision rules, on the distribution of conditional treatment effects, on the lack of knowledge for the propensity score, and on additional smoothness assumptions for the regression functions or propensity scores. We also discuss practically implementable computation for the EWM rule. As an empirical application, we derive an EWM rule for the a training program using the experiment data analyzed in La Londe (1986).},
	author = {Kitagawa, Toru and Tetenov, Aleksey},
	doi = {10.3982/ecta13288},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Treatment Assignment/Kitagawa, Tetenov (2018) Who Should Be Treated Empirical Welfare Maximization Methods for Treatment Choice.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	keywords = {Heterogeneous treatment effects, randomized experiments, program evaluation, individualized treatment rules, empirical risk minimization, risk bounds},
	mendeley-groups = {Treatment Effects/Assigning Treatments},
	number = {2},
	pages = {591--616},
	title = {{Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice}},
	volume = {86},
	year = {2018}
}

@article{Blomquist2021,
	abstract = {The elasticity of taxable income is vital when predicting the effect of taxes. Bunching at kinks/notches has been used to estimate this elasticity. We show that when the preference distribution is unrestricted, bunching at a kink or a notch is not informative about the size of the elasticity, and neither is the entire distribution of taxable income. Bunching identifies the taxable income elasticity when the preference distribution is correctly specified across the kink and provides bounds under restrictions on the preference distribution. We find wide bounds in an empirical example based on upper and lower bounds for the preference density.},
	author = {Blomquist, S{\"{o}}ren and Newey, Whitney K. and Kumar, Anil and Liang, Che Yuan},
	doi = {10.1086/714446},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Blomquist, Newey, Kumar, Liang (2021) On Bunching and Identification of the Taxable Income Elasticity.pdf:pdf},
	issn = {1537534X},
	journal = {Journal of Political Economy},
	mendeley-groups = {Economics/Fiscal Policy/Empirics},
	number = {8},
	pages = {2320--2343},
	title = {{On Bunching and Identification of the Taxable Income Elasticity}},
	volume = {129},
	year = {2021}
}

@article{Feldstein1999,
	author = {Feldstein, Martin},
	doi = {10.1162/003465399558391},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Feldstein (1999) Tax Avoidance and the Deadweight Loss of the Income Tax.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	mendeley-groups = {Economics/Fiscal Policy/Empirics},
	number = {4},
	pages = {674--680},
	title = {{Tax Avoidance and the Deadweight Loss of the Income}},
	volume = {81},
	year = {1999}
}

@article{Feldstein1995,
	abstract = {Federal tax reform in 1988 flattened the Canadian personal income tax schedule, changing the marginal tax rates for many individuals. Using methods similar to those applied by Auten and Carroll [Rev. Econ. 81(4) (1999) 681] in the study of the effects of the 1986 U.S. Tax Reform Act, we estimate the responsiveness of income to changes in taxes to be substantially smaller in Canada. However we find evidence of a much higher response in self-employment income, in the labor income of seniors and from those with high incomes. {\textcopyright} Elsevier Science S.A.},
	author = {Feldstein, Martin},
	doi = {10.1016/S0047-2727(00)00128-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Feldstein (1995) The Effect of Marginal Tax Rates on Taxable Income.pdf:pdf},
	issn = {00472727},
	journal = {Journal of Political Economy},
	keywords = {H24,Marginal tax rate effects on taxable income,Personal income and other nonbusiness taxes and su,Tax avoidance},
	mendeley-groups = {Economics/Fiscal Policy/Empirics},
	number = {3},
	pages = {551--572},
	title = {{The Effect of Marginal Tax Rates on Taxable Income: A Panel Study of the 1986 Tax Reform Act}},
	volume = {103},
	year = {1995}
}

@unpublished{Blomquist2015,
	abstract = {Many studies have estimated the effect of taxes on taxable income. To account for non- linear taxes these studies either use instrumental variables approaches that are not fully consistent or impose strong functional form assumptions. None allow for general hetero- geneity in preferences. In this paper we derive the expected value and distribution of taxable income conditional on a nonlinear budget set, allowing general heterogeneity and optimiza- tion error in taxable income. We find an important dimension reduction and use that to develop nonparametric estimation methods. We show how to nonparametrically estimate the expected value of taxable income imposing all the restrictions of utility maximization and allowing for measurement errors. We characterize what can be learned nonparamet- rically from kinks about compensated tax effects. We apply our results to Swedish data and estimate for prime age males a significant net of tax elasticity of 0.21 and a significant nonlabor income effect of about -1. The income effect is substantially larger in magnitude than it is found to be in other taxable income studies},
	author = {Blomquist, Soren and Kumar, Anil and Liang, Che-Yuan and Newey, Whitney K.},
	doi = {10.2139/ssrn.2603189},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Blomquist, Kumar, Liang, Newey (2015) Individual Heterogeneity, Nonlinear Budget Sets, and Taxable.pdf:pdf},
	keywords = {blundell,chamberlain,comments by r,fi nancial support,g,heterogeneous preferences,nonlinear budget sets,nonparametric estimation,revealed stochastic preference,taxable income,the nsf provided partial,we are grateful for},
	mendeley-groups = {Economics/Labor/Labor supply},
	title = {{Individual Heterogeneity, Nonlinear Budget Sets, and Taxable Income}},
	year = {2015}
}

@article{Facer2003,
	abstract = {We explore a nonparametric version of response surface analysis. Estimates for the location where maximum response occurs are proposed and their asymptotic distribution is investigated. The proposed estimates are based on kernel and local least squares methods. We construct asymptotic confidence regions for the location and include comparisons with the quadratic response surface approach. The methods are illustrated for the two-dimensional case with AIDS incidence data, where the point of maximum incidence is of interest. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
	author = {Facer, Matthew R. and M{\"{u}}ller, Hans Georg},
	doi = {10.1016/S0047-259X(03)00030-7},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Facer, M{\"{u}}ller (2003) Nonparametric Estimation of Location of Maximum in Response Surface.pdf:pdf},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {AIDS incidence,Kernel method,Local least squares,Maximal response,Nonparametric regression,Surface fitting},
	mendeley-groups = {Nonparametric Estimation/Zeros and Extrema},
	number = {1},
	pages = {191--217},
	title = {{Nonparametric Estimation of the Location of a Maximum in a Response Surface}},
	volume = {87},
	year = {2003}
}
@article{Belitser2012,
	abstract = {We propose a two-stage procedure for estimating the location $\mu$ and size M of the maximum of a smooth d-variate regression function f. In the first stage, a preliminary estimator of $\mu$ obtained from a standard nonparametric smoothing method is used. At the second stage, we "zoom-in" near the vicinity of the preliminary estimator and make further observations at some design points in that vicinity. We fit an appropriate polynomial regression model to estimate the location and size of the maximum. We establish that, under suitable smoothness conditions and appropriate choice of the zooming, the second stage estimators have better convergence rates than the corresponding first stage estimators of $\mu$ and M. More specifically, for $\alpha$-smooth regression functions, the optimal nonparametric rates n -($\alpha$-1)/(2$\alpha$+d)and n -$\alpha$/(2$\alpha$+d) at the first stage can be improved to n -($\alpha$-1)/(2$\alpha$) and n-1/2, respectively, for $\alpha$ {\textgreater} 1 + v1 +d/2. These rates are optimal in the class of all possible sequential estimators. Interestingly, the two-stage procedure resolves "the curse of the dimensionality" problem to some extent, as the dimension d does not control the second stage convergence rates, provided that the function class is sufficiently smooth. We consider a multi-stage generalization of our procedure that attains the optimal rate for any smoothness level $\alpha$ {\textgreater} 2 starting with a preliminary estimator with any power-law rate at the first stage. {\textcopyright} 2012 Institute of Mathematical Statistics.},
	author = {Belitser, Eduard and Ghosal, Subhashis and {Van Zanten}, Harry},
	doi = {10.1214/12-AOS1053},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Belitser, Ghosal, Zanten (2012) Optimal two-stage procedures for estimating location and size of the maximum of a multivariate regression function..pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Adaptive estimation,Multi-stage procedure,Optimal rate,Sequential design,Two-stage procedure},
	mendeley-groups = {Nonparametric Estimation/Zeros and Extrema},
	number = {6},
	pages = {2850--2876},
	title = {{Optimal Two-Stage Procedures for Estimating Location and Size of the Maximum of a Multivariate Regression Function}},
	volume = {40},
	year = {2012}
}

@article{Hausman2016,
	abstract = {Individual heterogeneity is an important source of variation in demand. Allowing for general heterogeneity is needed for correct welfare comparisons. We consider general heterogeneous demand where preferences and linear budget sets are statistically independent. Only the marginal distribution of demand for each price and income is identified from cross-section data where only one price and income is observed for each individual. Thus, objects that depend on varying price and/or income for an individual are not generally identified, including average exact consumer surplus. We use bounds on income effects to derive relatively simple bounds on the average surplus, including for discrete/continuous choice. We also sketch an approach to bounding surplus that does not use income effect bounds. We apply the results to gasoline demand. We find tight bounds for average surplus in this application, but wider bounds for average deadweight loss.},
	author = {Hausman, Jerry A. and Newey, Whitney K.},
	doi = {10.3982/ecta11899},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Special Applications/Welfare/Hausman, Newey (2016) Individual Heterogeneity and Average Welfare.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Nonparametric Estimation/Special Applications/Welfare},
	number = {3},
	pages = {1225--1248},
	title = {{Individual Heterogeneity and Average Welfare}},
	volume = {84},
	year = {2016}
}

@article{Hardle1987,
	author = {H{\"{a}}rdle, Wolfgang Karl and Nixdorf, Rainer},
	doi = {10.1109/TIT.1987.1057305},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Hardle, Nixdorf (1987) Nonparametric sequential estimation of zeros and extrema of regression functions.pdf:pdf},
	issn = {15579654},
	journal = {IEEE Transactions on Information Theory},
	mendeley-groups = {Nonparametric Estimation/Zeros and Extrema},
	number = {3},
	pages = {367--372},
	title = {{Nonparametric Sequential Estimation of Zeros and Extrema of Regression Functions}},
	volume = {33},
	year = {1987}
}

@article{Soest2002,
	abstract = {We show how non-parametric flexibility can be attained in a structural labour supply model that can be used to analyse all sorts of (non-linear) tax and benefits reforms. The direct utility function is approximated with a series expansion. For given length of the expansion, the model is estimated by smooth simulated maximum likelihood, using Dutch data on labour supply of married females. Estimates of own and cross wage elasticities and tax reform effects suggest that a series expansion of order two is enough. Monte Carlo simulations show that the estimator performs very well, unless there is measurement error in the hours variable. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
	author = {Soest, Arthur van and Das, Marcel and Gong, Xiaodong},
	doi = {10.1016/S0304-4076(01)00128-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Special Applications/Labor/van Soest, Das, Gong (2002) A structural labour supply model with flexible preferences.pdf:pdf},
	isbn = {3113466202},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Labour supply,Non-parametric estimation,Tax reforms},
	mendeley-groups = {Nonparametric Estimation/Special Applications/Demand and Labor Supply},
	number = {1-2},
	pages = {345--374},
	title = {{A Structural Labour Supply Model with Flexible Preferences}},
	volume = {107},
	year = {2002}
}

@article{Blundell1999,
	abstract = {Abstract This chapter surveys existing approaches to modeling labor supply and identifies important gaps in the literature that could be addressed in future research. The discussion begins with a look at recent policy reforms and labor market facts that motivate the study of ...},
	author = {Blundell, Richard and MaCurdy, Thomas},
	doi = {10.1016/S1573-4463(99)03008-4},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Blundell, Macurdy (1999) Labor Supply A Review of Alternative Approaches.pdf:pdf},
	isbn = {9780444501875},
	journal = {Handbook of Labor Economics},
	mendeley-groups = {Applied (By Field)/Labor,Economics/Labor/Labor supply},
	pages = {1559--1695},
	title = {{Labor Supply: a Review of Alternative Approaches}},
	volume = {3},
	year = {1999}
}

@article{Newey1984,
	abstract = {It is shown that sequential estimators can be interpreted as members of a class method of moments of estimators, and that this interpretation facilities derivation of asymptotic covariance matrices for two-step estimators. An example is given which deals with a two-step least squares estimator used to estimate rational expectations models. {\textcopyright} 1984.},
	author = {Newey, Whitney K.},
	doi = {10.1016/0165-1765(84)90083-1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Parametrics/Newey (1984) A method of moments interpretation of sequential estimators.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	mendeley-groups = {Limit Theory},
	number = {2-3},
	pages = {201--206},
	title = {{A Method of Moments Interpretation of Sequential Estimators}},
	volume = {14},
	year = {1984}
}

@article{Blomquist2002,
	author = {Blomquist, S{\"{o}}ren and Newey, Whitney K.},
	doi = {10.1111/j.1468-0262.2002.00445.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Special Applications/Blomquist, Newey (2002) Nonparametric Estimation with Nonlinear Budget Sets.pdf:pdf},
	journal = {Econometrica},
	keywords = {additive models,nonlinear budget sets,nonparametric estimation},
	mendeley-groups = {Nonparametric Estimation/Special Applications},
	number = {6},
	pages = {2455--2480},
	title = {{Nonparametric Estimation with Nonlinear Budget Sets}},
	volume = {70},
	year = {2002}
}

@article{Newey1999,
	abstract = {This paper presents a simple two-step nonparametric estimator for a triangular simultaneous equation model. Our approach employs series approximations that exploit the additive structure of the model. The first step comprises the nonparametric estimation of the reduced form and the corresponding residuals. The second step is the estimation of the primary equation via nonparametric regression with the reduced form residuals included as a regressor. We derive consistency and asymptotic normality results for our estimator, including optimal convergence rates. Finally we present an empirical example, based on the relationship between the hourly wage rate and annual hours worked, which illustrates the utility of our approach.},
	author = {Newey, Whitney K. and Powell, James L. and Vella, Francis},
	doi = {10.1111/1468-0262.00037},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonseparable Models/Newey, Powell, Vella (1999) Nonparametric Estimation of Triangular Simultaneous Equations Models.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Nonparametric estimation,Series estimation,Simultaneous equations,Two-step estimators},
	mendeley-groups = {Nonseparable Models},
	number = {3},
	pages = {565--603},
	title = {{Nonparametric Estimation of Triangular Simultaneous Equations Models}},
	volume = {67},
	year = {1999}
}
 

@article{Diamond1971,
	author = {Diamond, Peter A and Mirrlees, James A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Diamond, Mirrlees (1971) Optimal Taxation and Public Production I  Production Efficiency.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Economics/Fiscal Policy},
	number = {1},
	pages = {8--27},
	title = {{Optimal Taxation and Public Production I : Production Eficiency}},
	volume = {61},
	year = {1971}
}
@article{Diamond1971a,
	author = {Diamond, Peter A. and Mirrlees, James A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Fiscal Policy/Diamond, Mirrlees (1971) Optimal Taxation and Public Production II Tax Rules.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Economics/Fiscal Policy},
	number = {3},
	pages = {261--278},
	title = {{Optimal Taxation and Public Production II: Tax Rules}},
	volume = {61},
	year = {1971}
}

@article{Burtless1978,
	author = {Burtless, Gary and Hausman, Jerry A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Labor/Burtless, Hausman (1978) The Effect of Taxation on Labor Supply Evaluating the Gary Negative Tax Experiment.pdf:pdf},
	journal = {Journal of Political Economy},
	mendeley-groups = {Economics/Labor},
	number = {6},
	pages = {1103--1130},
	title = {{The Effect of Taxation on Labor Supply: Evaluating the Gary Negative Income Tax Experiment}},
	volume = {86},
	year = {1978}
}

@article{Hausman1981b,
	author = {Hausman, Jerry A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Economics/Welfare/Hausman (1981) Exact Consumer Surplus and Deadweight Loss.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Economics/Welfare},
	number = {4},
	pages = {662--676},
	title = {{Exact Consumer's Surplus and Deadweight Loss}},
	volume = {71},
	year = {1981}
}

@article{Hausman1995,
	author = {Hausman, Jerry A. and Newey, Whitney K.},
	doi = {10.2307/2171777},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Special Applications/Hausman, Newey (1995) Nonparametric Estimation of Exact Consumers Surplus and Deadweight Loss.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Nonparametric Estimation/Special Applications},
	number = {6},
	pages = {1445--1476},
	title = {{Nonparametric Estimation of Exact Consumers Surplus and Deadweight Loss}},
	volume = {63},
	year = {1995}
}

@InProceedings{Muller1984,
	author="M{\"u}ller, H.-G.",
	editor="Havr{\'a}nek, T.
	and {\v{S}}id{\'a}k, Z.
	and Nov{\'a}k, M.",
	title="Boundary Effects in Nonparametric Curve Estimation Models",
	booktitle="Compstat 1984",
	year="1984",
	publisher="Physica-Verlag HD",
	address="Heidelberg",
	pages="84--89",
	abstract="Boundary effects disturb nonparametric curve estimates, especially of derivatives, near the boundaries of the support of the curve. Modifications of estimates near the boundaries are necessary in order to obtain global asymptotic results as well as a satisfying finite sample behavior. We describe such modifications exhibiting different degrees of smoothness and derive the rate of a.s. convergence of the supremal deviation of nonparametric kernel regression, supremum taken over the deviation on the whole interval of support of the function to be estimated. A ``reference kernel method'' is proposed which allows the construction of modified kernels of different degrees of smoothness. This method requires substantially less computations than previous approaches.",
	isbn="978-3-642-51883-6"
}
@article{Muller1985,
 	author = {M{\"{u}}ller, Hans-Georg},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/M{\"{u}}ller (1985) Kernel Estimators of Zeros and of Location and Size of Extrema of Regression.pdf:pdf},
 	journal = {Scandinavian Journal of Statistics},
 	keywords = {asymptotic normality,empirical zeros and extrema,estimation of derivatives,non-parametric regression,rates of convergence},
 	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
 	number = {3},
 	pages = {221--232},
 	title = {{Kernel Estimators of Zeros and of Location and Size of Extrema of Regression Functions}},
 	volume = {12},
 	year = {1985}
 }
@incollection{Hardle1994,
	title = {Chapter 38 Applied nonparametric methods},
	series = {Handbook of Econometrics},
	publisher = {Elsevier},
	volume = {4},
	pages = {2295-2339},
	year = {1994},
	issn = {1573-4412},
	doi = {10.1016/S1573-4412(05)80007-8},
	author = {Wolfgang Härdle and Oliver Linton},
	abstract = {We review different approaches to nonparametric density and regression estimation. Kernel estimators are motivated from local averaging and solving ill-posed problems. Kernel estimators are compared to k-NN estimators, orthogonal series and splines. Pointwise and uniform confidence bands are described, and the choice of smoothing parameter is discussed. Finally, the method is applied to nonparametric prediction of time series and to semiparametric estimation.}
}
 @article{Gasser1984,
 	author = {Gasser, Theo and M{\"{u}}ller, Hans-Georg},
 	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Gasser, M{\"{u}}ller (1984) Estimating Regression Functions and Their Derivatives by the Kernel Method.pdf:pdf},
 	journal = {Scandinavian Journal of Statistics},
 	keywords = {applications,asymptotic normality,estimation of derivatives,finite sample results,nonparametric regression,rates of convergence},
 	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
 	number = {3},
 	pages = {171--185},
 	title = {{Estimating Regression Functions and Their Derivatives by the Kernel Method}},
 	volume = {11},
 	year = {1984}
 }
 
@article{Kneip1992,
	author = {Kneip, Alois and Gasser, Theo},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Kneip, Gasser (1992) Statistical Tools to Analyze Data Representing a Sample of Curves.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {1266-1305,1992,20,3,alois kneip and theo,author,data representing a sample,gasser,institute of mathematical statistics,no,of curves,pp,published by,s,sep,source,the annals of statistics,tistical tools to analyze,vol},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {3},
	pages = {1266--1305},
	title = {{Statistical Tools to Analyze Data Representing a Sample of Curves}},
	volume = {20},
	year = {1992}
}

@article{Gasser1995,
	author = {Gasser, Theo and Kneip, Alois},
	doi = {10.1080/01621459.1995.10476624},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Gasser, Kneip (1995) Searching for Structure in Curve Sample.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {432},
	pages = {1179--1188},
	title = {{Searching for Structure in Curve Samples}},
	volume = {90},
	year = {1995}
}
@article{Newey1997,
	abstract = {This paper gives general conditions for convergence rates and asymptotic normality of series estimators of conditional expectations, and specializes these conditions to polynomial regression and regression splines. Both mean-square and uniform convergence rates are derived. Asymptotic normality is shown for nonlinear functionals of series estimators, covering many cases not previously treated. Also, a simple condition for √n-consistency of a functional of a series estimator is given. The regularity conditions are straightforward to understand, and several examples are given to illustrate their application.},
	author = {Newey, Whitney K.},
	doi = {10.1016/S0304-4076(97)00011-0},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Asymptotic Theory/Newey (1997) Convergence rates and asymptotic normality for series estimators.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Asymptotic normality,Convergence rates,Nonparametric estimation,Series estimation},
	mendeley-groups = {Nonparametric Estimation/Asymptotic Theory},
	number = {1},
	pages = {147--168},
	title = {{Convergence Rates and Asymptotic Normality for Series Estimators}},
	volume = {79},
	year = {1997}
}


@article{Newey1994b,
	abstract = {Econometric applications of kernel estimators are proliferating, suggesting the need for convenient variance estimates and conditions for asymptotic normality. This paper develops a general “delta-method” variance estimator for functionals of kernel estimators. Also, regularity conditions for asymptotic normality are given, along with a guide to verify them for particular estimators. The general results are applied to partial means, which are averages of kernel estimators over some of their arguments with other arguments held fixed. Partial means have econometric applications, such as consumer surplus estimation, and are useful for estimation of additive nonparametric models. {\textcopyright} 1994, Cambridge University Press. All rights reserved.},
	author = {Newey, Whitney K.},
	doi = {10.1017/S0266466600008409},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Asymptotic Theory/Newey (1994) Kernel Estimation of Partial Means and a General Variance Estimator.pdf:pdf},
	issn = {14694360},
	journal = {Econometric Theory},
	mendeley-groups = {Nonparametric Estimation/Asymptotic Theory},
	number = {2},
	pages = {233--253},
	title = {{Kernel Estimation of Partial Means and a General Variance Estimator}},
	volume = {10},
	year = {1994}
}

@article{Matzkin1992,
	author = {Matzkin, Rosa L.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/General/Matzkin (1992) Nonparametric and Distribution-Free Estimation of the Binary Threshold Crossing and.pdf:pdf},
	journal = {Econometrica},
	keywords = {binary,binary threshold crossing model,choice model,concavity,consis-,distribution-free,homogeneity of degree one,identification,monotonicity,nonparametric},
	mendeley-groups = {Nonparametric Estimation/Discrete Choice},
	number = {2},
	pages = {239--270},
	title = {{Nonparametric and Distribution-Free Estimation of the Binary Threshold Crossing and The Binary Choice Models}},
	volume = {60},
	year = {1992}
}

@article{McElroy1987,
	abstract = {Many empirical studies of production specify a deterministic model of the firm, derive the implied behavioral equations (input demand or share system), and then "embed this system in a stochastic frame- work" by tacking on linear error terms. In contrast, this paper pro- poses general error models (GEMs) in which the error specification is an integral part of the optimization model. These models are the statistical embodiment of Stigler's view that apparent observed inefficiencies reflect the investigator's ignorance of the true optimi- zation problem. Additive GEMs are proposed and interpreted. Specification tests indicate that a translog additive GEM is superior to the standard translog specification.},
	author = {McElroy, Marjorie B.},
	doi = {10.1086/261483},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/McElroy (1987) Additive General Error Models for Production, Cost, and Derived Demand or Share.pdf:pdf},
	issn = {0022-3808},
	journal = {Journal of Political Economy},
	mendeley-groups = {Applied (By Econometrics)/Production Function Estimation},
	number = {4},
	pages = {737--757},
	title = {{Additive General Error Models for Production, Cost, and Derived Demand or Share Systems}},
	volume = {95},
	year = {1987}
}

@article{Altonji2005CrossSectionPanel,
  title = {Cross {{Section}} and {{Panel Data Estimators}} for {{Nonseparable Models}} with {{Endogenous Regressors}}},
  author = {Altonji, Joseph G. and Matzkin, Rosa L.},
  year = {2005},
  month = jul,
  journal = {Econometrica},
  volume = {73},
  number = {4},
  pages = {1053--1102},
  issn = {0012-9682, 1468-0262},
  doi = {10.1111/j.1468-0262.2005.00609.x},
  urldate = {2024-10-08},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Nonparametric Analysis\Altonji and Matzkin - 2005 - Cross Section and Panel Data Estimators for Nonseparable Models with Endogenous Regressors.pdf}
}

@article{Matzkin2003NonparametricEstimationNonadditive,
  title = {Nonparametric {{Estimation}} of {{Nonadditive Random Functions}}},
  author = {Matzkin, Rosa L.},
  year = {2003},
  journal = {Econometrica},
  volume = {71},
  number = {5},
  pages = {1339--1375},
  issn = {00129682},
  doi = {10.1111/1468-0262.00452},
  abstract = {We present estimators for nonparametric functions that are nonadditive in unobservable random terms. The distributions of the unobservable random terms are assumed to be unknown. We show that when a nonadditive, nonparametric function is strictly monotone in an unobservable random term, and it satisfies some other properties that may be implied by economic theory, such as homogeneity of degree one or separability, the function and the distribution of the unobservable random term are identified. We also present convenient normalizations, to use when the properties of the function, other than strict monotonicity in the unobservable random term, are unknown. The estimators for the nonparametric function and for the distribution of the unobservable random term are shown to be consistent and asymptotically normal. We extend the results to functions that depend on a multivariate random term. The results of a limited simulation study are presented.},
  keywords = {Conditional distributions,Conditional quantiles,Homogeneity,Monotonicity,Nonadditive random term,Nonparametric estimation,Nonseparable models,Shape restrictions},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametric Analysis\Models with Unobservables\Matzkin - 2003 - Nonparametric Estimation of Nonadditive Random Functions.pdf}
}


@article{Newey2003,
	abstract = {In econometrics there are many occasions where knowledge of the structural relationship among dependent variables is required to answer questions of interest. This paper gives identification and estimation results for nonparametric conditional moment restrictions. We characterize identification of structural functions as completeness of certain conditional distributions, and give sufficient identification conditions for exponential families and discrete variables. We also give a consistent, nonparametric estimator of the structural function. The estimator is nonparametric two-stage least squares based on series approximation, which overcomes an ill-posed inverse problem by placing bounds on integrals of higher-order derivatives.},
	author = {Newey, Whitney K. and Powell, James L.},
	doi = {10.1111/1468-0262.00459},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonseparable Models/Newey, Powell (2003) Instrumental Variable Estimation of Nonparametric Models.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Instrumental variables,Nonparametric estimation,Structural models},
	mendeley-groups = {Nonseparable Models,Nonparametric Estimation/Endogeneity},
	number = {5},
	pages = {1565--1578},
	title = {{Instrumental Variable Estimation of Nonparametric Models}},
	volume = {71},
	year = {2003}
}
@article{Darolles2011,
	abstract = {The focus of this paper is the nonparametric estimation of an instrumental regression function ϕ defined by conditional moment restrictions that stem from a structural econometric model E[Y−ϕ(Z) ΙW]=0, and involve endogenous variables Y and Z and instruments W. The function ϕ is the solution of an ill-posed inverse problem and we propose an estimation procedure based on Tikhonov regularization. The paper analyzes identification and overidentification of this model, and presents asymptotic properties of the estimated nonparametric instrumental regression function. [ABSTRACT FROM AUTHOR]},
	author = {Darolles, Serge and Fan, Yanqin and Florens, Jean Pierre and Renault, Eric},
	doi = {10.3982/ecta6539},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonseparable Models/Darolles et al. (2011) Nonparametric Instrumental Regression.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	keywords = {ill-posed problem,instrumental variables,integral equation,kernel smoothing,regularization,tikhono},
	mendeley-groups = {Nonseparable Models},
	number = {5},
	pages = {1541--1565},
	title = {{Nonparametric Instrumental Regression}},
	volume = {79},
	year = {2011}
}

@unpublished{Deaner2021,
	abstract = {We present non-parametric identification results for panel models in the presence of a vector of unobserved heterogeneity that is not additively separable in the structural function. We exploit the time-invariance and finite dimension of the heterogeneity to achieve identification of a number of objects of interest with the panel length fixed. Identification does not require that the researcher have access to an instrument that is uncorrelated with the unobserved heterogeneity. Instead the identification strategy relies on an assumption that some lags and leads of observables are independent conditional on the unobserved heterogeneity and some controls. The identification strategy motivates an estimation procedure based on penalized sieve minimum distance estimation in the non-parametric instrumental variables framework. We give conditions under which the estimator is consistent and derive its rate of convergence. We present Monte Carlo evidence of its efficacy in finite samples.},
	archivePrefix = {arXiv},
	arxivId = {1810.00283},
	author = {Deaner, Ben},
	eprint = {1810.00283},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Deaner (2021) Proxy Controls and Panel Data.pdf:pdf},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	pages = {1--84},
	title = {{Proxy Controls and Panel Data}},
	year = {2021}
}

@article{Kasy2011,
	abstract = {This note discusses identification in nonparametric, continuous triangular systems. It provides conditions that are both necessary and sufficient for the existence of control functions satisfying conditional independence and support requirements. Confirming a commonly noticed pattern, these conditions restrict the admissible dimensionality of unobserved heterogeneity in the first-stage structural relation, or more generally the dimensionality of the family of conditional distributions of second-stage heterogeneity given explanatory variables and instruments. These conditions imply that no such control function exists without assumptions that seem hard to justify in most applications. In particular, none exists in the context of a generic random coefficient model. {\textcopyright} Copyright Cambridge University Press 2011.},
	author = {Kasy, Maximilian},
	doi = {10.1017/S0266466610000460},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonseparable Models/Kasy (2010) Identification in Triangular Systems Using Control Functions.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {Nonseparable Models,Nonseparable Models/Control Function Approaches},
	number = {3},
	pages = {663--671},
	title = {{Identification in Triangular Systems Using Control Functions}},
	volume = {27},
	year = {2011}
}

@article{Bai2013,
	abstract = {We examine the relationship between Mi(xed) Da(ta) S(ampling) (MIDAS) regressions and the Kalman filter when forecasting with mixed frequency data. In general, state space models involve a system of equations, whereas MIDAS regressions involve a single equation. As a consequence, MIDAS regressions might be less efficient, but could also be less prone to parameter estimation error and/or specification errors. We examine how MIDAS regressions and Kalman filters match up under ideal circumstances, that is in population, and in cases where all the stochastic processes-low and high frequency-are correctly specified. We characterize cases where the MIDAS regression exactly replicates the steady state Kalman filter weights. We compare MIDAS and Kalman filter forecasts in population where the state space model is misspecified. We also compare MIDAS and Kalman filter forecasts in small samples. The paper concludes with an empirical application. Overall we find that the MIDAS and Kalman filter methods give similar forecasts. In most cases, the Kalman filter is a bit more accurate, but it is also computationally much more demanding. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
	author = {Bai, Jennie and Ghysels, Eric and Wright, Jonathan H.},
	doi = {10.1080/07474938.2012.690675},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Different Frequencies and MIDAS/Bai, Ghysels, Wright (2013) State Space Models and MIDAS Regressions.pdf:pdf},
	issn = {07474938},
	journal = {Econometric Reviews},
	keywords = {Kalman filter,Mixed frequency data},
	mendeley-groups = {Time Series/Different Frequencies and MIDAS},
	number = {7},
	pages = {779--813},
	title = {{State Space Models and MIDAS Regressions}},
	volume = {32},
	year = {2013}
}

@article{Schumacher2016,
	abstract = {This paper compares two single-equation approaches from the recent nowcasting literature: mixed-data sampling (MIDAS) regressions and bridge equations. Both approaches are suitable for nowcasting low-frequency variables such as the quarterly GDP using higher-frequency business cycle indicators. Three differences between the approaches are identified: (1) MIDAS is a direct multi-step nowcasting tool, whereas bridge equations provide iterated forecasts; (2) the weighting of high-frequency predictor observations in MIDAS is based on functional lag polynomials, whereas the bridge equation weights are fixed partly by time aggregation; (3) for parameter estimation, the MIDAS equations consider current-quarter leads of high-frequency indicators, whereas bridge equations typically do not. To assist in discussing the differences between the approaches in isolation, intermediate specifications between MIDAS and bridge equations are provided. The alternative models are compared in an empirical application to nowcasting GDP growth in the Euro area, given a large set of business cycle indicators.},
	author = {Schumacher, Christian},
	doi = {10.1016/j.ijforecast.2015.07.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Different Frequencies and MIDAS/Schumacher (2016) A comparison of MIDAS and bridge equations.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Bridge equations,Mixed-data sampling,Nowcasting},
	mendeley-groups = {Forecasting/Nowcasting,Time Series/Different Frequencies and MIDAS},
	number = {2},
	pages = {257--270},
	publisher = {Elsevier B.V.},
	title = {{A Comparison of MIDAS and Bridge Equations}},
	volume = {32},
	year = {2016}
}

@article{Poncela2021,
	abstract = {Dynamic factor models have been the main “big data” tool used by empirical macroeconomists during the last 30 years. In this context, Kalman filter and smoothing (KFS) procedures can cope with missing data, mixed frequency data, time-varying parameters, non-linearities, non-stationarity, and many other characteristics often observed in real systems of economic variables. The main contribution of this paper is to provide a comprehensive updated summary of the literature on latent common factors extracted using KFS procedures in the context of dynamic factor models, pointing out their potential limitations. Signal extraction and parameter estimation issues are separately analyzed. Identification issues are also tackled in both stationary and non-stationary models. Finally, empirical applications are surveyed in both cases. This survey is relevant to researchers and practitioners interested not only in the theory of KFS procedures for factor extraction in dynamic factor models but also in their empirical application in macroeconomics and finance.},
	author = {Poncela, Pilar and Ruiz, Esther and Miranda, Karen},
	doi = {10.1016/j.ijforecast.2021.01.027},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Factor Models/Poncela (2021) Factor extraction using Kalman filter and smoothing This is not just another survey.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Dynamic factor model,Expectation maximization algorithm,Identification,Macroeconomic forecasting,State-space model},
	mendeley-groups = {Factor Models,Forecasting/Factors},
	number = {4},
	pages = {1399--1425},
	publisher = {Elsevier B.V.},
	title = {{Factor Extraction Using Kalman Filter and Smoothing: This Is Not Just Another Survey}},
	volume = {37},
	year = {2021}
}

@article{Bok2018,
	abstract = {Data, data, data.... Economists know their importance well, especially when it comes to monitoring macroeconomic conditions - the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before so-called big data became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.},
	author = {Bok, Brandyn and Caratelli, Daniele and Giannone, Domenico and Sbordone, Argia M. and Tambalotti, Andrea},
	doi = {10.1146/annurev-economics-080217-053214},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Bok et al. (2018) Macroeconomic Nowcasting and Forecasting with Big Data..pdf:pdf},
	issn = {19411391},
	journal = {Annual Review of Economics},
	keywords = {business cycle analysis,high-dimensional data,monitoring economic conditions,real-time data flow},
	mendeley-groups = {Forecasting/Nowcasting},
	pages = {615--643},
	title = {{Macroeconomic Nowcasting and Forecasting with Big Data}},
	volume = {10},
	year = {2018}
}

@incollection{Banbura2013,
	author = {Ba{\'{n}}bura, Marta and Giannone, Domenico and Modugno, Michele and Reichlin, Lucrezia},
	booktitle = {Handbook of Economic Forecasting, Vol. 2 Part A},
	chapter = {4},
	doi = {10.1016/B978-0-444-53683-9.00004-9 ©},
	editor = {Elliott, Graham and Timmermann, Allan},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Ba{\'{n}}bura et al. (2013) Now-Casting and the Real-Time Data Flow.pdf:pdf},
	mendeley-groups = {Forecasting/Nowcasting},
	pages = {195--237},
	title = {{Now-Casting and the Real-Time Data Flow}},
	year = {2013}
}

@article{Marcellino2010,
	abstract = {In this article, we merge two strands from the recent econometric literature. First, factor models based on large sets of macroeconomic variables for forecasting, which have generally proven useful for forecasting. However, there is some disagreement in the literature as to the appropriate method. Second, forecast methods based on mixed-frequency data sampling (MIDAS). This regression technique can take into account unbalanced datasets that emerge from publication lags of high- and low-frequency indicators, a problem practitioner have to cope with in real time. In this article, we introduce Factor MIDAS, an approach for nowcasting and forecasting low-frequency variables like gross domestic product (GDP) exploiting information in a large set of higher-frequency indicators. We consider three alternative MIDAS approaches (basic, smoothed and unrestricted) that provide harmonized projection. {\textcopyright} Blackwell Publishing Ltd and the Department of Economics, University of Oxford, 2010.},
	author = {Marcellino, Massimiliano and Schumacher, Christian},
	doi = {10.1111/j.1468-0084.2010.00591.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Marcellino, Schumacher (2010) Factor MIDAS for Nowcasting and Forecasting with Ragged-Edge Data A Model Comparison for German GDP.pdf:pdf},
	issn = {03059049},
	journal = {Oxford Bulletin of Economics and Statistics},
	mendeley-groups = {Applied (By Econometrics)/Mixed Frequency,Applied (By Field)/Growth/Forecasting GDP},
	number = {4},
	pages = {518--550},
	title = {{Factor MIDAS for Nowcasting and Forecasting with Ragged-Edge Data: A Model Comparison for German GDP}},
	volume = {72},
	year = {2010}
}

@article{Clements2009,
	author = {Clements, Michael P. and Galv{\~{a}}o, Ana Beatriz},
	doi = {10.1002/jae.1075},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Clements, Galv{\~{a}}o (2009) Forecasting US output growth using leading indicators an appraisal using MIDAS models.pdf:pdf},
	journal = {Journal of Applied Econometrics},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP},
	number = {7},
	pages = {1187--1207},
	title = {{Forecasting US Output Growth Using Leading Indicators: An Appraisal Using MIDAS Models}},
	volume = {25},
	year = {2009}
}

@article{Clements2008,
	abstract = {Many macroeconomic series, such as U.S. real output growth, are sampled quarterly, although potentially useful predictors are often observed at a higher frequency. We look at whether a mixed data-frequency sampling (MIDAS) approach can improve forecasts of output growth. The MIDAS specification used in the comparison uses a novel way of including an autoregressive term. We find that the use of monthly data on the current quarter leads to significant improvement in forecasting current and next quarter output growth, and that MIDAS is an effective way to exploit monthly data compared with alternative methods. {\textcopyright} 2008 American Statistical Association.},
	author = {Clements, Michael P. and Galv{\~{a}}o, Ana Beatriz},
	doi = {10.1198/073500108000000015},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Clements, Galv{\~{a}}o. (2008) Macroeconomic Forecasting With Mixed-Frequency Data.pdf:pdf},
	issn = {07350015},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Forecasting,Mixed-frequency data,U.S. Output growth},
	mendeley-groups = {Applied (By Econometrics)/Mixed Frequency,Applied (By Field)/Growth/Forecasting GDP},
	number = {4},
	pages = {546--554},
	title = {{Macroeconomic Forecasting with Mixed-Frequency Data: Forecasting Output Growth in the United States}},
	volume = {26},
	year = {2008}
}

@unpublished{Ghysels2002,
	abstract = {We introduce Mixed Data Sampling (henceforth MIDAS) regression models. The regressions involve time series data sampled at different frequencies. Technically speaking MIDAS models specify conditional expectations as a distributed lag of regressors recorded at some higher sampling frequencies. We examine the asymptotic properties of MIDAS regression estimation and compare it with traditional distributed lag models. MIDAS regressions have wide applicability in macroeconomics and finance},
	author = {Ghysels, Eric and Santa-Clara, Pedro and Valkanov, Rossen},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Different Frequencies and MIDAS/Ghysels, Santa-Clara, Valkanov (2002) The MIDAS Touch.pdf:pdf},
	mendeley-groups = {Time Series/Different Frequencies and MIDAS},
	title = {{The MIDAS Touch: Mixed Data Sampling Regression Models}},
	year = {2002}
}

@article{Ghysels2006,
	abstract = {We consider various mixed data sampling (MIDAS) regressions to predict volatility. The regressions differ in the specification of regressors (squared returns, absolute returns, realized volatility, realized power, and return ranges), in the use of daily or intra-daily (5-min) data, and in the length of the past history included in the forecasts. The MIDAS framework allows us to compare regressions across all these dimensions in a very tightly parameterized fashion. Using equity return data, we find that daily realized power (involving 5-min absolute returns) is the best predictor of future volatility (measured by increments in quadratic variation) and outperforms models based on realized volatility (i.e. past increments in quadratic variation). Surprisingly, the direct use of high-frequency (5 min) data does not improve volatility predictions. Finally, daily lags of 1-2 months are sufficient to capture the persistence in volatility. These findings hold both in- and out-of-sample. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
	author = {Ghysels, Eric and Santa-Clara, Pedro and Valkanov, Rossen},
	doi = {10.1016/j.jeconom.2005.01.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Different Frequencies and MIDAS/Ghysels, Santa-Clara, Valkanov (2005) Predicting volatility getting the most out of MF data.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {High-frequency data,MIDAS,Model selection,Volatility forecasting},
	mendeley-groups = {Time Series/Different Frequencies and MIDAS},
	number = {1-2},
	pages = {59--95},
	title = {{Predicting Volatility: Getting the Most Out of Return Data Sampled at Different Frequencies}},
	volume = {131},
	year = {2006}
}


@article{Foroni2015,
	author = {Foroni, Claudia and Marcellino, Massimiliano and Schumacher, Christian},
	doi = {10.1111/rssa.12043},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Different Frequencies and MIDAS/Foroni, Marcellino, Schumacher (2013) Unrestricted mixed data sampling (MIDAS) MIDAS regressions with unrestricted lag polynomials.pdf:pdf},
	journal = {Journal of the Royal Statistical Society: Series A},
	mendeley-groups = {Time Series/Different Frequencies and MIDAS},
	number = {1},
	pages = {57--82},
	title = {{Unrestricted Mixed Data Sampling (MIDAS): MIDAS Regressions with Unrestricted Lag Polynomials}},
	volume = {178},
	year = {2015}
}

@article{Aprigliano2017,
	abstract = {In this paper, we study alternative methods to construct a daily indicator of growth for the euro area. We aim for an indicator that (i) provides reliable predictions, (ii) can be easily updated at the daily frequency, (iii) gives interpretable signals, and (iv) it is linear. Using a large panel of daily and monthly data for the euro area we explore the performance of two classes of models: bridge and U-MIDAS models, and different forecast combination strategies. Forecasts obtained from U-MIDAS models, combined with the inverse MSE weights, best satisfy the required criteria.},
	author = {Aprigliano, Valentina and Foroni, Claudia and Marcellino, Massimiliano and Mazzi, Gianluigi and Venditti, Fabrizio},
	doi = {10.1504/IJCEE.2017.080636},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Aprigliano et al. (2017)  A daily indicator of economic growth for the euro area.pdf:pdf},
	issn = {17571189},
	journal = {International Journal of Computational Economics and Econometrics},
	keywords = {Mixed-frequency data,Nowcasting},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP},
	number = {1-2},
	pages = {43--63},
	title = {{A Daily Indicator of Economic Growth for the Euro Area}},
	volume = {7},
	year = {2017}
}

@article{Giannone2008,
	abstract = {A formal method is developed for evaluating the marginal impact that intra-monthly data releases have on current-quarter forecasts (nowcasts) of real gross domestic product (GDP) growth. The method can track the real-time flow of the type of information monitored by central banks because it can handle large data sets with staggered data-release dates. Each time new data are released, the nowcasts are updated on the basis of progressively larger data sets that, reflecting the unsynchronized data-release dates, have a "jagged edge" across the most recent months. {\textcopyright} 2008 Elsevier B.V.},
	author = {Giannone, Domenico and Reichlin, Lucrezia and Small, David},
	doi = {10.1016/j.jmoneco.2008.05.010},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Giannone, Reichlin, Small  (2008. Nowcasting the real-time informational content of macroeconomic data.pdf:pdf},
	issn = {03043932},
	journal = {Journal of Monetary Economics},
	keywords = {Factor model,Forecasting,Monetary policy,Nowcast,Real-time data},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP,Forecasting,Forecasting/Factors},
	number = {4},
	pages = {665--676},
	title = {{Nowcasting: The Real-Time Informational Content of Macroeconomic Data}},
	volume = {55},
	year = {2008}
}

@article{Angelini2011,
	author = {Angelini, Elena and Camba-mendez, Gonzalo and Giannone, Domenico and Reichlin, Lucrezia and R{\"{u}}nstler, Gerhard},
	doi = {10.1111/j.1368-423X.2010.00328.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Angelini et al. (2011) Short-term forecasts of euro area GDP growth.pdf:pdf},
	journal = {The Econometrics Journal},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP Eurozone},
	number = {1},
	pages = {C25--C44},
	title = {{Short-Term Forecasts of Euro Area GDP Growth}},
	volume = {14},
	year = {2011}
}

@article{Stock2002,
	abstract = {This article studies forecasting a macroeconomic time series variable using a large number of predictors. The predictors are summarized using a small number of indexes constructed by principal component analysis. An approximate dynamic factor model serves as the statistical framework for the estimation of the indexes and construction of the forecasts. The method is used to construct 6-, 12-, and 24-month-ahead forecasts for eight monthly U.S. macroeconomic time series using 215 predictors in simulated real time from 1970 through 1998. During this sample period these new forecasts outperformed univariate autoregressions, small vector autoregressions, and leading indicator models.},
	author = {Stock, James H. and Watson, Mark W.},
	doi = {10.1198/073500102317351921},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Factor Models/Stock, Watson (2002)  Macroeconomic Forecasting Using Diffusion Indexes.pdf:pdf},
	issn = {07350015},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Factor model,Forecasting,Principal components},
	mendeley-groups = {Forecasting/Factors},
	number = {2},
	pages = {147--162},
	title = {{Macroeconomic Forecasting Using Diffusion Indexes}},
	volume = {20},
	year = {2002}
}

@article{Marcellino2003,
	abstract = {This paper compares several time series methods for short-run forecasting of Euro-wide inflation and real activity using data from 1982 to 1997. Forecasts are constructed from univariate autoregressions, vector autoregressions, single equation models that include Euro-wide and US aggregates, and large-model methods in which forecasts are based on estimates of common dynamic factors. Aggregate Euro-wide forecasts are constructed from models that utilize only aggregate Euro-wide variables and by aggregating country-specific models. The results suggest that forecasts constructed by aggregating the country-specific models are more accurate than forecasts constructed using the aggregate data. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
	author = {Marcellino, Massimiliano and Stock, James H. and Watson, Mark W.},
	doi = {10.1016/S0014-2921(02)00206-4},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Marcellino, Stock, Watson (2003) Macroeconomic forecasting in the Euro area Country specific versus area-wide information.pdf:pdf},
	issn = {00142921},
	journal = {European Economic Review},
	keywords = {Aggregation,Factor models,Forecasting},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP Eurozone},
	number = {1},
	pages = {1--18},
	title = {{Macroeconomic Forecasting in the Euro Area: Country Specific Versus Area-Wide Information}},
	volume = {47},
	year = {2003}
}

@article{Runstler2009,
	abstract = {This paper performs a large-scale forecast evaluation exercise to assess the performance of different models for the short-term forecasting of GDP, resorting to large datasets from ten European countries. Several versions of factor models are considered and cross-country evidence is provided. The forecasting exercise is performed in a simulated real-time context, which takes account of publication lags in the individual series. In general, we fi nd that factor models perform best and models that exploit monthly information outperform models that use purely quarterly data. However, the improvement over the simpler, quarterly models remains contained. {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	author = {R{\"{u}}nstler, Gerhard and Barhoum, Karim and Benk, Szilard and Cristadoro, Riccardo and Reijer, Ard Den and Jakaitiene, Audrone and Jelonek, Piotr and Rua, Antonio and Ruth, Karsten and van Nieuwenhuyze, Christophe},
	doi = {10.1002/for.1105},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Barhoumi et al. (2009) Short-term forecasting of GDP using large datasets A pseudo real-time forecast evaluation exercise.pdf:pdf},
	issn = {02776693},
	journal = {Journal of Forecasting},
	keywords = {Dynamic factor models,State space models,Uoc models},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP},
	number = {7},
	pages = {595--611},
	title = {{Short-Term Forecasting of GDP Using Large Datasets: A Pseudo Real-Time Forecast Evaluation Exercise}},
	volume = {28},
	year = {2009}
}

@article{Garcia-Ferrer1987,
	abstract = {Introduction It has long been recognized that national economies are economically interdependent (see, e.g., Burns and Mitchell 1946 for evidence of comovements of business activity in several countries and Zarnowitz 1985 for a summary of recent evidence). Recognition of such interdependence raises the question: Can such interdependence be exploited econometrically to produce improved forecasts of countries' macroeconomic variables such as rates of growth of output, and so forth? This is the problem that we address in this chapter, using annual and quarterly data for a sample of European Economic Community (EEC) countries and the United States. We recognize that there are several alternative approaches to the problem of obtaining improved international macroeconomic forecasts. First, there is the approach of Project Link that attempts to link together elaborate structural models of national economies in an effort to produce a world structural econometric model. A recent report on this ambitious effort was given by Klein (1985). We refer to this approach as a “top-down” approach, since it uses highly elaborate country models to approach the international forecasting problem. In our work, we report results based on a “bottom-up” approach that involves examining the properties of particular macroeconomic time series variables, building simple forecasting models for them, and appraising the quality of forecasts yielded by them. We regard this as a first step in the process of constructing more elaborate models in the structural econometric modeling time series analysis (SEMTSA) approach described by Palm (1983), Zellner (1979), and Zellner and Palm (1974).},
	author = {Garcia-Ferrer, Antonio and Highfield, Richard A. and Palm, Franz C. and Zellner, Arnold},
	doi = {10.2307/1391215},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Forecasting GDP/Garcia-Ferrer et al. (1987) Macroeconomic Forecasting Using Pooled International Data.pdf:pdf},
	journal = {Journal of Business and Economic Statistics},
	mendeley-groups = {Applied (By Field)/Growth/Forecasting GDP},
	number = {1},
	pages = {53--67},
	title = {{Macroeconomic Forecasting Using Pooled International Data}},
	volume = {5},
	year = {1987}
}

@article{Hoogstrate2000,
	abstract = {In this article, we analyze issues of pooling models for a given set of N individual units observed over T periods of time. When the parameters of the models are different but exhibit some similarity, pooling may lead to a reduction of the mean squared error of the estimates and forecasts. We investigate theoretically and through simulations the conditions that lead to improved performance of forecasts based on pooled estimates. We show that the superiority of pooled forecasts in small samples can deteriorate as the sample size grows. Empirical results for postwar international real gross domestic product growth rates of 18 Organization for Economic Cooperation and Development countries using a model put forward by Garcia-Ferrer, Highfield, Palm, and Zellner and Hong, among others illustrate these findings. When allowing for contemporaneous residual correlation across countries, pooling restrictions and criteria have to be rejected when formally tested, but generalized least squares (GLS)-based pooled forecasts are found to outperform GLS-based individual and ordinary least squares-based pooled and individual forecasts. {\textcopyright} 2000 Taylor {\&} Francis Group, LLC.},
	author = {Hoogstrate, Andr{\'{e}} J. and Palm, Franz C. and Pfann, Gerard A.},
	doi = {10.1080/07350015.2000.10524870},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Hoogstrate, Palm, Pfann (2000) Pooling in Dynamic Panel-Data Models An Application to Forecasting GDP Growth Rates.pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Contemporaneous disturbance correlation,Linear restrictions,Systems of regression equations},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Pooling and Partial Pooling,Applied (By Field)/Growth/Forecasting GDP},
	number = {3},
	pages = {274--283},
	title = {{Pooling in Dynamic Panel-Data Models: An Application to Forecasting GDP Growth Rates}},
	volume = {18},
	year = {2000}
}

@article{Kim2014,
	abstract = {In this paper, we empirically assess the predictive accuracy of a large group of models that are specified using principle components and other shrinkage techniques, including Bayesian model averaging and various bagging, boosting, least angle regression and related methods. Our results suggest that model averaging does not dominate other well designed prediction model specification methods, and that using "hybrid" combination factor/shrinkage methods often yields superior predictions. More specifically, when using recursive estimation windows, which dominate other "windowing" approaches, "hybrid" models are mean square forecast error "best" around 1/3 of the time, when used to predict 11 key macroeconomic indicators at various forecast horizons. Baseline linear (factor) models also "win" around 1/3 of the time, as do model averaging methods. Interestingly, these broad findings change noticeably when considering different sub-samples. For example, when used to predict only recessionary periods, "hybrid" models "win" in 7 of 11 cases, when condensing findings across all "windowing" approaches, estimation methods, and models, while model averaging does not "win" in a single case. However, in expansions, and during the 1990s, model averaging wins almost 1/2 of the time. Overall, combination factor/shrinkage methods "win" approximately 1/2 of the time in 4 of 6 different sample periods. Ancillary findings based on our forecasting experiments underscore the advantages of using recursive estimation strategies, and provide new evidence of the usefulness of yield and yield-spread variables in nonlinear prediction model specification. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
	author = {Kim, Hyun Hak and Swanson, Norman R.},
	doi = {10.1016/j.jeconom.2013.08.033},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Forecasting with factors/Swanson, Kim (2014) Forecasting Financial and Macroeconomic Variables Using Data.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Bagging,Bayesian model averaging,Boosting,Diffusion index,Elastic net,Forecasting,Least angle regression,Non-negative garotte,Prediction,Reality check,Ridge regression},
	mendeley-groups = {Forecasting/Factors},
	number = {PART 2},
	pages = {352--367},
	title = {{Forecasting Financial and Macroeconomic Variables Using Data Reduction Methods: New Empirical Evidence}},
	volume = {178},
	year = {2014}
}

@article{Blundell2014,
	abstract = {The control function approach (Heckman and Robb (1985)) in a system of linear simultaneous equations provides a convenient procedure to estimate one of the functions in the system using reduced form residuals from the other functions as additional regressors. The conditions on the structural system under which this procedure can be used in nonlinear and nonparametric simultaneous equations has thus far been unknown. In this paper, we define a new property of functions called control function separability and show it provides a complete characterization of the structural systems of simultaneous equations in which the control function procedure is valid. {\textcopyright} 2014 Richard Blundell and Rosa L. Matzkin.},
	author = {Blundell, Richard and Matzkin, Rosa L.},
	doi = {10.3982/qe281},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonseparable Models/Blundell, Matzkin (2014) Control functions in nonseparable simultaneous equations models.pdf:pdf},
	issn = {1759-7331},
	journal = {Quantitative Economics},
	keywords = {C3,Nonseparable models,control functions,simultaneous equations},
	mendeley-groups = {Nonseparable Models},
	number = {2},
	pages = {271--295},
	title = {{Control Functions in Nonseparable Simultaneous Equations Models}},
	volume = {5},
	year = {2014}
}

@article{Chesher2017,
	abstract = {This paper concerns the assessment of direct causal effects from a combination of: (i) non-experimental data, and (ii) qualitative domain knowledge. Domain knowledge is encoded in the form of a directed acyclic graph (DAG), in which all interactions are assumed linear, and some variables are presumed to be unobserved. We provide a generalization of the well-known method of Instrumental Variables, which allows its application to models with few conditional independeces.},
	author = {Chesher, Andrew and Rosen, Adam M.},
	doi = {10.3982/ecta12223},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Partial Identification/Chesher, Rosen (2017). Generalized Instrumental Variable Models.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Partial Identification},
	number = {3},
	pages = {959--989},
	title = {{Generalized Instrumental Variable Models}},
	volume = {85},
	year = {2017}
}
@book{Chesher2020,
	abstract = {This chapter sets out the extension of the scope of the classical IV model to cases in which unobserved variables are set-valued functions of observed variables. The resulting Generalized IV (GIV) models can be used when outcomes are discrete while unobserved variables are continuous, when there are rich specifications of heterogeneity as in random coefficient models, and when there are inequality restrictions constraining observed outcomes and unobserved variables. There are many other applications and classical IV models arise as a special case. The chapter provides characterizations of the identified sets delivered by GIV models. It gives details of the application of GIV analysis to models with an interval censored endogenous variable and to binary outcome models – for example probit models – with endogenous explanatory variables. It illustrates how the identified sets delivered by GIV models can be represented by moment inequality characterizations that have been the focus of recently developed methods for inference. An empirical application to a binary outcome model of female labor force participation is worked through in detail.},
	author = {Chesher, Andrew and Rosen, Adam M.},
	booktitle = {Handbook of Econometrics},
	doi = {10.1016/bs.hoe.2019.11.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Partial Identification/Chesher, Rosen (2020) Generalized IV Handbook Chapter.pdf:pdf},
	isbn = {9780444636492},
	issn = {15734412},
	keywords = {Instrumental variables,discrete outcomes,endogeneity,excess heterogeneity,incomplete models,interval censoring,moment inequalities,partial identification,random sets,structural econometrics},
	mendeley-groups = {Panel Data/Partial Identification},
	pages = {1--110},
	publisher = {Elsevier B.V.},
	title = {{Generalized Instrumental Variable Models, Methods, and Applications}},
	volume = {7},
	year = {2020}
}
 
@article{Wu2018,
	abstract = {In this paper, we investigate a hypothesis testing problem in regular semiparametric models using the Hellinger distance approach. Specifically, given a sample from a semiparametric family of $\nu$-densities of the form {\{} f$\theta$,$\eta$: $\theta$∈ $\Theta$ , $\eta$∈ $\Gamma$ {\}} , we consider the problem of testing a null hypothesis H0: $\theta$∈ $\Theta$ 0 against an alternative hypothesis H1: $\theta$∈ $\Theta$ 1, where $\eta$ is a nuisance parameter (possibly of infinite dimensional), $\nu$ is a $\sigma$-finite measure, $\Theta$ is a bounded open subset of Rp, and $\Gamma$ is a subset of some Banach or Hilbert space. We employ the Hellinger distance to construct a test statistic. The proposed method results in an explicit form of the test statistic. We show that the proposed test is asymptotically optimal (i.e., locally uniformly most powerful) and has some desirable robustness properties, such as resistance to deviations from the postulated model and in the presence of outliers.},
	author = {Wu, Jingjing and Karunamuni, Rohana J.},
	doi = {10.1007/s10463-017-0608-y},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Testing/Wu, Karunamuni (2017) Efficient and robust tests for semiparametric models.pdf:pdf},
	issn = {15729052},
	journal = {Annals of the Institute of Statistical Mathematics},
	keywords = {Adaptivity,Asymptotic optimality,Hellinger distance,Robustness,Semiparametric models,Tests of hypotheses},
	mendeley-groups = {Semiparametric Estimation/Inference},
	number = {4},
	pages = {761--788},
	publisher = {Springer Japan},
	title = {{Efficient and Robust Tests for Semiparametric Models}},
	volume = {70},
	year = {2018}
}

@article{Rabinowitz2000,
	abstract = {An approach to organizing the calculation of the efficient score for a finite dimensional parameter of interest in the presence of an infinite dimensional nuisance parameter is presented. The approach involves choosing a set of submodels for the nuisance parameter. The condition that the scores for the submodels be orthogonal to the efficient score takes the form of an equation whose solution appears in a representation of the efficient score. The approach is illustrated with several examples.},
	author = {Rabinowitz, Daniel},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Rabinowitz (2000) Computing the Efficient Score in Semiparametric Problems.pdf:pdf},
	issn = {10170405},
	journal = {Statistica Sinica},
	keywords = {Case-control,Censoring,Interval censored data,Missing covariates,Projection,Tangent space},
	mendeley-groups = {Semiparametric Estimation/Inference},
	number = {1},
	pages = {265--280},
	title = {{Computing the Efficient Score in Semi-Parametric Problems}},
	volume = {10},
	year = {2000}
}

@article{Andrews2010,
	abstract = {This paper analyzes the properties of subsampling, hybrid subsampling, and size-correction methods in two non-regular models. The latter two procedures are introduced in Andrews and Guggenberger (2009a). The models are non-regular in the sense that the test statistics of interest exhibit a discontinuity in their limit distribution as a function of a parameter in the model. The first model is a linear instrumental variables (IV) model with possibly weak IVs estimated using two-stage least squares (2SLS). In this case, the discontinuity occurs when the concentration parameter is zero. The second model is a linear regression model in which the parameter of interest may be near a boundary. In this case, the discontinuity occurs when the parameter is on the boundary. The paper shows that in the IV model one-sided and equal-tailed two-sided subsampling tests and confidence intervals (CIs) based on the 2SLS t statistic do not have correct asymptotic size. This holds for both fully- and partially-studentized t statistics. But, subsampling procedures based on the partially-studentized t statistic can be size-corrected. On the other hand, symmetric two-sided subsampling tests and CIs are shown to have (essentially) correct asymptotic size when based on a partially-studentized t statistic. Furthermore, all types of hybrid subsampling tests and CIs are shown to have correct asymptotic size in this model. The above results are consistent with "impossibility" results of Dufour (1997) because subsampling and hybrid subsampling CIs are shown to have infinite length with positive probability. Subsampling CIs for a parameter that may be near a lower boundary are shown to have incorrect asymptotic size for upper one-sided and equal-tailed and symmetric two-sided CIs. Again, size-correction is possible. In this model as well, all types of hybrid subsampling CIs are found to have correct asymptotic size. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
	author = {Andrews, Donald W.K. and Guggenberger, Patrik},
	doi = {10.1016/j.jeconom.2010.01.002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Nonregular/Andrews, Guggenberger (2009) Applications of Subsampling, Hybrid, Size-Correction.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Asymptotic size,Finite-sample size,Hybrid test,Instrumental variable,Over-rejection,Parameter near boundary,Size correction,Subsampling confidence interval,Subsampling test,Weak instrument},
	mendeley-groups = {Resampling/Nonregular},
	number = {2},
	pages = {285--305},
	publisher = {Elsevier B.V.},
	title = {{Applications of Subsampling, Hybrid, and Size-Correction Methods}},
	volume = {158},
	year = {2010}
}

@article{Robinson2018,
	abstract = {Semiparametric panel data modelling and statistical inference with fractional stochastic trends, nonparametrically time-trending individual effects, and general cross-sectional correlation and heteroscedasticity in innovations are developed. The fractional stochastic trends allow for a wide range of nonstationarity, indexed by a memory parameter, nesting the familiar I(1) case and allowing for parametric short-memory. The individual effects can nonparametrically vary simultaneously across time and across units. The cross-sectional covariance matrix is also nonparametric. The main focus is on estimation of the time series parameters. Two methods are considered, both of which entail an only approximate differencing out of the individual effects, leaving an error which has to be taken account of in our theory. In both cases we obtain standard asymptotics, with a central limit theorem, over a wide range of possible parameter values, unlike the nonstandard asymptotics for autoregressive parameter estimates at a unit root. For statistical inference, consistent estimation of the limiting covariance matrix of the parameter estimates requires consistent estimation of a functional of the cross-sectional covariance matrix. We examine efficiency loss due to cross-sectional correlation in a spatial model example. A Monte Carlo study of finite-sample performance is included.},
	author = {Robinson, Peter M. and Velasco, Carlos},
	doi = {10.1016/j.jeconom.2018.06.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Integrated Panels/Robinson, Velasco (2018) Inference on Trending Panel Data.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Asymptotic normality,Consistency,Nonparametric cross-sectional correlation and hete,Nonparametrically time-trending individual effects,Parametric fractional dependence,Semiparametric panel data modelling,Spatial model},
	mendeley-groups = {Panel Data/Dynamic Panels/Integrated},
	number = {2},
	pages = {282--304},
	publisher = {Elsevier B.V.},
	title = {{Inference on Trending Panel Data}},
	volume = {206},
	year = {2018}
}

@article{Parke1999,
	author = {Parke, William R},
	doi = {10.1162/003465399558490},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Fracional Integration/Parke (1999) What is Fractional Integration.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	mendeley-groups = {Time Series/Fractional Integration},
	number = {4},
	pages = {632--638},
	title = {{What is Fractional Integration?}},
	volume = {81},
	year = {1999}
}

@article{Robinson2015,
	abstract = {A dynamic panel data model is considered that contains possibly stochastic individual components and a common stochastic time trend that allows for stationary and nonstationary long memory and general parametric short memory. We propose four different ways of coping with the individual effects so as to estimate the parameters. Like models with autoregressive dynamics, ours nests I(1) behaviour, but unlike the nonstandard asymptotics in the autoregressive case, estimates of the fractional parameter can be asymptotically normal. For three of the estimates, establishing this property is made difficult due to bias caused by the individual effects, or by the consequences of eliminating them, which appears in the central limit theorem except under stringent conditions on the growth of the cross-sectional size N relative to the time series length T, though in case of two estimates these can be relaxed by bias correction, where the biases depend only on the parameters describing autocorrelation. For the fourth estimate, there is no bias problem, and no restrictions on N. Implications for hypothesis testing and interval estimation are discussed, with central limit theorems for feasibly bias-corrected estimates included. A Monte Carlo study of finite-sample performance is included.},
	author = {Robinson, Peter M. and Velasco, Carlos},
	doi = {10.1016/j.jeconom.2014.12.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Integrated Panels/Robinson, Velasco (2015) Efficient inference on fractionally integrated panel data models with fixed effects.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Bias correction,Estimation,Fractional time series,Panel data,Testing},
	mendeley-groups = {Panel Data/Dynamic Panels/Integrated},
	number = {2},
	pages = {435--452},
	publisher = {Elsevier B.V.},
	title = {{Efficient inference on fractionally integrated panel data models with fixed effects}},
	volume = {185},
	year = {2015}
}

@article{Ergemen2019,
	abstract = {We consider large N,T panel data models with fixed effects, a common factor allowing for cross-section dependence, and persistent data and shocks, which are assumed fractionally integrated. In a basic setup, the main interest is on the fractional parameter of the idiosyncratic component, which is estimated in first differences after factor removal by projection on the cross-section average. The pooled conditional-sum-of-squares estimate is √NT consistent but the normal asymptotic distribution might not be centred, requiring the time series dimension to grow faster than the cross-section size for correction. We develop tests of homogeneity of dynamics, including the degree of integration, that have no trivial power under local departures from the null hypothesis of a non-negligible fraction of cross-section units. A simulation study shows that our estimates and tests have good performance even in moderately small panels.},
	author = {Ergemen, Yunus Emre and Velasco, Carlos},
	doi = {10.1111/jtsa.12436},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Ergemen, Velasco (2019) Persistence Heterogeneity Testing in Panels with Interactive Fixed Effects.pdf:pdf},
	issn = {14679892},
	journal = {Journal of Time Series Analysis},
	keywords = {Fractional integration,factor models,homogeneity test,long memory,panel data},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {4},
	pages = {573--589},
	title = {{Persistence Heterogeneity Testing in Panels with Interactive Fixed Effects}},
	volume = {40},
	year = {2019}
}

@article{Choi1996,
	abstract = {Tests of hypotheses about finite-dimensional parameters in a semiparametric model are studied from Pitman's moving alternative (or local) approach using Le Cam's local asymptotic normality concept. For the case of a real parameter being tested, asymptotically uniformly most powerful (AUMP) tests are characterized for one-sided hypotheses, and AUMP unbiased tests for two-sided ones. An asymptotic invariance principle is introduced for multidimensional hypotheses, and AUMP invariant tests are characterized. These provide optimality for Wald, Rao (score), Neyman-Rao (effective score) and likelihood ratio tests in parametric models, and for Neyman-Rao tests in semiparametric models when constructions are feasible. Inversions lead to asymptotically uniformly most accurate confidence sets. Examples include one-, two-and k-sample problems, a linear regression model with unknown error distribution and a proportional hazards regression model with arbitrary baseline hazards. Results are presented in a format that facilitates application in strictly parametric models.},
	author = {Choi, Sungsub and Hall, W. J. and Schick, Anton},
	doi = {10.1214/aos/1032894469},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Testing/Choi, Hall, Schick( 1996) Asymptotically uniformly most powerful tests in parametric and semiparametric models.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Adaptation,Asymptotic confidence sets,Effective scores,Efficient tests,Invariance,Local alternatives,Unbiased tests},
	mendeley-groups = {Semiparametric Estimation/Inference},
	number = {2},
	pages = {841--861},
	title = {{Asymptotically Uniformly Most Powerful Tests in Parametric and Semiparametric Models}},
	volume = {24},
	year = {1996}
}

@unpublished{Lee2021,
	author = {Lee, Adam},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Score Tests/Lee (2021) Robust and Efficient Inference for Non-Regular Semiparametric Models.pdf:pdf},
	keywords = {adam,barcelona gse,boundary,edu,einarsson,email,for his guidance,from discussions with bjarni,g,geert mesters,hypothesis testing,i also have benefitted,i thank i thank,lee,local asymptotics,lukas,my advisor,regularisation,semiparametric models,simultaneous equations,tification,uniformity,universitat pompeu fabra and,upf,weak iden-},
	mendeley-groups = {Semiparametric Estimation/Inference,Testing/Score/LM Tests},
	pages = {1--114},
	title = {{Robust and Efficient Inference for Non-Regular Semiparametric Models}},
	year = {2021}
}

@article{Breusch1980,
	abstract = {No abstract is available for this item.},
	author = {Breusch, T. S. and Pagan, A. R.},
	doi = {10.2307/2297111},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Score Tests/Breusch, Pagan (1980) The Lagrange Multiplier Test and its Applications to Model Specification in Econometrics.pdf:pdf},
	issn = {00346527},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Testing/LM Tests},
	number = {1},
	pages = {239},
	title = {{The Lagrange Multiplier Test and its Applications to Model Specification in Econometrics}},
	volume = {47},
	year = {1980}
}

@article{Bester2009,
	abstract = {In this article, we consider identification and estimation of average marginal effects in a correlated random effects model without imposing functional form assumptions on the structural likelihood or the mixing distribution. Identification is achieved through imposing that the mixing distribution depends on observed covariates only through an index function.We leave the functional form of the index function unrestricted subject to smoothness conditions.We present identification results for this model and consider estimation of the marginal effects of interest. We illustrate the approach through a brief empirical example, which considers the relationship between insider trading activity and trading volume. {\textcopyright} 2009 American Statistical Association.},
	author = {Bester, C Alan and Hansen, Christian},
	doi = {10.1198/jbes.2009.0017},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Bester, Hansen (2009) Identification of Marginal Effects in a Nonparametric Correlated Random Effects Model.pdf:pdf},
	issn = {07350015},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Index,Insider trading,Sufficient statistic},
	mendeley-groups = {Panel Data/Nonlinear Panels,Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {235--250},
	title = {{Identification of Marginal Effects in a Nonparametric Correlated Random Effects Model}},
	volume = {27},
	year = {2009}
}

@unpublished{Cooprider2022PanelDataEstimator,
  title = {A {{Panel Data Estimator}} for the {{Distribution}} and {{Quantiles}} of {{Marginal Effects}} in {{Nonlinear Structural Models}} with an {{Application}} to the {{Demand}} for {{Junk Food}}},
  author = {Cooprider, Joseph and Hoderlein, Stefan and Meister, Alexander},
  year = {2022},
  doi = {10.2139/ssrn.3545485},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametric Analysis\Nonparametric Panel\Cooprider et al. - 2022 - A Panel Data Estimator for the Distribution and Quantiles of Marginal Effects in Nonlinear Structura.pdf}
}

@article{Hoderlein2007IdentificationMarginalEffects,
  title = {Identification of {{Marginal Effects}} in {{Nonseparable Models}} without {{Monotonicity}}},
  author = {Hoderlein, Stefan and Mammen, Enno},
  year = {2007},
  journal = {Econometrica},
  volume = {75},
  number = {5},
  pages = {1513--1518},
  doi = {10.1111/j.1468-0262.2007.00801.x},
  keywords = {gression,nonparametric,nonparametric instrumental variable,nonseparable model,partial identification,quantile re-,weak axiom},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametric Analysis\Models with Unobservables\Hoderlein and Mammen - 2007 - Identification of Marginal Effects in Nonseparable Models without Monotonicity.pdf}
}

@article{Hoderlein2009IdentificationEstimationLocal,
  title = {Identification and {{Estimation}} of {{Local Average Derivatives}} in {{Non-Separable Models Without Monotonicity}}},
  author = {Hoderlein, Stefan and Mammen, Enno},
  year = {2009},
  journal = {The Econometrics Journal},
  volume = {12},
  number = {1},
  pages = {1--25},
  doi = {10.1111/j.1368-423X.2008.00273.x},
  file = {D:\Academic Things\Econometrics\Articles\Nonseparable Models\Hoderlein, Mammen (2007) Identification and estimation of local average derivatives in non-separable models without monotonicity.pdf}
}

@article{Hoderlein2010AnalyzingRandomCoefficient,
  title = {Analyzing the {{Random Coefficient Model Nonparametrically}}},
  author = {Hoderlein, Stefan and Klemel{\"a}, Jussi and Mammen, Enno},
  year = {2010},
  journal = {Econometric Theory},
  volume = {26},
  number = {03},
  pages = {804--837},
  doi = {10.1017/S0266466609990119},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Nonparametric Analysis\Distribution Estimation\Deconvolution\Hoderlein et al. - 2010 - Analyzing the Random Coefficient Model Nonparametrically.pdf}
}

@article{Hoderlein2012NonparametricIdentificationNonseparable,
  title = {Nonparametric {{Identification}} in {{Nonseparable Panel Data Models With Generalized Fixed Effects}}},
  author = {Hoderlein, Stefan and White, Halbert},
  year = {2012},
  month = jun,
  journal = {Journal of Econometrics},
  volume = {168},
  number = {2},
  pages = {300--314},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2012.01.033},
  urldate = {2024-10-10},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Nonparametric Analysis\Hoderlein and White - 2012 - Nonparametric Identification in Nonseparable Panel Data Models With Generalized Fixed Effects.pdf}
}


@article{Ahn2018,
	abstract = {This article considers estimation of the unknown linear index coefficients of a model in which a number of nonparametrically identified reduced form parameters are assumed to be smooth and invertible function of one or more linear indices. The results extend the previous literature by allowing the number of reduced form parameters to exceed the number of indices (i.e., the indices are “overdetermined” by the reduced form parameters. The estimator of the unknown index coefficients (up to scale) is the eigenvector of a matrix (defined in terms of a first-step nonparametric estimator of the reduced form parameters) corresponding to its smallest (in magnitude) eigenvalue. Under suitable conditions, the proposed estimator is shown to be root-n-consistent and asymptotically normal, and under additional restrictions an efficient choice of a “weight matrix” is derived in the overdetermined case.},
	author = {Ahn, Hyungtaik and Ichimura, Hidehiko and Powell, James L. and Ruud, Paul A.},
	doi = {10.1080/07350015.2017.1379405},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Ahn, H et al. (2017) Simple Estimators for Invertible Index Models.pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Invertible models,Multinomial response,Semiparametric estimation,Single index models},
	mendeley-groups = {Semiparametric Estimation/Index Models Partially Linear},
	number = {1},
	pages = {1--10},
	title = {{Simple Estimators for Invertible Index Models}},
	volume = {36},
	year = {2018}
}

@article{Muris2017,
	abstract = {This paper introduces a new estimator for the fixed-effects ordered logit model. The proposed method has two advantages over existing estimators. First, it estimates the differences in the cut points along with the regression coefficient, leading to provide bounds on partial effects. Second, the proposed estimator for the regression coefficient is more efficient. I use the fact that the ordered logit model with J outcomes and T observations can be converted to a binary choice logit model in (J - 1)T ways. As an empirical illustration, I examine the income-health gradient for children using the Medical Expenditure Panel Survey.},
	author = {Muris, Chris},
	doi = {10.1162/REST_a_00617},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Panel/Muris (2017) Estimation in the Fixed-Effects Ordered Logit Model.pdf:pdf},
	issn = {15309142},
	journal = {Review of Economics and Statistics},
	mendeley-groups = {Discrete Choice/Ordered,Panel Data/Nonlinear Panels/Discrete Choice},
	number = {3},
	pages = {465--477},
	title = {{Estimation in the Fixed-Effects Ordered Logit Model}},
	volume = {99},
	year = {2017}
}

@misc{Davezies2022IdentificationEstimationAverage,
	title = {Identification and {{Estimation}} of {{Average Marginal Effects}} in {{Fixed Effects Logit Models}}},
	author = {Davezies, Laurent and D'Haultfoeuille, Xavier and Laage, Louise},
	year = {2022},
	month = oct,
	number = {arXiv:2105.00879},
	eprint = {2105.00879},
	primaryclass = {econ, stat},
	publisher = {arXiv},
	urldate = {2024-10-08},
	abstract = {This article considers average marginal effects (AME) in a panel data fixed effects logit model. Relating the identified set of the AME to an extremal moment problem, we first show how to obtain sharp bounds on the AME straightforwardly, without any optimization. Then, we consider two strategies to build confidence intervals on the AME. In the first, we estimate the sharp bounds with a semiparametric two-step estimator. The second, very simple strategy estimates instead a quantity known to be at a bounded distance from the AME. It does not require any nonparametric estimation but may result in larger confidence intervals. Monte Carlo simulations suggest that both approaches work well in practice, the second being often very competitive. Finally, we show that our results also apply to average treatment effects, the average structural functions and ordered, fixed effects logit models.},
	archiveprefix = {arXiv},
	langid = {english},
	keywords = {Economics - Econometrics,Statistics - Methodology},
	file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Discrete Choice\Heterogeneity\Davezies et al. - 2022 - Identification and Estimation of Average Marginal Effects in Fixed Effects Logit Models.pdf}
}

@unpublished{Liu2021,
	abstract = {Average partial effects (APEs) are generally not point-identified in binary response panel models with unrestricted unobserved heterogeneity. We show their point-identification under an index sufficiency assumption on the unobserved heterogeneity, even when the error distribution is unspecified. This assumption does not impose parametric restrictions on the unobserved heterogeneity. We then construct a three-step semiparametric estimator for the APE. In the first step, we estimate the common parameters using either a conditional logit or smoothed maximum score estimator. In the second step, we estimate the conditional expectation of the outcomes given the indices and a generated regressor that depends on first-step estimates. In the third step, we average derivatives of this conditional expectation to obtain a partial mean that estimates the APE. We show that this proposed three-step APE estimator is consistent and asymptotically normal. We evaluate its finite-sample properties in Monte Carlo simulations. We then illustrate our estimator in a study of determinants of married women's labor supply.},
	archivePrefix = {arXiv},
	arxivId = {2105.12891},
	author = {Liu, Laura and Poirier, Alexandre and Shiu, Ji-Liang},
	eprint = {2105.12891},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Liu et al. (2021) Identification and Estimation of Average Partial Effects in Semiparametric Binary Response Panel Models.pdf:pdf},
	keywords = {average partial effects,binary response models,c13,c14,c23,c25,department of economics,edu,indiana university,iu,jel classification,lauraliu,liu,panel data,semiparametric estimation,unobserved heterogeneity},
	mendeley-groups = {Panel Data/Nonlinear Panels/Discrete Choice},
	pages = {1--70},
	title = {{Identification and Estimation of Average Partial Effects in Semiparametric Binary Response Panel Models}},
	year = {2021}
}

@article{Baetschmann2015,
	abstract = {Summary: The paper considers panel data methods for estimating ordered logit models with individual-specific correlated unobserved heterogeneity. We show that a popular approach is inconsistent, whereas some consistent and efficient estimators are available, including minimum distance and generalized method-of-moment estimators. A Monte Carlo study reveals the good properties of an alternative estimator that has not been considered in econometric applications before, is simple to implement and almost as efficient. An illustrative application based on data from the German Socio-Economic Panel confirms the large negative effect of unemployment on life satisfaction that has been found in the previous literature.},
	author = {Baetschmann, Gregori and Staub, Kevin E. and Winkelmann, Rainer},
	doi = {10.1111/rssa.12090},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Panel/Baetschmann, Staub, Winkelmann (2014) Consistent estimation of the fixed effects ordered logit model.pdf:pdf},
	issn = {1467985X},
	journal = {Journal of the Royal Statistical Society: Series A},
	keywords = {Fixed effects,Logistic regression,Ordered response data,Panel data,Plant closures,Subjective wellbeing,Unemployment},
	mendeley-groups = {Panel Data/Nonlinear Panels/Discrete Choice},
	number = {3},
	pages = {685--703},
	title = {{Consistent Estimation of the Fixed Effects Ordered Logit Model}},
	volume = {178},
	year = {2015}
}

@article{Galichon2009,
	abstract = {We propose an easily implementable test of the validity of a set of theoretical restrictions on the relationship between economic variables, which do not necessarily identify the data generating process. The restrictions can be derived from any model of interactions, allowing censoring and multiple equilibria. When the restrictions are parameterized, the test can be inverted to yield confidence regions for partially identified parameters, thereby complementing other proposals, primarily Chernozhukov et al. [Chernozhukov, V., Hong, H., Tamer, E., 2007. Estimation and confidence regions for parameter sets in econometric models. Econometrica 75, 1243-1285]. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
	author = {Galichon, Alfred and Henry, Marc},
	doi = {10.1016/j.jeconom.2009.01.010},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Partial Identification/Galichon, Henry (2009) A test of non-identifying restrictions and confidence regions for partially identified parameters.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Mass transportation,Partial identification,Specification test},
	mendeley-groups = {Partial Identification and Random Sets},
	number = {2},
	pages = {186--196},
	publisher = {Elsevier B.V.},
	title = {{A Test of Non-Identifying Restrictions and Confidence Regions for Partially Identified Parameters}},
	volume = {152},
	year = {2009}
}

@article{Chesher2013,
	abstract = {This paper studies identification in multiple discrete choice models in which there may be endogenous explanatory variables, that is, explanatory variables that are not restricted to be distributed independently of the unobserved determinants of latent utilities. The model does not employ large support, special regressor, or control function restrictions; indeed, it is silent about the process that delivers values of endogenous explanatory variables, and in this respect it is incomplete. Instead, the model employs instrumental variable restrictions that require the existence of instrumental variables that are excluded from latent utilities and distributed independently of the unobserved components of utilities. We show that the model delivers set identification of latent utility functions and the distribution of unobserved heterogeneity, and we characterize sharp bounds on these objects. We develop easy-to-compute outer regions that, in parametric models, require little more calculation than what is involved in a conventional maximum likelihood analysis. The results are illustrated using a model that is essentially the conditional logit model of 41, but with potentially endogenous explanatory variables and instrumental variable restrictions. The method employed has wide applicability and for the first time brings instrumental variable methods to bear on structural models in which there are multiple unobservables in a structural equation.},
	author = {Chesher, Andrew and Rosen, Adam M. and Smolinski, Konrad},
	doi = {10.3982/qe240},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Multiple/Chesher, Rosen, Smolinski (2013) An instrumental variable model of multiple discrete choice.pdf:pdf},
	issn = {1759-7331},
	journal = {Quantitative Economics},
	keywords = {C25,C26,Partial identification,endogeneity,incomplete models,instrumental variables,multiple discrete choice,random sets},
	mendeley-groups = {Discrete Choice/Multinomial,Discrete Choice/Endogeneity,Nonseparable Models},
	number = {2},
	pages = {157--196},
	title = {{An Instrumental Variable Model of Multiple Discrete Choice}},
	volume = {4},
	year = {2013}
}

@article{Chesher2010,
	abstract = {Single equation instrumental variable models for discrete outcomes are shown to be set identifying, not point identifying, for the structural functions that deliver the values of the discrete outcome. Bounds on identified sets are derived for a general nonparametric model and sharp set identification is demonstrated in the binary outcome case. Point identification is typically not achieved by imposing parametric restrictions. The extent of an identified set varies with the strength and support of instruments, and typically shrinks as the support of a discrete outcome grows. The paper extends the analysis of structural quantile functions with endogenous arguments to cases in which there are discrete outcomes.},
	author = {Chesher, Andrew},
	doi = {10.3982/ecta7315},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Endogeneity/Chesher (2010) Instrumental Variable Models for Discrete Outcomes.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	keywords = {Partial identification, nonparametric methods, nonadditive models, discrete distributions, ordered choice, endogeneity, instrumental variables, structural quantile functions, incomplete models},
	mendeley-groups = {Discrete Choice/Endogeneity},
	number = {2},
	pages = {575--601},
	title = {{Instrumental Variable Models for Discrete Outcomes}},
	volume = {78},
	year = {2010}
}

@article{Chesher2012,
	abstract = {This paper studies single equation instrumental variable models of ordered choice in which explanatory variables may be endogenous. The models are weakly restrictive, leaving unspecified the mechanism that generates endogenous variables. These incomplete models are set, not point, identifying for parametrically (e.g. ordered probit) or nonparametrically specified structural functions. The paper gives results on the properties of the identified set for the case in which potentially endogenous explanatory variables are discrete. The results are used as the basis for calculations showing the rate of shrinkage of identified sets as the number of classes in which the outcome is categorised increases. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
	author = {Chesher, Andrew and Smolinski, Konrad},
	doi = {10.1016/j.jeconom.2011.06.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Ordered/Chesher, Smolinski (2012) IV models of ordered choice.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Endogeneity,Incomplete models,Instrumental variables,Ordered choice,Ordered probit,Set identification,Threshold crossing models},
	mendeley-groups = {Discrete Choice/Ordered},
	number = {1},
	pages = {33--48},
	publisher = {Elsevier B.V.},
	title = {{IV Models of Ordered Choice}}, 
	volume = {166},
	year = {2012}
}

@article{Lewbel2000,
	abstract = {This paper provides estimators of discrete choice models, including binary, ordered, and multinomial response (choice) models. The estimators closely resemble ordinary and two-stage least squares. The distribution of the model's latent variable error is unknown and may be related to the regressors, e.g., the model could have errors that are heteroscedastic or correlated with regressors. The estimator does not require numerical searches, even for multinomial choice. For ordered and binary choice models the estimator is root N consistent and asymptotically normal. A consistent estimator of the conditional error distribution is also provided. {\textcopyright} 2000 Elsevier Science S.A. All rights reserved.},
	author = {Lewbel, Arthur},
	doi = {10.1016/S0304-4076(00)00015-4},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Ordered/Lewbel (2000) Semiparametric qualitative response model estimation with unknown heteroscedasticity or instrumental variables..pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Binary choice,Binomial response,Discrete choice,Heteroscedasticity,Instrumental variables,Latent variable models,Measurement error,Multinomial response,Ordered choice,Qualitative response,Semiparametric},
	mendeley-groups = {Discrete Choice/Ordered},
	number = {1},
	pages = {145--177},
	title = {{Semiparametric Qualitative Response Model Estimation with Unknown Heteroscedasticity or Instrumental Variables}},
	volume = {97},
	year = {2000}
}

@book{Greene2010,
	abstract = {It is increasingly common for analysts to seek out the opinions of individuals and organizations using attitudinal scales such as degree of satisfaction or importance attached to an issue. Examples include levels of obesity, seriousness of a health condition, attitudes towards service levels, opinions on products, voting intentions, and the degree of clarity of contracts. Ordered choice models provide a relevant methodology for capturing the sources of influence that explain the choice made amongst a set of ordered alternatives. The methods have evolved to a level of sophistication that can allow for heterogeneity in the threshold parameters, in the explanatory variables (through random parameters), and in the decomposition of the residual variance. This book brings together contributions in ordered choice modeling from a number of disciplines, synthesizing developments over the last fifty years, and suggests useful extensions to account for the wide range of sources of influence on choice.},
	address = {Cambridge},
	author = {Greene, William H and Hensher, David A},
	doi = {10.1017/CBO9780511845062},
	isbn = {9780521194204},
	mendeley-groups = {Discrete Choice},
	publisher = {Cambridge University Press},
	title = {{Modeling Ordered Choices: A Primer}}, 
	year = {2010}
}

@Book{Wooldridge2010book,
	author={Wooldridge, Jeffrey M.},
	title={{Econometric Analysis of Cross Section and Panel Data}},
	publisher={The MIT Press},
	year=2010,
	month={},
	number={0262232588},
	series={MIT Press Books},
	edition={},
	keywords={econometrics; cross section data; panel data},
	doi={},
	isbn={9780262296793},
	abstract={The second edition of this acclaimed graduate text provides a unified treatment of the analysis of two kinds of data structures used in contemporary econometric research: cross section data and panel data. The book covers both linear and nonlinear models, including models with dynamics and/or individual heterogeneity. In addition to general estimation frameworks (particularly methods of moments and maximum likelihood), specific linear and nonlinear methods are covered in detail, including probit and logit models, multinomial and ordered choice models, Tobit models and two-part extensions, models for count data, various censored and missing data schemes, causal (or treatment) effect estimation, and duration analysis. Control function and correlated random effects approaches are expanded to allow estimation of complicated models in the presence of endogeneity and heterogeneity. This second edition has been substantially updated and revised. Improvements include a broader class of models for missing data problems; more detailed treatment of cluster sampling problems, an important topic for empirical researchers; expanded discussion of \&quot;generalized instrumental variables\&quot; (GIV) estimation; new coverage of inverse probability weighting; a more complete framework for estimating treatment effects with assumptions concerning the intervention and different data structures, including panel data, and a firmly established link between econometric approaches to nonlinear panel data and the \&quot;generalized estimating equation\&quot; literature popular in statistics and other fields. New attention is given to explaining when particular econometric methods can be applied; the goal is not only to tell readers what does work, but why certain “obvious” procedures do not. The numerous included exercises, both theoretical and computer-based, allow the reader to extend methods covered in the text and discover new insights.},
	url={https://ideas.repec.org/b/mtp/titles/0262232588.html}
}
@article{Chen2019,
	abstract = {We revisit the panel data analysis of Acemoglu et al. (forthcoming) on the relationship between democracy and economic growth using state-of-the-art econometric methods. We argue that panel data settings are high-dimensional, resulting in estimators to be biased to a degree that invalidates statistical inference. We remove these biases by using simple analytical and sample-splitting methods, and thereby restore valid statistical inference. We find that debiased fixed effects and Arellano-Bond estimators produce higher estimates of the long-run effect of democracy on growth, providing even stronger support for the key hypothesis of Acemoglu et al.},
	archivePrefix = {arXiv},
	arxivId = {1901.03821},
	author = {Chen, Shuowen and Chernozhukov, Victor and Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
	doi = {10.1257/pandp.20191071},
	eprint = {1901.03821},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Biases and Stuff/Chen, Chernozhukov, Fern{\'{a}}ndez-Val (2019) Mastering Panel Metrics.pdf:pdf},
	issn = {2574-0768},
	journal = {AEA Papers and Proceedings},
	mendeley-groups = {Applied (By Econometrics)/General Panel,Applied (By Field)/Political},
	pages = {77--82},
	title = {{Mastering Panel Metrics: Causal Impact of Democracy on Growth}},
	volume = {109},
	year = {2019}
}

@article{Imbens2009IdentificationEstimationTriangular,
  title = {Identification and {{Estimation}} of {{Triangular Simultaneous Equations Models Without Additivity}}},
  author = {Imbens, Guido W. and Newey, Whitney K.},
  year = {2009},
  journal = {Econometrica},
  volume = {77},
  number = {5},
  pages = {1481--1512},
  issn = {0012-9682},
  doi = {10.3982/ecta7108},
  abstract = {This paper uses control variables to identify and estimate models with nonseparable, multidimensional disturbances. Triangular simultaneous equations models are considered, with instruments and disturbances that are independent and a reduced form that is strictly monotonic in a scalar disturbance. Here it is shown that the conditional cumulative distribution function of the endogenous variable given the instruments is a control variable. Also, for any control variable, identification results are given for quantile, average, and policy effects. Bounds are given when a common support assumption is not satisfied. Estimators of identified objects and bounds are provided, and a demand analysis empirical example is given. [ABSTRACT FROM AUTHOR]},
  file = {D:\Academic Things\Econometrics\Articles\Nonseparable Models\Imbens, Newey (2009) Identification and Estimation of Triangular Simultaneous Equations Models Without Additivity.pdf}
}


@article{Blundell2004,
	abstract = {This paper develops and implements semiparametric methods for estimating binary response (binary choice) models with continuous endogenous regressors. It extends existing results on semiparametric estimation in single-index binary response models to the case of endogenous regressors. It develops a control function approach to account for endogeneity in triangular and fully simultaneous binary response models. The proposed estimation method is applied to estimate the income effect in a labour market participation problem using a large micro data-set from the British Family Expenditure Survey. The semiparametric estimator is found to perform well, detecting a significant attenuation bias. The proposed estimator is contrasted to the corresponding probit and linear probability specifications. {\textcopyright} 2004 The Review of Economic Studies Limited.},
	author = {Blundell, Richard W. and Powell, James L.},
	doi = {10.1111/j.1467-937X.2004.00299.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Blundell, Powell (2004)  Endogeneity in Semiparametric Binary Response Models.pdf:pdf},
	issn = {00346527},
	journal = {Review of Economic Studies},
	mendeley-groups = {Semiparametric Estimation/Endogeneity},
	number = {3},
	pages = {655--679},
	title = {{Endogeneity in Semiparametric Binary Response Models}},
	volume = {71},
	year = {2004}
}

@incollection{Matzkin2007,
	abstract = {When one wants to estimate a model without specifying the functions and distributions parametrically, or when one wants to analyze the identification of a model independently of any particular parametric specification, it is useful to perform a nonparametric analysis of identification. This chapter presents some of the recent results on the identification of nonparametric econometric models. It considers identification in models that are additive in unobservable random terms and in models that are nonadditive in unobservable random terms. Single equation models as well as models with a system of equations are studied. Among the latter, special attention is given to structural models whose reduced forms are triangular in the unobservable random terms, and to simultaneous equations, where the reduced forms are functions of all the unobservable variables in the system. The chapter first presents some general identification results for single-equation models that are additive in unobservable random terms, single-equation models that are nonadditive in unobservable random terms, single-equation models that possess and index structure, simultaneous equations nonadditive in unobservable random terms, and discrete choice models. Then, particular ways of achieving identification are considered. These include making use of conditional independence restrictions, marginal independence restrictions, shape restrictions on functions, shape restrictions on distributions, and restrictions in both functions and distributions. The objective is to provide insight into some of the recent techniques that have been developed recently, rather than on presenting a complete survey of the literature.},
	author = {Matzkin, Rosa L.},
	booktitle = {Handbook of Econometrics, vol. 6B},
	doi = {https://doi.org/10.1016/S1573-4412(07)06073-4},
	editor = {Heckman, James J and Leamer, Edward E B T - Handbook of Econometrics},
	isbn = {1573-4412},
	keywords = {identification,nonadditive models,nonparametric models,nonseparable models,simultaneous equations},
	mendeley-groups = {Nonparametric Estimation/Identification},
	pages = {5307--5368},
	publisher = {Elsevier},
	title = {{Nonparametric identification}}, 
	volume = {6},
	year = {2007}
}

@article{Imbens1994b,
	abstract = {RANDOM ASSIGNMENT OF TREATMENT and concurrent data collection on treatment and control groups is the norm in medical evaluation research. In contrast, the use of random assignment to evaluate social programs remains controversial. Following criticism of parametric evaluation models (e.g., Lalonde (1986)), econometric research has been geared towards establishing conditions that guarantee nonparametric identification of treatment effects in observational studies, i.e. identification without relying on functional form restrictions or distributional assumptions. The focus has been on identification of average treatment effects in a population of interest, or on the average effect for the subpopulation that is treated. The conditions required to nonparametrically identify these parameters can be restrictive, however, and the derived identification results fragile. In particular, results in Chamberlain (1986), Manski (1990), Heckman (1990), and Angrist and Imbens (1991) require that there be some subpopulation for whom the probability of treatment is zero, at least in the limit. The purpose of this paper is to show that even when there is no subpopulation available for whom the probability of treatment is zero, we can still identify an average treatment effect of interest under mild restrictions satisfied in a wide range of models and circumstances. We call this a local average treatment effect (LATE). Examples of problems where the local average treatment effect is identified include latent index models and evaluations based on natural experiments such as those studied by Angrist (1990) and Angrist and Krueger (1991). LATE is the average treatment effect for individuals whose treatment status is influenced by changing an exogenous regressor that satisfies an exclusion restriction.},
	author = {Imbens, Guido W. and Angrist, Joshua D.},
	doi = {10.2307/2951620},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Imbens, Angrist  (1994) Identification and Estimation of Local Average Treatment Effects.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Treatment Effects},
	number = {2},
	pages = {467},
	title = {{Identification and Estimation of Local Average Treatment Effects}},
	volume = {62},
	year = {1994}
}

@article{Chamberlain1982,
	abstract = {The paper examines the relationship between heterogeneity bias and strict exogeneity in a distributed lag regression of y on x. The relationship is very strong when x is continuous, weaker when x is discrete, and non-existent as the order of the distributed lag becomes infinite. The individual specific random variables introduce nonlinearity and heteroskedasticity; so the paper provides an appropriate framework for the estimation of multivariate linear predictors. Restrictions are imposed using a minimum distance estimator. It is generally more efficient than the conventional estimators such as quasi-maximum likelihood. There are computationally simple generalizations of two- and three-stage least squares that achieve this efficiency gain. Some of these ideas are illustrated using the sample of Young Men in the National Longitudinal Survey. The paper reports regressions on the leads and lags of variables measuring union coverage, SMSA, and region. The results indicate that the leads and lags could have been generated just by a random intercept. This gives some support for analysis of covariance type estimates; these estimates indicate a substantial heterogeneity bias in the union, SMSA, and region coefficients. {\textcopyright} 1982.},
	author = {Chamberlain, Gary},
	doi = {10.1016/0304-4076(82)90094-X},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Other/Chamberlain (1982)  Multivariate regression models for panel data.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data,Panel Data/Other},
	number = {1},
	pages = {5--46},
	title = {{Multivariate Regression Models for Panel Data}},
	volume = {18},
	year = {1982}
}

@article{Andersen1970,
	author = {Andersen, Erling Bernhard},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Andersen (1970) Asymptotic Properties of Conditional Maximum-Likelihood Estimators.pdf:pdf},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	mendeley-groups = {Panel Data/Nonlinear Panels/Discrete Choice},
	number = {2},
	pages = {283--301},
	title = {{Asymptotic Properties of Conditional Maximum-Likelihood Estimators}},
	volume = {32},
	year = {1970}
}

@article{Neyman1948,
	author = {Neyman, Jerzy and Scott, Elizabeth L.},
	doi = {10.2307/1914288},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Other/Neyman, Scott (1948) Consistent Estimates Based on Partially Consistent Observations.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data},
	number = {1},
	pages = {1--32},
	title = {{Consistent Estimates Based on Partially Consistent Observations}},
	volume = {16},
	year = {1948}
}

@article{Chernozhukov2015NonparametricIdentificationPanels,
  title = {Nonparametric {{Identification}} in {{Panels Using Quantiles}}},
  author = {Chernozhukov, Victor and {Fern{\'a}ndez-Val}, Iv{\'a}n and Hoderlein, Stefan and Holzmann, Hajo and Newey, Whitney K.},
  year = {2015},
  month = oct,
  journal = {Journal of Econometrics},
  volume = {188},
  number = {2},
  pages = {378--392},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2015.03.006},
  urldate = {2024-10-04},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Nonparametric Analysis\Chernozhukov et al. - 2015 - Nonparametric Identification in Panels Using Quantiles.pdf}
}

@article{Chernozhukov2013,
	abstract = {Nonseparable panel models are important in a variety of economic settings, including discrete choice. This paper gives identification and estimation results for nonseparable models under time homogeneity conditions that are like "time is randomly assigned" or "time is an instrument." Partial identification results for average and quantile effects are given for discrete regressors, under static or dynamic conditions, in fully nonparametric and in semiparametric models, with time effects. It is shown that the usual, linear, fixed-effects estimator is not a consistent estimator of the identified average effect, and a consistent estimator is given. A simple estimator of identified quantile treatment effects is given, providing a solution to the important problem of estimating quantile treatment effects from panel data. Bounds for overall effects in static and dynamic models are given. The dynamic bounds provide a partial identification solution to the important problem of estimating the effect of state dependence in the presence of unobserved heterogeneity. The impact of {\$}T{\$}, the number of time periods, is shown by deriving shrinkage rates for the identified set as {\$}T{\$} grows. We also consider semiparametric, discrete-choice models and find that semiparametric panel bounds can be much tighter than nonparametric bounds. Computationally-convenient methods for semiparametric models are presented. We propose a novel inference method that applies in panel data and other settings and show that it produces uniformly valid confidence regions in large samples. We give empirical illustrations.},
	archivePrefix = {arXiv},
	arxivId = {0904.1990},
	author = {Chernozhukov, Victor and Fern{\'{a}}ndez-Val, Iv{\'{a}}n and Hahn, Jinyong and Newey, Whitney K.},
	doi = {10.3982/ecta8405},
	eprint = {0904.1990},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Chernozhukov et al. (2013) Average and Quantile Effects in Nonseparable Panel Models.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {2},
	pages = {535--580},
	title = {{Average and Quantile Effects in Nonseparable Panel Models}},
	volume = {81},
	year = {2013}
}
 

@article{Honore2006,
	abstract = {Identification of dynamic nonlinear panel data models is an important and delicate problem in econometrics. In this paper we provide insights that shed light on the identification of parameters of some commonly used models. Using these insights, we are able to show through simple calculations that point identification often fails in these models. On the other hand, these calculations also suggest that the model restricts the parameter to lie in a region that is very small in many cases, and the failure of point identification may, therefore, be of little practical importance in those cases. Although the emphasis is on identification, our techniques are constructive in that they can easily form the basis for consistent estimates of the identified sets.},
	author = {Honor{\'{e}}, Bo E. and Tamer, Elie},
	doi = {10.1111/j.1468-0262.2006.00676.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Honore, Tamer (2006) Bounds on Parameters in Panel Dynamic Discrete Choice Models.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Bounds,Identification,Initial condition problem,Nonlinear panel data models},
	mendeley-groups = {Panel Data/Dynamic Panels/Nonlinear},
	number = {3},
	pages = {611--629},
	title = {{Bounds on Parameters in Panel Dynamic Discrete Choice Models}},
	volume = {74},
	year = {2006}
}

@article{Stefanski1985,
	author = {Stefanski, Leonard A. and Carroll, Raymond J.},
	doi = {10.1214/aos/1176349741},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Errors in Variables/Stefanski, Carroll (1985) Covariate Measurement Error in Logistic Regression.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Errors in Variables/Discrete Choice},
	number = {4},
	pages = {1335--1351},
	title = {{Covariate Measurement Error in Logistic Regression}},
	volume = {13},
	year = {1985}
}

@article{Carroll1984,
	abstract = {We consider binary regression models when some of the predictors are measured with error. For normal measurement errors, structural maximum likelihood estimates are considered. We show that if the measurement error is large, the usual estimate of the probability of the event in question can be substantially in error, especially for high risk groups. In the situation of large measurement error, we investigate a conditional maximum likelihood estimator and its properties. {\textcopyright} 1984 Biometrika Trust.},
	author = {Carroll, Raymond J. and Spiegelman, Clifford H. and Lan, K. K. Gordon and Bailey, Kent T. and Abbott, Robert D.},
	doi = {10.1093/biomet/71.1.19},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Errors in Variables/Carroll, et al. (1984) On Errors-in-Variables for Binary Regression Models..pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Functional model,Logistic regression,Measurement error,Probit regression,Structural model},
	mendeley-groups = {Errors in Variables,Errors in Variables/Discrete Choice},
	number = {1},
	pages = {19--25},
	title = {{On Errors-in-Variables for Binary Regression Models}},
	volume = {71},
	year = {1984}
}

@article{Garen1984,
	author = {Garen, John},
	doi = {10.2307/1910996},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Sample Selection/Garen (1984) The Returns to Schooling A Selectivity Bias Approach with a Continuous Choice Variable.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Sample Selection},
	number = {5},
	pages = {1199--1218},
	title = {{The Returns to Schooling: A Selectivity Bias Approach with a Continuous Choice Variable}},
	volume = {52},
	year = {1984}
}

@article{Wooldridge2015,
	author = {Wooldridge, Jeffrey M.},
	doi = {10.3368/jhr.50.2.420},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Control Function Estimation/Wooldridge (2015) Control Function Methods in Applied Econometrics.pdf:pdf},
	journal = {Journal of Human Resources},
	mendeley-groups = {Control Function Estimation,Control Function Estimation/Theory},
	number = {2},
	pages = {420--445},
	title = {{Control Function Methods in Applied Econometrics}},
	volume = {50},
	year = {2015}
}

@article{Heckman1990,
	author = {Heckman, James J. and Honor{\'{e}}, Bo E.},
	doi = {10.2307/2938303},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Sample Selection/Heckman, Honore (1990) The Empirical Content of the Roy Model.pdf:pdf},
	journal = {Econometrica},
	keywords = {earnings inequality,identifiability,log concave random variables,log convexity,roy model,self selection and,total positivity},
	mendeley-groups = {Sample Selection},
	number = {5},
	pages = {1121--1149},
	title = {{The Empirical Content of the Roy Model}},
	volume = {58},
	year = {1990}
}

@incollection{Heckman2010,
	abstract = {Roy Adaptation Model Assumptions • The person is a bio-psycho-social being. The person is in constant interaction with a changing environment. • To cope with a changing world, person uses both innate and acquired mechanisms which are biological, psychological and social in origin. • To respond positively to environmental changes, the person must adapt. • The person has 4 modes of adaptation: physiologic needs, self-concept, role function and inter-dependence. Major Concepts • Adaptation --goal of nursing • Person --adaptive system • Environment --stimuli • Health --outcome of adaptation • Nursing-promoting adaptation and health Adaptation • Responding positively to environmental changes. • The process and outcome of individuals and groups who use conscious awareness, self reflection and choice to create human and environmental integration Person • Bio-psycho-social being in constant interaction with a changing environment • Uses innate and acquired mechanisms to adapt • Includes people as individuals or in groups-families, organizations, communities, and society as a whole. Environment • Focal -internal or external and immediately confronting the person • Contextual-all stimuli present in the situation that contribute to effect of focal stimulus • Residual-a factor whose effects in the current situation are unclear • All conditions, circumstances, and influences surrounding and affecting the development and behavior of persons and groups with particular consideration of mutuality of person and earth resources, including focal, contextual and residual stimuli Health • Represented by a health-illness continuum • A state and a process of being and becoming integrated and whole Nursing • To promote adaptation for individuals and groups in the four adaptive modes, thus contributing to health, quality of life, and dying with dignity by assessing behaviors and factors that influence adaptive abilities and by intervening to enhance environmental interactions},
	author = {Heckman, James J. and Taber, Christopher},
	booktitle = {Microeconometrics},
	doi = {10.1057/9780230280816_27},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Sample Selection/Heckman, Taber (2010) Roy model. Microeconometrics.pdf:pdf},
	mendeley-groups = {Sample Selection},
	pages = {221--228},
	title = {{Roy Model}},
	year = {2010}
}
@article{Roy1951,
	author = {Roy, Andrew D.},
	doi = {10.1093/oxfordjournals.oep.a041827},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Sample Selection/Roy  (1951)  Some Thoughts on Distribution of Earnings.pdf:pdf},
	issn = {00307653},
	journal = {Oxford Economic Papers},
	mendeley-groups = {Sample Selection},
	number = {2},
	pages = {135--146},
	title = {{Some Thoughts on the Distribution of Earnings}},
	volume = {3},
	year = {1951}
}
  
@article{Aakvik2010,
	abstract = {In this paper, we explore the impact of a mandatory education reform as well as pre-reform availability of schools above the mandatory level, on educational attainment and returns to education in Norway. We contribute to the existing literature by focusing on the heterogeneity of the impact of reforms. Our results indicate that increased compulsory education from seven to nine years increased the general level of education beyond the compulsory education. We also find that the effect of family background on educational attainment was weaker after the reform. The average treatment effect on returns to education is surprisingly high for education of intermediate duration. This means that increasing the general level of education potentially generates high returns in the form of wages. We also find that the effect of treatment on the treated on the returns to education is 1-4 percentage points higher than the average treatment effect. {\textcopyright} 2009 Elsevier B.V.},
	author = {Aakvik, Arild and Salvanes, Kjell G. and Vaage, Kjell},
	doi = {10.1016/j.euroecorev.2009.09.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Aakvik, Salvanes, Vaage (2010) Measuring heterogeneity in the returns to education using an education reform.pdf:pdf},
	issn = {00142921},
	journal = {European Economic Review},
	keywords = {C3,Education reform,Heterogeneity,I2,Returns to education,Roy model},
	mendeley-groups = {Applied (By Econometrics)/Heterogeneous Coefficients,Applied (By Field)/Education},
	number = {4},
	pages = {483--500},
	publisher = {Elsevier},
	title = {{Measuring Heterogeneity in the Returns to Education Using an Education Reform}}, 
	volume = {54},
	year = {2010}
}

@article{Auld2005,
	abstract = {A large literature documents a strong correlation between health and educational outcomes. In this paper we investigate the role of cognitive ability in the health-education nexus. Using NLSY data, we show that one standard deviation increase in cognitive ability is associated with roughly the same increase in health as two years of schooling and that cognitive ability accounts for roughly one quarter of the association between schooling and health. Both schooling and ability are strongly associated with health at low levels but less related or unrelated at high levels. Estimates treating schooling as endogenous to health suggest that much of the correlation between schooling and health is attributable to unobserved heterogeneity; the causal effect of schooling on health is large only for respondents with low levels of schooling and low cognitive ability. An implication is that policies which increase schooling will only increase health to the extent that they increase the education of poorly-educated individuals. Subsidies to college education, for example, are unlikely to increase population health. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	author = {Auld, M. Christopher and Sidhu, Nirmal},
	doi = {10.1002/hec.1050},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Christopher Auld, Sidhu (2005). Schooling, cognitive ability and health.pdf:pdf},
	issn = {10579230},
	journal = {Health Economics},
	keywords = {Correlated random coefficient models,Education,Health,Intelligence},
	mendeley-groups = {Applied (By Field)/Health Economics,Applied (By Econometrics)/Heterogeneous Coefficients},
	number = {10},
	pages = {1019--1034},
	pmid = {16167318},
	title = {{Schooling, cognitive ability and health}},
	volume = {14},
	year = {2005}
}

@article{Mundlak1978,
	author = {Mundlak, Yair},
	doi = {10.2307/1913646},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Other/Mundlak (1978) On the Pooling of Time Series and Cross Section Data.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data},
	number = {1},
	pages = {69--85},
	title = {{On the Pooling of Time Series and Cross Section Data}},
	volume = {46},
	year = {1978}
}

@article{Breitung2021,
	abstract = {We analyse estimation procedures for the panel data models with heterogeneous slopes. Specifically we take into account a possible dependence between regressors and heterogeneous slope coefficients, which is referred to as systematic variation. It is shown that under relevant forms of systematic slope variations (i) the pooled OLS estimator is severely biased, (ii) Swamy's GLS estimator is inconsistent if the number of time periods T is fixed, whereas (iii) the mean-group estimator always provides consistent estimators at the risk of high variances. Following Mundlak (1978) we propose an augmentated regression which results in a simple and robust version of the pooled estimator. The latter approach avoids the risk of large standard errors of the mean-group estimator, whenever T is small. We also propose two test statistics for systematic slope variation using the Lagrange multiplier and Hausman principles. We derive their asymptotic properties and provide a local power analysis of both test statistics. Monte Carlo experiments corroborate our theoretical findings and show that for all combinations of N and T the Mundlak-type GLS estimator outperform all other estimators.},
	author = {Breitung, J{\"{o}}rg and Salish, Nazarii},
	doi = {10.1016/j.jeconom.2020.04.007},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Breitung, Salish (2020) Estimation of heterogeneous panels with systematic slope variations.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Panel data,Random effects,Slope heterogeneity},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {399--415},
	publisher = {Elsevier B.V.},
	title = {{Estimation of Heterogeneous Panels with Systematic Slope Variations}}, 
	volume = {220},
	year = {2021}
}

@article{Chernozhukov2013b,
	abstract = {Counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. In this article we develop modeling and inference tools for counterfactual distributions based on regression methods. The counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. For either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. These results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. These confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. Our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. We illustrate the results with an empirical application to wage decompositions using data for the United States. As a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the $\backslash$textit{\{}entire{\}} conditional distribution. We show that distribution regression encompasses the Cox duration regression and represents a useful alternative to quantile regression. We establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.},
	archivePrefix = {arXiv},
	arxivId = {0904.0951},
	author = {Chernozhukov, Victor and Fern{\'{a}}ndez-Val, Iv{\'{a}}n and Melly, Blaise},
	doi = {10.3982/ecta10582},
	eprint = {0904.0951},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Distributional Analysis/Chernozhukov et al. (2013) Inference on Counterfactual Distributions.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Treatment Effects/Distributional Analysis},
	number = {6},
	pages = {2205--2268},
	title = {{Inference on Counterfactual Distributions}},
	volume = {81},
	year = {2013}
}

@article{Eeckhout2014,
	author = {Eeckhout, Jan and Pinheiro, Roberto and Schmidheiny, Kurt},
	doi = {10.1086/676141},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Eeckhout, Pinheiro, Schmidheiny (2014) Spatial Sorting.pdf:pdf},
	journal = {Journal of Political Economy},
	mendeley-groups = {Applied/Heterogeneous Coefficients},
	number = {3},
	title = {{Spatial Sorting}},
	volume = {122},
	year = {2014}
}

@article{Combes2012b,
	abstract = {Firms are more productive, on average, in larger cities. Two main explanations have been offered: firm selection (larger cities toughen competition, allowing only the most productive to survive) and agglomeration economies (larger cities promote interactions that increase productivity), possibly reinforced by localized natural advantage. To distinguish between them, we nest a generalized version of a tractable firm selection model and a standard model of agglomeration. Stronger selection in larger cities left-truncates the productivity distribution, whereas stronger agglomeration right-shifts and dilates the distribution. Using this prediction, French establishment-level data, and a new quantile approach, we show that firm selection cannot explain spatial productivity differences. This result holds across sectors, city size thresholds, establishment samples, and area definitions. {\textcopyright} 2012 The Econometric Society.},
	author = {Combes, Pierre Philippe and Duranton, Gilles and Gobillon, Laurent and Puga, Diego and Roux, S{\'{e}}bastien},
	doi = {10.3982/ecta8442},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Combes et al. (2012) The Productivity Advantages of Large Cities Distinguishing Agglomeration From Firm Selection.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Applied/Heterogeneous Coefficients},
	number = {6},
	pages = {2543--2594},
	title = {{The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection}},
	volume = {80},
	year = {2012}
}

@article{Combes2012,
	abstract = {This paper provides descriptive evidence about the distribution of wages and skills in denser and less dense employment areas in France. We confirm that on average, workers in denser areas are more skilled. There is also strong over-representation of workers with particularly high and low skills in denser areas. These features are consistent with patterns of migration including negative selection of migrants to less dense areas and positive selection towards denser areas. Nonetheless migration, even in the long-run, accounts for little of the skill differences between denser and less dense areas. Finally, we find marked differences across age groups and some suggestions that much of the skill differences across areas can be explained by differences between occupational groups rather than within. {\textcopyright} 2012.},
	author = {Combes, Pierre Philippe and Duranton, Gilles and Gobillon, Laurent and Roux, S{\'{e}}bastien},
	doi = {10.1016/j.regsciurbeco.2012.11.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Combes, Duranton, Gobillon, Roux (2012) Sorting and local wage and skill distributions in France..pdf:pdf},
	issn = {01660462},
	journal = {Regional Science and Urban Economics},
	keywords = {Skill distribution,Sorting,Wage distribution},
	mendeley-groups = {Applied/Heterogeneous Coefficients},
	number = {6},
	pages = {913--930},
	publisher = {Elsevier B.V.},
	title = {{Sorting And Local Wage and Skill Distributions in France}}, 
	volume = {42},
	year = {2012}
}

@article{Bun2021,
	archivePrefix = {arXiv},
	arxivId = {2105.08346},
	author = {Bun, Maurice J.G. and Kleibergen, Frank R.},
	doi = {10.1017/S026646662100027X},
	eprint = {2105.08346},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Bun, Kleibergen (2021) Identification robust inference for moments based analysis of.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Panel Data/Dynamic Panels/Robustness},
	pages = {1--66},
	title = {{Identification Robust Inference for Moments Based Analysis of Linear Dynamic Panel Data Models}},
	year = {2021}
}
@book{VanderVaart2023,
	author = {{Van der Vaart}, Aad and Wellner, Jon A.},
	doi = {10.1007/978-3-031-29040-4},
	edition = {2},
	isbn = {978-3-031-29038-1},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {XVII, 679},
	publisher = {Springer Nature Switzerland},
	title = {{Weak Convergence and Empirical Processes: With Applications to Statistics}},
	year = {2023}
}

@book{VanderVaart1998,
	abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
	address = {Cambridge},
	author = {{Van der Vaart}, Aad},
	booktitle = {Cambridge Series in Statistical and Probabilistic Mathematics},
	doi = {DOI: 10.1017/CBO9780511802256},
	isbn = {9780521784504},
	mendeley-groups = {Random Useful Math/Probability/Limit Theorems},
	publisher = {Cambridge University Press},
	title = {{Asymptotic Statistics}}, 
	year = {1998}
}


@article{Williamson1990,
	abstract = {Probabilistic arithmetic involves the calculation of the distribution of arithmetic functions of random variables. This work on probabilistic arithmetic began as an investigation into the possibility of adapting existing numerical procedures (developed for fixed numbers) to handle random variables (by replacing the basic operations of arithmetic by the appropriate convolutions). The general idea is similar to interval arithmetic and fuzzy arithmetic. In this paper we present a new and general numerical method for calculating the appropriate convolutions of a wide range of probability distributions. An important feature of the method is the manner in which the probability distributions are represented. We use lower and upper discrete approximations to the quantile function (the quasi-inverse of the distribution function). This results in any representation error being always contained within the lower and upper bounds. This method of representation has advantages over other methods previously proposed. The representation fits in well with the idea of dependency bounds. Stochastic dependencies that arise within the course of a sequence of operations on random variables are the severest limit to the simple application of convolution algorithms to the formation of a general probabilistic arithmetic. We examine this dependency error and show how dependency bounds are a possible means of reducing its effect. Dependency bounds are lower and upper bounds on the distribution of a function of random variables that contain the true distribution even when nothing is known of the dependence of the random variables. They are based on the Fr{\'{e}}chet inequalities for the joint distribution of a set of random variables in terms of their marginal distributions. We show how the dependency bounds can be calculated numerically using our numerical representation of probability distributions. Examples of the methods developed are presented, and relationships with other work on numerically handling uncertainties are briefly described. {\textcopyright} 1990.},
	author = {Williamson, Robert C. and Downs, Tom},
	doi = {10.1016/0888-613X(90)90022-T},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Williamson, Downs (1990) Probabilistic arithmetic.pdf:pdf},
	issn = {0888613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Fr{\'{e}}chet bound,convolution,copula,dependency bound,functions of random variables,fuzzy arithmetic,interval arithmetic,lower and upper probability distributions,spurious correlation},
	mendeley-groups = {Random Useful Math/Probability/Inequalities},
	number = {2},
	pages = {89--158},
	title = {{Probabilistic Arithmetic. I. Numerical Methods for Calculating Convolutions and Dependency Bounds}},
	volume = {4},
	year = {1990}
}

@article{Firpo2019,
	abstract = {In the treatment effect problem, the available information is on the marginal distributions of potential outcomes, but not on their joint distribution. The only point identified functional of the treatment effect distribution is its average, the average treatment effect (ATE). Quantiles and other functionals of the distribution of treatment effect are only partially identified. Bounds on a single quantile and on the cumulative distribution function (c.d.f.) in a single point are sharp (Makarov bounds). We show that bounds on functionals of the quantile process that use Makarov bounds are not sharp, because the Makarov bounds are pointwise, but not uniformly sharp. This allows us to propose improved bounds on functionals of the c.d.f. As an intermediate result, we find that the Makarov bounds on the region that contains the c.d.f. of the treatment effect distribution in a finite number of points can be improved. We provide numerical illustrations throughout the paper permitting a clear visualization of how the method works.},
	author = {Firpo, Sergio and Ridder, Geert},
	doi = {10.1016/j.jeconom.2019.04.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Firpo, Ridder (2019) Partial identification of the treatment effect distribution and its functionals..pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Bounds,Distribution of treatment effects},
	mendeley-groups = {Treatment Effects},
	number = {1},
	pages = {210--234},
	publisher = {Elsevier B.V.},
	title = {{Partial identification of the treatment effect distribution and its functionals}}, 
	volume = {213},
	year = {2019}
}

@article{delaRoca2017LearningWorkingBig,
  title = {Learning {{By Working}} in {{Big Cities}}},
  author = {{de la Roca}, Jorge and Puga, Diego},
  year = {2017},
  journal = {Review of Economic Studies},
  volume = {84},
  number = {1},
  pages = {106--142},
  issn = {1467937X},
  doi = {10.1093/restud/rdw031},
  abstract = {Individual earnings are higher in bigger cities. We consider three reasons: Spatial sorting of initially more productive workers, static advantages from workers' current location, and learning by working in bigger cities. Using rich administrative data for Spain, we find that workers in bigger cities do not have higher initial unobserved ability as reflected in fixed effects. Instead, they obtain an immediate static premium and accumulate more valuable experience. The additional value of experience in bigger cities persists after leaving and is stronger for those with higher initial ability. This explains both the higher mean and greater dispersion of earnings in bigger cities.},
  keywords = {Agglomeration economies,City sizes,Earnings premium,Learning},
  file = {D:\Academic Things\Econometrics\Articles\Applied\Heterogeneous Coefficients\Roca, Puga (2016) Learning by Working in Big Cities..pdf}
}


@article{Magnac2021,
	abstract = {Using data on French male wage workers observed over 30 years, we estimate by random and fixed effect methods a wage equation with pervasive heterogeneity. Individual wage profiles are derived from a human capital investment model and described by a level, a slope and a curvature. Among others, our empirical application delivers original results on issues like the Mincer dip, and the time-varying correlations between wage growth and levels, or between initial wages and growth. Static and long-run inequality indices can easily be compared and decomposed into their multidimensional components. (c) 2021 Elsevier B.V. All rights reserved. FU  - European Research Council under the European Community's Seventh Framework Program FP7/2007-2013 [295298]; ANRFrench National Research Agency (ANR) [ANR-17-EURE-0010, ANR-17-CE41-0008-01]; Institut Universitaire de France FX  - We thank Christian Belzil, Richard Blundell, Laurent Gobillon, Jim Heckman, Nicolas Pistolesi, Jean-Marc Robin, Bernard Salanie, the editor and referees for helpful comments as well as participants in numerous seminars where we presented earlier versions of this research. This research has received financial support from the European Research Council under the European Community's Seventh Framework Program FP7/2007-2013 grant agreement No 295298, funding from ANR under grant ANR-17-EURE-0010 (Investissements d'Avenir program) and grant ANR-17-CE41-0008-01 as well as funding from the Institut Universitaire de France. All errors remain ours. NR  - 66 PU  - ELSEVIER PI  - AMSTERDAM PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	author = {Magnac, Thierry and Roux, S{\'{e}}bastien},
	doi = {10.1016/j.euroecorev.2021.103715},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Magnac, Roux (2021) Heterogeneity and Wage Inequalities over the Life Cycle.pdf:pdf},
	issn = {00142921},
	journal = {European Economic Review},
	keywords = {c orresponding author,ects,human capital investment,inequality,magnac,post-schooling,random and,thierry,toulouse school of economics,tse-,universit{\'{e}} de toulouse capitole,wage dynamics,wage growth,xed e},
	mendeley-groups = {Applied/Heterogeneous Coefficients},
	number = {March},
	pages = {103715},
	title = {{Heterogeneity and wage inequalities over the life cycle}},
	volume = {134},
	year = {2021}
}

@article{Vivalt2015,
	author = {Vivalt, Eva},
	doi = {10.1257/aer.p20151015},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Vivalt (2015) Heterogeneous Treatment Effects in Impact Evaluation.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {Treatment Effects},
	number = {5},
	pages = {467--470},
	title = {{Heterogeneous Treatment Effects in Impact Evaluation}},
	volume = {105},
	year = {2015}
}

@article{Calonico2018,
	abstract = {Nonparametric methods play a central role in modern empirical work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the mean-squared error (MSE)-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest coverage error decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too “large.” Finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate Studentization is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available. Supplementary materials for this article are available online.},
	archivePrefix = {arXiv},
	arxivId = {1508.02973},
	author = {Calonico, Sebastian and Cattaneo, Matias D. and Farrell, Max H.},
	doi = {10.1080/01621459.2017.1285776},
	eprint = {1508.02973},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Inference/Calonico, Cattaneo, Farrell (2018)  On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric Inference.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Coverage error,Edgeworth expansion,Kernel methods,Local polynomial regression},
	mendeley-groups = {Nonparametric Estimation/Inference},
	number = {522},
	pages = {767--779},
	title = {{On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric Inference}},
	volume = {113},
	year = {2018}
}

@article{Barras2021,
	author = {Barras, Laurent and Gagliardini, Patrick and Scaillet, Olivier},
	doi = {10.2139/ssrn.3269995},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Heterogeneous Coefficients/Barras et al (2020)  Skill, Scale, and Value Creation in the Mutual Fund Industry.pdf:pdf},
	mendeley-groups = {Applied/Heterogeneous Coefficients},
	title = {{Skill, Scale and Value Creation in the Mutual Fund Industry}},
	year = {2021}
}

@article{Imbens1994,
	abstract = {Census reports can be interpreted as providing nearly exact knowledge of moments of the marginal distribution of economic variables. This information can be combined with cross-sectional or panel samples to improve accuracy of estimation. In this paper we show how to do this efficiently. We show that the gains from use of marginal information can be substantial. We also discuss how to test the compatibility of sample and marginal information. {\textcopyright} 1994 The Review of Economic Studies Limited.},
	author = {Imbens, Guido W. and Lancaster, Tony},
	doi = {10.2307/2297913},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Imbens, Lancaster (1994) Combining Micro and Macro Data for Microeconometric Models.pdf:pdf},
	issn = {1467937X},
	journal = {Review of Economic Studies},
	mendeley-groups = {Aggregate/Disaggregate Data},
	number = {4},
	pages = {655--680},
	title = {{Combining Micro and Macro Data in Microeconometric Models}},
	volume = {61},
	year = {1994}
}

@article{Abadie2003,
	abstract = {This article investigates the economic effects of conflict, using the terrorist conflict in the Basque Country as a case study. We find that, after the outbreak of terrorism in the late 1960's, per capita GDP in the Basque Country declined about 10 percentage points relative to a synthetic control region without terrorism. In addition, we use the 1998-1999 truce as a natural experiment. We find that stocks of firms with a significant part of their business in the Basque Country showed a positive relative performance when truce became credible, and a negative relative performance at the end of the cease-fire.},
	author = {Abadie, Alberto and Gardeazabal, Javier},
	doi = {10.1257/000282803321455188},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Synthetic Controls/Abadie, Gardeazabal (2003) Economic Costs of Conflict.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {Treatment Effects/Synthetic Controls},
	number = {1},
	pages = {113--132},
	title = {{The Economic Costs of Conflict: A Case Study of the Basque Country}},
	volume = {93},
	year = {2003}
}
@article{Abadie2010,
	abstract = {Building on an idea in Abadie and Gardeazabal (2003), this article investigates the application of synthetic control methods to comparative case studies. We discuss the advantages of these methods and apply them to study the effects of Proposition 99, a large-scale tobacco control program that California implemented in 1988. We demonstrate that, following Proposition 99, tobacco consumption fell markedly in California relative to a comparable synthetic control region. We estimate that by the year 2000 annual per-capita cigarette sales in California were about 26 packs lower than what they would have been in the absence of Proposition 99. Using new inferential methods proposed in this article, we demonstrate the significance of our estimates. Given that many policy interventions and events of interest in social sciences take place at an aggregate level (countries, regions, cities, etc.) and affect a small number of aggregate units, the potential applicability of synthetic control methods to comparative case studies is very large, especially in situations where traditional regression methods are not appropriate. {\textcopyright} 2010 American Statistical Association.},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	doi = {10.1198/jasa.2009.ap08746},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Synthetic Controls/Abadie, Diamond, Hainmueller (2010) Synthetic Control Methods for Comparative Case Studies Estimating the Effect.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Observational studies,Proposition 99,Tobacco control legislation,Treatment effects},
	mendeley-groups = {Treatment Effects/Synthetic Controls},
	number = {490},
	pages = {493--505},
	title = {{Synthetic Control Methods for Comparative Case studies: Estimating the Effect of California's Tobacco Control Program}},
	volume = {105},
	year = {2010}
}

@article{Abadie2021,
	abstract = {Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.},
	author = {Abadie, Alberto},
	doi = {10.1257/jel.20191450},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Synthetic Controls/Abadie (2021) Using Synthetic Controls Feasibility, Data Requirements, and Methodological Aspects†.pdf:pdf},
	journal = {Journal of Economic Literature},
	mendeley-groups = {Treatment Effects/Synthetic Controls},
	number = {2},
	pages = {391--425},
	title = {{Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects}},
	volume = {59},
	year = {2021}
}


@article{Bofinger1975,
	author = {Bofinger, Eve},
	doi = {10.1111/j.1467-842X.1975.tb01366.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Density Estimation/Bofingeb (1975) Estimation of Density Function Using Order Statistics.pdf:pdf},
	journal = {Australian Journal of Statistics},
	mendeley-groups = {Nonparametric Estimation/Density Estimation},
	number = {1},
	pages = {1--7},
	title = {{Estimation of a Density Function Using Order Statistics}},
	volume = {17},
	year = {1975}
}

@article{Renyi1953,
	author = {R{\'{e}}nyi, Alfr{\'{e}}d},
	doi = {10.1007/BF02127580},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/R{\'{e}}nyi (1953) On the theory of order statistics.pdf:pdf},
	journal = {Acta Mathematica Academiae Scientiarum Hungarica},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {191--231},
	title = {{On The Theory Of Order Statistics}},
	volume = {4},
	year = {1953}
}

@book{Gallant1988,
	author = {Gallant, A. Ronald and White, Halbert},
	isbn = {0631157654},
	mendeley-groups = {Limit Theory},
	publisher = {Blackwell Publishing Ltd},
	title = {{A Unified Theory of Estimation and Inference for Nonlinear Dynamic Models}},
	year = {1988}
}

@article{OrdonezCabrera1994,
	author = {{Ordo{\~{n}}ez Cabrera}, Manuel},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Ordo{\~{n}}ez Cabrera (1994) Convergence Of Weighted Sums and Weights UI.pdf:pdf},
	journal = {Collectanea Mathematica},
	mendeley-groups = {Random Useful Math/Probability},
	pages = {121--132},
	title = {{Convergence of Weighted Sums of Random Variables and Uniform Integrability Concerning the Weights}},
	volume = {2},
	year = {1994}
}

@article{Elandt1961,
	abstract = {The general formula for the rth moment of the folded normal distribution is obtained, and formulae for the first four non-central and central moments are calculated explicitly. To illustrate the mode of convergence of the folded normal to the normal distribution, as $\mu$/$\sigma$ = $\theta$ increases, the shape factors $\beta$f1 and $\beta$f2 were calculated and the relationship between them represented graphically. Two methods, one using first and second moments (Method I) and the other using second and fourth moments (Method II) of estimating the parameters $\mu$ and $\sigma$ of the parent normal distribution are presented and their standard errors calculated. The accuracy of both methods, for various values of $\theta$, are discussed. {\textcopyright} 1961 Taylor {\&} Francis Group, LLC.},
	author = {Elandt, Regina C.},
	doi = {10.1080/00401706.1961.10489975},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Elandt (1961) The Folded Normal Distribution.pdf:pdf},
	issn = {15372723},
	journal = {Technometrics},
	mendeley-groups = {Random Useful Math/Probability},
	number = {4},
	pages = {551--562},
	title = {{The Folded Normal Distribution: Two Methods of Estimating Parameters from Moments}},
	volume = {3},
	year = {1961}
}

@article{Pruitt1966,
	author = {Pruitt, William E.},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Probability/Pruitt (1966) Summability of Independent Random Variables.pdf:pdf},
	journal = {Journal of Mathematics and Mechanics},
	mendeley-groups = {Random Useful Math/Probability},
	number = {5},
	pages = {769--776},
	title = {{Summability of Independent Random Variables}},
	volume = {15},
	year = {1966}
}

@book{Bingham1987,
	abstract = {This book is a comprehensive account of the theory and applications of regular variation. It is concerned with the asymptotic behaviour of a real function of a real variable x which is 'close' to a power of x. Such functions are much more than a convenient extension of powers. In many limit theorems regular variation is intrinsic to the result, and exactly characterises the limit behaviour. The book emphasises such characterisations, and gives a comprehensive treatment of those applications where regular variation plays an essential (rather then merely convenient) role. The authors rigorously develop the basic ideas of Karamata theory and de Haan theory including many new results and 'second-order' theorems. They go on to discuss the role of regular variation in Abelian, Tauberian, and Mercerian theorems. These results are then applied in analytic number theory, complex analysis, and probability, with the aim above all of setting the theory in context. A widely scattered literature is thus brought together in a unified approach. With several appendices and a comprehensive list of references, analysts, number theorists, and probabilists will find this an invaluable and complete account of regular variation. It will provide a rigorous and authoritative introduction to the subject for research students in these fields.},
	address = {Cambridge},
	author = {Bingham, N H and Goldie, C M and Teugels, J L},
	doi = {10.1017/CBO9780511721434},
	isbn = {9780521379434},
	mendeley-groups = {Random Useful Math/Regular Variation},
	publisher = {Cambridge University Press},
	title = {{Regular Variation}}, 
	year = {1987}
}

@article{Bertail2004,
	abstract = {In this paper we propose a subsampling estimator for the distribution of statistics diverging at either known or unknown rates when the underlying time series is strictly stationary and strong mixing. Based on our results we provide a detailed discussion of how to estimate extreme order statistics with dependent data and present two applications to assessing financial market risk. Our method performs well in estimating Value at Risk and provides a superior alternative to Hill's estimator in operationalizing Safety First portfolio selection. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
	author = {Bertail, Patrice and Haefke, Christian and Politis, Dimitris N. and White, Halbert},
	doi = {10.1016/S0304-4076(03)00215-X},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Bertail et al. (2004) A Subsampling Approach to Estimating the Distribution of Diverging Extreme Statistics with Applications to Assessing Financial Market Risks p.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Extreme value statistics,Portfolio selection,Resampling methods,Value at Risk},
	mendeley-groups = {Quantile/Extreme Value/Estimators,Resampling/Subsampling},
	number = {2},
	pages = {295--326},
	title = {{Subsampling The Distribution of Diverging Statistics with Applications to Finance}},
	volume = {120},
	year = {2004}
}

@article{Hansen2012,
	abstract = {We consider the problem of obtaining appropriate weights for averaging M approximate (misspecified) models for improved estimation of an unknown conditional mean in the face of non-nested model uncertainty in heteroskedastic error settings. We propose a "jackknife model averaging" (JMA) estimator which selects the weights by minimizing a cross-validation criterion. This criterion is quadratic in the weights, so computation is a simple application of quadratic programming. We show that our estimator is asymptotically optimal in the sense of achieving the lowest possible expected squared error. Monte Carlo simulations and an illustrative application show that JMA can achieve significant efficiency gains over existing model selection and averaging methods in the presence of heteroskedasticity. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
	author = {Hansen, Bruce E. and Racine, Jeffrey S.},
	doi = {10.1016/j.jeconom.2011.06.019},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Hansen, Racine (2012) Jackknife model averaging.pdf:pdf},
	journal = {Journal of Econometrics},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	number = {1},
	pages = {38--46},
	publisher = {Elsevier B.V.},
	title = {{Jackknife Model Averaging}},
	volume = {167},
	year = {2012}
}

@article{Vaida2005,
	author = {Vaida, Florin and Blanchard, Suzette},
	doi = {10.1093/biomet/92.2.351},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Vaida, Blanchard (2005) Conditional Akaike Information for Mixed-Effects Models.pdf:pdf},
	journal = {Biometrika},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	number = {2},
	pages = {351--370},
	title = {{Conditional Akaike Information for Mixed-Effects Models}},
	volume = {92},
	year = {2005}
}

@article{Donohue2011,
	abstract = {We study model selection for clustered data, when the focus is on cluster specific inference. Such data are often modelled using random effects, and conditional Akaike information was proposed in Vaida {\&} Blanchard (2005) and used to derive an information criterion under linear mixed models. Here we extend the approach to generalized linear and proportional hazards mixed models. Outside the normal linear mixed models, exact calculations are not available and we resort to asymptotic approximations. In the presence of nuisance parameters, a profile conditional Akaike information is proposed. Bootstrap methods are considered for their potential advantage in finite samples. Simulations show that the performance of the bootstrap and the analytic criteria are comparable, with bootstrap demonstrating some advantages for larger cluster sizes. The proposed criteria are applied to two cancer datasets to select models when the cluster-specific inference is of interest. {\textcopyright} 2011 Biometrika Trust.},
	author = {Donohue, Michael C. and Overholser, R. and Xu, Ronghui and Vaida, Florin},
	doi = {10.1093/biomet/asr023},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Donohue et al. (2011) Conditional Akaike information under generalized linear and proportional hazards mixed models.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Akaike information,Conditional likelihood,Effective degrees of freedom},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	number = {3},
	pages = {685--700},
	title = {{Conditional Akaike Information Under Generalized Linear and Proportional Hazards Mixed Models}},
	volume = {98},
	year = {2011}
}

@article{Buckland1997,
	author = {Buckland, Stephen T. and Burnham, Kenneth P. and Augustin, Nicole H.},
	doi = {10.2307/2533961},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Buckland, Burnham, Augustin (1997) Model Selection An Integral Part of Inference.pdf:pdf},
	journal = {Biometrics},
	mendeley-groups = {Model Selection and Averaging/Model Selection,Model Selection and Averaging/Averaging},
	number = {2},
	pages = {603--618},
	title = {{Model Selection: An Integral Part of Inference}},
	volume = {53},
	year = {1997}
}

@article{Robinson1991,
	author = {Robinson, G.K.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Robinson (1991) That BLUP is a Good Thing The Estimation of Random Effects.pdf:pdf},
	journal = {Statistical Science},
	keywords = {and phrases,best linear unbiased prediction,blup,credibility theory,esti-,fixed versus random effects,foundations of,kalman filtering,likelihood,mation of random effects,parametric em-,pirical bayes methods,rank-,selection index,small-area estimation,statistics},
	mendeley-groups = {Panel Data/Heterogeneous Panels/RCM Specifications},
	number = {1},
	pages = {15--32},
	title = {{That BLUP is a Good Thing: The Estimation of Random Effects}},
	volume = {6},
	year = {1991}
}

@article{Wan2010,
	author = {Wan, Alan T. K. and Zhang, Xinyu and Zou, Guohua},
	doi = {10.1016/j.jeconom.2009.10.030},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Averaging/Wan, Zhang, Zou, (2010) Least squares model averaging by Mallows criterion.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Model Selection and Averaging/Averaging},
	number = {2},
	pages = {277--283},
	publisher = {Elsevier B.V.},
	title = {{Least Squares Model Averaging by Mallows Criterion}},
	volume = {156},
	year = {2010}
}

@article{Li1987,
	author = {Li, Ker-Chau},
	doi = {10.1214/aos/1176350486},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Li (1987) Asymptotic Optimality for Cp, CL, Cross-Validation and Generalized Cross-Validation Discrete Index Set.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	number = {3},
	pages = {958--975},
	title = {{Asymptotic Optimality for Cp, CL, Cross-Validation and Generalized Cross-Validation: Discrete Index Set}},
	volume = {15},
	year = {1987}
}



@book{Pesaran2015TimeSeriesPanel,
  title = {Time {{Series}} and {{Panel Data Econometrics}}},
  author = {Pesaran, M. Hashem},
  year = {2015},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198736912.001.0001},
  isbn = {978-0-19-873691-2}
}


@article{Pesaran2018,
	abstract = {This paper provides a new comparative analysis of pooled least squares and fixed effects (FE) estimators of the slope coefficients in the case of panel data models when the time dimension (T) is fixed while the cross section dimension (N) is allowed to increase without bounds. The individual effects are allowed to be correlated with the regressors, and the comparison is carried out in terms of an exponent coefficient, $\delta$, which measures the degree of pervasiveness of the FE in the panel. The use of $\delta$ allows us to distinguish between poolability of small N dimensional panels with large T from large N dimensional panels with small T. It is shown that the pooled estimator remains consistent so long as $\delta${\textless}1, and is asymptotically normally distributed if $\delta${\textless}1/2, for a fixed T and as N→∞. It is further shown that when $\delta${\textless}1/2, the pooled estimator is more efficient than the FE estimator. We also propose a Hausman type diagnostic test of $\delta${\textless}1/2 as a simple test of poolability, and propose a pretest estimator that could be used in practice. Monte Carlo evidence supports the main theoretical findings and gives some indications of gains to be made from pooling when $\delta${\textless}1/2.},
	author = {Pesaran, M. Hashem and Zhou, Qiankun},
	doi = {10.1111/obes.12220},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Pesaran, Zhou (2017) To Pool or Not to Pool Revisited.pdf:pdf},
	issn = {14680084},
	journal = {Oxford Bulletin of Economics and Statistics},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Pooling and Partial Pooling},
	number = {2},
	pages = {185--217},
	title = {{To Pool or Not to Pool: Revisited}},
	volume = {80},
	year = {2018}
}

@article{Maddala2001,
	abstract = {The present paper uses small-sigma asymptotics to show that in general the shrinkage estimators have superior properties among the individual least squares estimators, the simple average estimators, the weighted average estimators, estimators obtained by shrinking towards the simple average, and estimators obtained by shrinking towards the weighted average. The shrinkage estimators are used to derive short-run and long-run price and income elasticities for residential natural gas demand and electricity demand in the US based on panel data covering 49 states over 21 years (1970-90). They are also used for out of sample forecasting.},
	author = {Maddala, G. S. and Li, Hongyi and Srivastava, V. K.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Maddala et al. (2001) A Comparative Study of Different Shrinkage Estimators Panel Data Models.pdf:pdf},
	issn = {15297373},
	journal = {Annals of Economics and Finance},
	keywords = {Bayesian shrinkage estimator,Parameter heterogeneity,Stein-rule estimator},
	mendeley-groups = {Panel Data/Heterogeneous Panels,Panel Data/Heterogeneous Panels/Pooling and Partial Pooling},
	number = {1},
	pages = {1--30},
	title = {{A Comparative Study of Different Shrinkage Estimators for Panel Data Models}},
	volume = {2},
	year = {2001}
}

@incollection{Baltagi2008a,
	abstract = {This paper provides a new comparative analysis of pooled least squares and fixed effects (FE) estimators of the slope coefficients in the case of panel data models when the time dimension (T) is fixed while the cross section dimension (N) is allowed to increase without bounds. The individual effects are allowed to be correlated with the regressors, and the comparison is carried out in terms of an exponent coefficient, $\delta$, which measures the degree of pervasiveness of the FE in the panel. The use of $\delta$ allows us to distinguish between poolability of small N dimensional panels with large T from large N dimensional panels with small T. It is shown that the pooled estimator remains consistent so long as $\delta${\textless}1, and is asymptotically normally distributed if $\delta${\textless}1/2, for a fixed T and as N→∞. It is further shown that when $\delta${\textless}1/2, the pooled estimator is more efficient than the FE estimator. We also propose a Hausman type diagnostic test of $\delta${\textless}1/2 as a simple test of poolability, and propose a pretest estimator that could be used in practice. Monte Carlo evidence supports the main theoretical findings and gives some indications of gains to be made from pooling when $\delta${\textless}1/2.},
	author = {Baltagi, Badi H. and Bresson, Georges and Pirotte, Alain},
	booktitle = {The Econometrics of Panel Data},
	chapter = {16},
	doi = {10.1007/978-3-540-75892-1_16},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Baltagi, Bresson, Pirotte (2008) To Pool or Not to Pool The Econometrics of Panel Data.pdf:pdf},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {517--546},
	publisher = {Springer Berlin Heidelberg},
	title = {{To Pool or Not to Pool}},
	year = {2008}
}
а
@article{Bertail1999,
	abstract = {Politis and Romano have put forth a general subsampling methodology for the construction of large-sample confidence regions for a general unknown parameter $\theta$ associated with the probability distribution generating the stationary sequence X1,{\ldots}, Xn. The subsampling methodology hinges on approximating the large-sample distribution of a statistic Tn = Tn(X1,{\ldots}, Xn) that is consistent for $\theta$ at some known rate $\tau$n. Although subsampling has been shown to yield confidence regions for $\theta$ of asymptotically correct coverage under very weak assumptions, the applicability of the methodology as it has been presented so far is limited if the rate of convergence $\tau$n happens to be unknown or intractable in a particular setting. In this article we show how it is possible to circumvent this limitation by (a) using the subsampling methodology to derive a consistent estimator of the rate $\tau$n, and (b) using the estimated rate to construct asymptotically correct confidence regions for $\theta$ based on subsampling. {\textcopyright} 1999 Taylor {\&} Francis Group, LLC.},
	author = {Bertail, Patrice and Politis, Dimitris N. and Romano, Joseph P.},
	doi = {10.1080/01621459.1999.10474151},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Subsampling/Bertail, Politis, Romano (1999) Subsampling With Unknown Rate of Convergence.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Asymptotic inference,Bootstrap,Confidence regions,Jackknife,Nonparametric estimation,Strong mixing,Subseries},
	mendeley-groups = {Resampling/Subsampling},
	number = {446},
	pages = {569--579},
	title = {{On Subsampling Estimators with Unknown Rate of Convergence}},
	volume = {94},
	year = {1999}
}


@article{Pickands1975,
	author = {Pickands, James},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Extreme Value Index Estimation/Pickands (1975) Statistical Inference Using Extreme.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	number = {1},
	pages = {119--131},
	title = {{Statistical Inference Using Extreme Order Statistics}},
	volume = {3},
	year = {1975}
}

@article{Dekkers1989,
	author = {Dekkers, Arnold L.M. and de Haan, Laurens},
	doi = {10.1214/aos/1176347396},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Extreme Value Index Estimation/Dekkers, de Haan (1989) On the Estimation of the Extreme-Value Index and Large Quantile Estimation.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	number = {4},
	pages = {1795--1832},
	title = {{On the Estimation of the Extreme-Value Index and Large Quantile Estimation}},
	volume = {17},
	year = {1989}
}

@article{DeHaan1996b,
	abstract = {Assume that for a measurable function f on (0, ∞) there exist a positive auxiliary function a(t) and some $\gamma$ ∈ ℝ such that $\phi$(x) = limt→∞(f(tx) - f(t))/a(t) = ∫x1 sy-1 ds, x {\textgreater} 0. Then f is said to be of generalized regular variation. In order to control the asymptotic behaviour of certain estimators for distributions in extreme value theory we are led to study regular variation of second order, that is, we assume that limt→∞ (f(tx) - f(t)-a(t)$\phi$(x))a1(t) exists non-trivially with a second auxiliary function a1 (t). We study the possible limit functions in this limit relation (denning generalized regular variation of second order) and their domains of attraction. Furthermore we give the corresponding relation for the inverse function of a monotone f with the stated property. Finally, we present an Abel-Tauber theorem relating these functions and their Laplace transforms.},
	author = {{De Haan}, Laurens and Stadtm{\"{u}}ller, Ulrich},
	doi = {10.1017/s144678870000046x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/De Haan, Stadtm{\"{u}}ller (1996) Generalized regular variation of second order.pdf:pdf},
	issn = {14467887},
	journal = {Journal of the Australian Mathematical Society},
	keywords = {Abelian theorem,Domain of attraction,Inverse functions,Laplace transform,Limit functions,Regular variation,Second order variation,Tauberian theorem},
	mendeley-groups = {Quantile/Extreme Value/General Theory},
	number = {3},
	pages = {381--395},
	title = {{Generalized regular variation of second order}},
	volume = {61},
	year = {1996}
}

@article{Xie2011,
	abstract = {This article develops a unifying framework, as well as robust meta-analysis approaches, for combining studies from independent sources. The device used in this combination is a confidence distribution (CD), which uses a distribution function, instead of a point (point estimator) or an interval (confidence interval), to estimate a parameter of interest. A CD function contains a wealth of information for inferences, and it is a useful device for combining studies from different sources. The proposed combining framework not only unifies most existing metaanalysis approaches, but also leads to development of new approaches. We illustrate in this article that this combining framework can include both the classical methods of combining p-values and modern model-based meta-analysis approaches. We also develop, under the unifying framework, two new robust meta-analysis approaches, with supporting asymptotic theory. In one approach each study size goes to infinity, and in the other approach the number of studies goes to infinity. Our theoretical development suggests that both these robust meta-analysis approaches have high breakdown points and are highly efficient for normal models. The new methodologies are applied to study-level data from publications on prophylactic use of lidocaine in heart attacks and a treatment of stomach ulcers. The robust methods performed well when data are contaminated and have realistic sample sizes and number of studies. {\textcopyright} 2011 American Statistical Association.},
	author = {Xie, Minge and Singh, Kesar and Strawderman, William E.},
	doi = {10.1198/jasa.2011.tm09803},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Confidence Distributions/Xie, Singh, Strawderman (2011) Confidence Distributions and a Unifying Framework for Meta-Analysis.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Combination of p-values,Fixed-effects model,Random-effects model,Robust methods},
	mendeley-groups = {Confidence Sets/Confidence Distributions},
	number = {493},
	pages = {320--333},
	title = {{Confidence Distributions and a Unifying Framework for Meta-Analysis}},
	volume = {106},
	year = {2011}
}

@article{Geweke1981,
	author = {Geweke, John},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Tests/Geweke (1981) The Approximate Slopes of Econometric Tests.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Efficiency/Efficiency of Tests/Combining Tests},
	number = {6},
	pages = {1427--1442},
	title = {{The Approximate Slopes of Econometric Tests}},
	volume = {49},
	year = {1981}
}

@article{Littell1973,
	abstract = {It was shown by Littell and Folks [3] that Fisher's method of combining independent tests is asymptotically optimal among four commonly used methods of combining, according to Bahadur relative efficiency. The result of the present article is that Fisher's method is asymptotically optimal among essentially all methods of combining independent tests. {\textcopyright} Taylor {\&} Francis Group, LLC.},
	author = {Littell, Ramon C. and Folks, J. Leroy},
	doi = {10.1080/01621459.1973.10481362},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Tests/Littell, Folks (1971) Asymptotic Optimality of Fisher's Method of Combining Independent Tests II.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Efficiency/Efficiency of Tests/Combining Tests},
	number = {341},
	pages = {193--194},
	title = {{Asymptotic Optimality of Fisher's Method of Combining Independent Tests II}},
	volume = {68},
	year = {1973}
}

@article{Bahadur1967,
	abstract = {The classical chi-square test for independence in a two-way contigency table often rejects the independence hypothesis at an extremely small significance level, particularly when the sample size is large. This paper proposes some alternative distributions to independence, to help interpret the X{\^{}}2 statistic in such situations. The uniform alternative, in which every possible contingency table of the given dimension and sample size receives equal probability, leads to the volume test, as originally suggested in a regression context by H. Hotelling. Exponential family theory is used to generate a class of intermediate alternatives between independence and uniformity, leading to a random effects model for contingency tables.},
	author = {Bahadur, Raghu R.},
	doi = {10.1214/aoms/1177698949},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Tests/Bahadur (1966) Rates of Convergence of Estimates and Test Statistics.pdf:pdf},
	issn = {0003-4851},
	journal = {The Annals of Mathematical Statistics},
	mendeley-groups = {Efficiency/Efficiency of Tests/Combining Tests},
	number = {2},
	pages = {303--324},
	title = {{Rates of Convergence of Estimates and Test Statistics}},
	volume = {38},
	year = {1967}
}

@article{Littell1971,
	abstract = {Four methods of combining independent tests of hypothesis are compared via exact Bahadur relative efficiency. The methods considered are Fisher's method, the mean of the normal transforms of the significance levels, the maximum significance level, and the minimum significance level. None of these is uniformly more powerful than the others, but, according to Bahadur efficiency, Fisher's method is the most efficient of the four. In some cases, Fisher's method is most efficient of all tests based on the data, but this is not generally true. {\textcopyright} Taylor {\&} Francis Group, LLC.},
	author = {Littell, Ramon C. and Folks, J. Leroy},
	doi = {10.1080/01621459.1971.10482347},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Tests/Littell, Folks (1971) Asymptotic Optimality of Fisher's Method of Combining Independent Tests.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Efficiency/Efficiency of Tests/Combining Tests},
	number = {336},
	pages = {802--806},
	title = {{Asymptotic Optimality of Fisher's Method of Combining Independent Tests}},
	volume = {66},
	year = {1971}
}

@article{Singh2005,
	archivePrefix = {arXiv},
	arxivId = {arXiv:math/0504507v1},
	author = {Singh, Kesar and Xie, Minge and Strawderman, William E.},
	doi = {10.1214/009053604000001084},
	eprint = {0504507v1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Confidence Distributions/Singh, Xie, Strawderman (2005)  Combining Information From Independent Sources Through Confidence Distributions.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {and phrases,bootstrap,com-,combining information,common mean problem,confidence distribution,ference,frequentist in-,meta-analysis,p -value function,puter intensive methods,robust scale,u -statistic},
	mendeley-groups = {Confidence Sets/Confidence Distribution},
	number = {1},
	pages = {159--183},
	primaryClass = {arXiv:math},
	title = {{Combining Information From Independent Sources Through Confidence Distributions}},
	volume = {33},
	year = {2005}
}
@article{Singh2007,
	abstract = {The notion of confidence distribution (CD), an entirely frequentist concept, is in essence a Neymanian interpretation of {\{}F{\}}isher's Fiducial distribution. It contains information related to every kind of frequentist inference. In this article, a CD is ...},
	author = {Singh, Kesar and Xie, Minge and Strawderman, William E.},
	doi = {10.1214/074921707000000102},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Confidence Distributions/Singh, Xie, Strawderman (2007)  Confidence Distribution (CD) Distribution Estimator of a Parameter.pdf:pdf},
	journal = {Complex Datasets and Inverse Problems},
	mendeley-groups = {Confidence Sets/Confidence Distribution},
	number = {Cd},
	pages = {132--150},
	title = {{Confidence Distribution (CD): Distribution Estimator of a Parameter}},
	volume = {54},
	year = {2007}
}

@article{Xie2013,
	author = {Xie, Min-ge and Singh, Kesar},
	doi = {10.1111/insr.12000},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Confidence Distributions/Xie, Singh (2013) Confidence Distribution, the Frequentist Distribution Estimator.pdf:pdf},
	journal = {International Statistical Review},
	keywords = {bayesian method,confidence distribution,estimation theory,fiducial distribution,likelihood function,statistical inference},
	mendeley-groups = {Confidence Sets/Confidence Distribution},
	number = {1},
	pages = {3--39},
	title = {{Confidence Distribution, the Frequentist Distribution Estimator of a Parameter: A Review}},
	volume = {81},
	year = {2013}
}

@article{Schweder2002,
	author = {Schweder, Tore and Hjort, Nils Lid},
	doi = {10.1111/1467-9469.00285},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Confidence Distributions/Schweder, Hjort (2002) Confidence and Likelihood.pdf:pdf},
	journal = {Scandinavian Journal of Statistics},
	mendeley-groups = {Confidence Sets/Confidence Distribution},
	number = {2},
	pages = {309--332},
	title = {{Confidence and Likelihood}},
	volume = {29},
	year = {2002}
}

@book{Serfling1980,
	author = {Serfling, Robert J.},
	doi = {10.1002/9780470316481},
	isbn = {9780471024033},
	mendeley-groups = {Random Useful Math/Probability},
	publisher = {John Wiley {\&} Sons, Ltd},
	title = {{Approximation Theorems of Mathematical Statistics}},
	year = {1980}
}

@article{Bickel1997,
	author = {Bickel, Peter J. and G{\"{o}}tze, Friedrich and {Van Zwet}, Willem R.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Subsampling/Bickel, G{\"{o}}tze, van Zwet (1997) Resampling Fewer Than n  Observations.pdf:pdf},
	journal = {Statistica Sinica},
	keywords = {and phrases,asymptotic,bootstrap,nonparametric,parametric,test-},
	mendeley-groups = {Resampling/Subsampling},
	number = {1},
	pages = {1--31},
	title = {{Resampling Fewer Than n Observations: Gains, Losses, and Remedies For Losses}},
	volume = {7},
	year = {1997}
}

@article{Bickel1981,
	author = {Bickel, Peter J. and Freedman, David A.},
	doi = {10.1214/aos/1176345637},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Bootstrap/Bickel, Freedman (1981) Some Asymptotic Theory for the Bootstrap.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Resampling/Bootstrap},
	number = {6},
	pages = {1196--1217},
	title = {{Some Asymptotic Theory For The Bootstrap}},
	volume = {9},
	year = {1981}
}

@article{Efron1979,
	author = {Efron, Bradley},
	doi = {10.1214/aos/1176344552},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Bootstrap/Efron (1979) Bootstrap Methods Another Look at the Jackknife.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Resampling/Bootstrap},
	number = {1},
	pages = {1--26},
	title = {{Bootstrap Methods: Another Look at the Jackknife}},
	volume = {7},
	year = {1979}
}

@article{Bickel2008b,
	author = {Bickel, Peter J. and Sakov, Anat},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Subsampling/Bickel, Sakov (2008) On Choice of m.pdf:pdf},
	journal = {Statistica Sinica},
	keywords = {adaptive choice,and phrases,bootstrap,choice of m,data-dependent,extrema,m out of n,rule},
	mendeley-groups = {Resampling/Subsampling},
	number = {3},
	pages = {967--985},
	title = {{On The Choice of m In The m Out Of n Bootstrap And Confidence Bounds For Extrema}},
	volume = {18},
	year = {2008}
}

@article{Jeyaratnam1985,
	author = {Jeyaratnam, Sakthivel},
	doi = {10.1016/0167-7152(85)90061-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Clas/Jeyaratnam (1985) Minimum volume confidence regions.pdf:pdf},
	journal = {Statistics {\&} Probability Letters},
	keywords = {confidence region,jacobian,neyman-pearson lemma,pivotal quantity},
	mendeley-groups = {Confidence Sets/Classical Theory},
	number = {October},
	pages = {307--308},
	title = {{Minimum Volume Confidence Regions}},
	volume = {C},
	year = {1985}
}

@article{Pratt1961,
	author = {Pratt, John W.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Clas/Pratt (1961) Length of Confidence Intervals.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Confidence Sets/Classical Theory},
	number = {295},
	pages = {549--567},
	title = {{Length of Confidence Intervals}},
	volume = {56},
	year = {1961}
}

@article{Politis1994,
	author = {Politis, Dimitris N. and Romano, Joseph P.},
	doi = {10.1214/aos/1176325770},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Resampling/Subsampling/Politis, Romano (1994) Large Sample CIs Based on Subsamples Under Minimal Assumptions.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Resampling/Subsampling},
	number = {4},
	pages = {2031--2050},
	title = {{Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions}},
	volume = {22},
	year = {1994}
}

@unpublished{Knight1999,
	abstract = {Epi-convergence in distribution is a useful tool in establishing limiting distributions of $\backslash$argmin" estimators; however, it is not always easy to nd the epi-limit of a given sequence of objective functions. In this paper, we deene the notion of stochastic equi-lower-semicontinuity of a sequence of random objective functions. It is shown that epi-convergence in distribution and nite dimensional convergence in distribution (to a given limit) of a sequence of random objective functions are equivalent under this condition.},
	author = {Knight, Keith},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Optimization/Knight (1999) Epi-Convergence in Distribution And Stochastic Equi-Continuity.pdf:pdf},
	keywords = {and phrases,argmin estimators,convergence in distribution,epi-convergence},
	mendeley-groups = {Limit Theory/Optimization},
	pages = {1--22},
	title = {{Epi-Convergence in Distribution and Stochastic Equi-semicontinuity}}, 
	year = {1999}
}

@article{Gleser1987,
	author = {Gleser, Leon Jay and Hwang, Jiunn T.},
	doi = {10.1214/aos/1176350597},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Gleser, Hwang (1987) Nonexistence Of Confidence Sets WIth Finite Expected Diameter.pdf:pdf},
	issn = {00905364},
	journal = {The Annals of Statistics},
	mendeley-groups = {Confidence Sets/Nonstandard And Failures},
	number = {4},
	pages = {1351--1362},
	title = {{The Nonexistence of 100(1-alpha){\%} Confidence Sets of Finite Expected Diameter in Errors-in-Variables and Related Models}},
	volume = {15},
	year = {1987}
}


@article{Windmeijer2005,
	abstract = {Monte Carlo studies have shown that estimated asymptotic standard errors of the efficient two-step generalized method of moments (GMM) estimator can be severely downward biased in small samples. The weight matrix used in the calculation of the efficient two-step GMM estimator is based on initial consistent parameter estimates. In this paper it is shown that the extra variation due to the presence of these estimated parameters in the weight matrix accounts for much of the difference between the finite sample and the usual asymptotic variance of the two-step GMM estimator, when the moment conditions used are linear in the parameters. This difference can be estimated, resulting in a finite sample corrected estimate of the variance. In a Monte Carlo study of a panel data model it is shown that the corrected variance estimate approximates the finite sample variance well, leading to more accurate inference. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
	author = {Windmeijer, Frank},
	doi = {10.1016/j.jeconom.2004.02.005},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/Inference/Windmeijer (2005) A finite sample correction for the variance of linear efficient two-step GMM estimators.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Generalized method of moments,Panel data,Variance correction},
	mendeley-groups = {GMM/Inference},
	number = {1},
	pages = {25--51},
	title = {{A Finite Sample Correction for the Variance of Linear Efficient Two-Step GMM Estimators}},
	volume = {126},
	year = {2005}
}

@article{Newey1988,
	author = {Newey, Whitney K.},
	doi = {10.1017/S0266466600012093},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/Newey (1988) Asymptotic Equivalence of Closest Moments and GMM Estimators.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {GMM/Different Estimators},
	number = {2},
	pages = {336--340},
	title = {{Asymptotic Equivalence of Closest Moments and GMM Estimators}},
	volume = {4},
	year = {1988}
}

@article{DeJong2002,
	abstract = {This paper considers generalized method of moment-type estimators for which a criterion function is minimized that is not the "standard" quadratic distance measure but instead is a general Lp distance measure. It is shown that the resulting estimators are root-n consistent but not in general asymptotically normally distributed, and we derive the limit distribution of these estimators. In addition, we prove that it is not possible to obtain estimators that are more efficient than the "usual" L2-GMM estimators by considering Lp-GMM estimators. We also consider the issue of the choice of the weight matrix for Lp-GMM estimators.},
	author = {{De Jong}, Robert and Han, Chirok},
	doi = {10.1017/S0266466602182107},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/De Jong, Han (2002) Properties of Lp GMM Estimators.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {GMM/Different Estimators},
	number = {2},
	pages = {491--504},
	title = {{The Properties of Lp-GMM Estimators}},
	volume = {18},
	year = {2002}
}

@article{Donald2000,
	abstract = {Recent work on Generalized Method of Moments (GMM) estimators has suggested that the continuous updating estimator is less biased than the commonly used two-step estimator. We show that the continuous updating estimator can be interpreted as jackknife estimator. The interpretation gives some insight into why there is less bias associated with this estimator. {\textcopyright} Elsevier Science S.A.},
	author = {Donald, Stephen G. and Newey, Whitney K.},
	doi = {10.1016/s0165-1765(99)00281-5},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/Donald, Newey (2000) A jackknife interpretation of the continuous updating estimator.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {Bias,C1,C2,C3,C4,Continuous updating estimator,Generalized Method of Moments,Instrumental variables,Jackknife},
	mendeley-groups = {GMM/Different Estimators},
	number = {3},
	pages = {239--243},
	title = {{A Jackknife Interpretation of the Continuous Updating Estimator}},
	volume = {67},
	year = {2000}
}

@book{Hall2004,
	author = {Hall, Alastair H.},
	isbn = {9780198775201},
	mendeley-groups = {GMM},
	pages = {1--416},
	publisher = {Oxford University Press},
	title = {{Generalized Method of Moments}},
	year = {2004}
}

@article{Hansen1996,
	author = {Hansen, Lars P. and Heaton, John and Yaron, Amir},
	doi = {10.2307/1392442},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/Hansen, Heaton, Yaron (1996) Finite-Sample Properties of Some Alternative GMM Estimators.pdf:pdf},
	journal = {Journal of Business and Economic Statistics},
	keywords = {1990a,all of our experiments,article is to investigate,asset pricing,come from,generalized method of moments,kocherlakota,method of moments,monte carlo,sample properties of generalized,the purpose of this,the small-},
	mendeley-groups = {GMM,GMM/Different Estimators},
	number = {3},
	pages = {262--280},
	title = {{Finite-Sample Properties of Some Alternative GMM Estimators}},
	volume = {14},
	year = {1996}
}

@article{Newey1987b,
	author = {Newey, Whitney K. and West, Kenneth D.},
	doi = {10.2307/1913610},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/HAC/Newey, West (1987).pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Robustness/Of Errors},
	number = {3},
	pages = {703--708},
	title = {{A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix}},
	volume = {55},
	year = {1987}
}

@article{Portnoy2000,
	author = {Portnoy, Stephen and Jure{\v{c}}kov{\'{a}}, Jana},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Portnoy, Jureckova (2000) On Extreme Regression Quantiles.pdf:pdf},
	journal = {Extremes},
	keywords = {ams 1991 subject classi,cations,extreme value distribution,regression quantiles,tail behavior},
	mendeley-groups = {Quantile/Extreme Value/Regression},
	number = {3},
	pages = {227--243},
	title = {{On Extreme Regression Quantiles}},
	volume = {2},
	year = {2000}
}

@article{Potscher1991,
	author = {P{\"{o}}tscher, Benedikt M.},
	doi = {10.1017/S0266466600004382},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/P{\"{o}}tscher (1991) Effects of Model Selection on Inference.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	number = {2},
	pages = {163--185},
	title = {{Effects of Model Selection on Inference}},
	volume = {7},
	year = {1991}
}

@article{Hansen1982,
	author = {Hansen, Lars P.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/GMM/Hansen (1982) Large Sample Properties of Generalized Method of Moments Estimators.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {GMM/Limit Theory,Limit Theory},
	number = {4},
	pages = {1029--1054},
	title = {{Large Sample Properties of Generalized Method of Moments Estimators}},
	volume = {50},
	year = {1982}
}

@article{Leon-Ledesma2010,
	author = {Le{\'{o}}n-Ledesma, Miguel A. and McAdam, Peter and Willman, Alpo},
	doi = {10.1257/aer.100.4.1330},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Le{\'{o}}n-Ledesma, McAdam, Willman (2010) Identifying the Elasticity of Substitution with Biased Technical Change.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {September},
	pages = {1330--1357},
	title = {{Identifying the Elasticity of Substitution with Biased Technical Change}},
	volume = {100},
	year = {2010}
}


@article{DeLoecker2020,
	author = {{De Loecker}, Jan and Eeckhout, Jan and Unger, Gabriel},
	doi = {10.1093/qje/qjz041},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/De Loecker, Eeckhout, Unger (2020) Rise Of Market Power.pdf:pdf},
	journal = {The Quarterly Journal of Economics},
	mendeley-groups = {Applied/Production Function Estimation},
	pages = {561--644},
	title = {{The Rise of Market Power and the Macroeconomic Implications}},
	volume = {135},
	year = {2020}
}

@article{Chernozhukov2011,
	abstract = {Quantile regression (QR) is an increasingly important empirical tool in economics and other sciences for analysing the impact a set of regressors has on the conditional distribution of an outcome. Extremal QR, or QR applied to the tails, is of interest in many economic and financial applications, such as conditional value at risk, production efficiency, and adjustment bands in (S, s) models. This paper provides feasible inference tools for extremal conditional quantile models that rely on extreme value approximations to the distribution of self-normalized QR statistics. The methods are simple to implement and can be of independent interest even in the univariate (non-regression) case. We illustrate the results with two empirical examples analysing extreme fluctuations of a stock return and extremely low percentiles of live infant birthweight in the range between 250 and 1500 g. {\textcopyright} The Author 2011. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
	archivePrefix = {arXiv},
	arxivId = {0912.5013},
	author = {Chernozhukov, Victor and Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
	doi = {10.1093/restud/rdq020},
	eprint = {0912.5013},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Chernozhukov, Fernandez-Val (2011) Inference for Extremal Conditional Quantile Models, with an Application to Market and Birthweight Risks.pdf:pdf},
	issn = {00346527},
	journal = {Review of Economic Studies},
	keywords = {Birthweights,Extreme value theory,Feasible inference,Market risk,Quantile regression,Stress testing,Systemic risk},
	mendeley-groups = {Quantile/Extreme Value,Quantile/Extreme Value/Regression},
	number = {2},
	pages = {559--589},
	title = {{Inference for Extremal Conditional Quantile Models, With An Application to Market and Birthweight Risks}},
	volume = {78},
	year = {2011}
}

@article{Li2011,
	author = {Li, Deyuan and Peng, Liang and Xu, Xinping},
	doi = {10.1007/s10687-010-0118-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Extreme Value Index Estimation/Li et al. (2010) Bias reduction for endpoint estimation.pdf:pdf},
	journal = {Extremes},
	keywords = {62g32,ams 2000 subject classification,bias reduction,endpoint,extreme value index,order statistics},
	mendeley-groups = {Quantile/Extreme Value/Estimators},
	number = {April 2010},
	pages = {393--412},
	title = {{Bias reduction for endpoint estimation}},
	year = {2011}
}

@article{DeHaan1996,
	author = {de Haan, Laurens and Resnick, Sidney I.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/General Extreme Value Theory/De Haan, Resnick (1995) Second-Order Regular Variation and Rates of Convergence in Extreme-Value Theory.pdf:pdf},
	journal = {The Annals of Probability},
	mendeley-groups = {Quantile/Extreme Value/General Theory},
	number = {1},
	pages = {97--124},
	title = {{Second-Order Regular Variation and Rates of Convergence in Extreme-Value Theory}},
	volume = {24},
	year = {1996}
}
@article{Drees1998,
	abstract = {Many estimators of the extreme value index of a distribution function F that are based on a certain number kn of largest order statistics can be represented as a statistical tail functional, that is a functional T applied to the empirical tail quantile function Qn. We study the asymptotic behaviour of such estimators with a scale and location invariant functional T under weak second order conditions on F. For that purpose first a new approximation of the empirical tail quantile function is established. As a consequence we obtain weak consistency and asymptotic normality of T(Qn) if T is continuous and Hadamard differentiable, respectively, at the upper quantile function of a generalized Pareto distribution and kn tends to infinity sufficiently slowly. Then we investigate the asymptotic variance and bias. In particular, those functionals T are characterized that lead to an estimator with minimal asymptotic variance. Finally, we introduce a method to construct estimators of the extreme value index with a made-to-order asymptotic behaviour.},
	author = {Drees, Holger},
	doi = {10.1111/1467-9469.00097},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Drees (1998) On Smooth Statistical Tail Functionals.pdf:pdf},
	issn = {03036898},
	journal = {Scandinavian Journal of Statistics},
	keywords = {Adaptive estimator,Empirical tail quantile function,Extreme value distribution,Extreme value index,Hadamard differentiability,Statistical functional,Strong approximation},
	mendeley-groups = {Quantile/Extreme Value/General Theory},
	number = {1},
	pages = {187--210},
	title = {{On smooth statistical tail functionals}},
	volume = {25},
	year = {1998}
}

@article{Hotz1994,
	author = {Hotz, V. Joseph and Miller, Robert A.},
	doi = {10.2307/2298122},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Dynamic/Hotz, Miller (1994) Conditional Choice Probabilities and the Estimation of Dynamic Models.pdf:pdf},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Discrete Choice/Dynamic},
	number = {3},
	pages = {497--529},
	title = {{Conditional Choice Probabilities and the Estimation of Dynamic Models}},
	volume = {60},
	year = {1994}
}

@article{Eckstein1989b,
	author = {Eckstein, Zvi and Wolpin, Kenneth I},
	doi = {10.2307/2297553},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Dynamic/Eckstein, Wolpin (1989) Dynamic Labour Force Participation of Married Women and Endogenous Work Experience.pdf:pdf},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Discrete Choice/Dynamic},
	number = {3},
	pages = {375--390},
	title = {{Dynamic Labour Force Participation of Married Women and Endogenous Work Experience}},
	volume = {56},
	year = {1989}
}

@article{Valentinyi2008,
	abstract = {Many applications in economics use multi-sector versions of the growth model. In this paper, we measure the income shares of capital and labor at the sectoral level for the US economy. We also decompose the capital shares into the income shares of land, structures, and equipment. We find that the capital shares differ across sectors. For example, the capital share of agriculture is more than two times that of construction and more than 50{\%} larger than that of the aggregate economy. Moreover, agriculture has by far the largest land share, which mostly explains why it has the largest capital share. Our numbers can directly be used to calibrate standard multi-sector models. Alternatively, if one wants to abstract from differences in sector capital shares, our numbers can be used to establish that this is not crucial for the results. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
	author = {Valentinyi, {\'{A}}kos and Herrendorf, Berthold},
	doi = {10.1016/j.red.2008.02.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Factor Income Shares/Valentinyi, Herrendorf (2008) Measuring factor income shares at the sectoral level.pdf:pdf},
	issn = {10942025},
	journal = {Review of Economic Dynamics},
	keywords = {Industry-by-commodity total requirement matrix,Input-output tables,Sector factor shares},
	mendeley-groups = {Applied/Factor Income Shares},
	number = {4},
	pages = {820--835},
	title = {{Measuring factor income shares at the sectoral level}},
	volume = {11},
	year = {2008}
}

@incollection{Blundell2003,
	author = {Blundell, Richard and Powell, James L},
	booktitle = {Advances in Economics and Econometrics: Theory and Applications, Eighth World Congress},
	doi = {10.1017/CBO9780511610257.011},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Blundell, Powell (2003) Endogeneity in Nonparametric and.pdf:pdf},
	mendeley-groups = {Semiparametric Estimation/Partially Linear Regression},
	pages = {312--357},
	title = {{Endogeneity in Nonparametric and Semiparametric Regression Models}},
	year = {2003}
}
@article{Florens2012,
	abstract = {We consider the semi-parametric regression model Y=X t$\beta$+$\phi$(Z) where $\beta$ and $\phi$({\textperiodcentered}) are unknown slope coefficient vector and function, and where the variables (X, Z) are endogenous. We propose necessary and sufficient conditions for the identification of the parameters in the presence of instrumental variables. We also focus on the estimation of $\beta$. It is known that the presence of $\phi$ may lead to a slow rate of convergence for the estimator of $\beta$. An additional complication in the fully endogenous model is that the solution of the equation necessitates the inversion of a compact operator that has to be estimated non-parametrically. In general this inversion is not stable, thus the estimation of $\beta$ is ill-posed. In this paper, a -consistent estimator for $\beta$ is derived in this setting under mild assumptions. One of these assumptions is given by the so-called source condition that is explicitly interpreted in the paper. Monte Carlo simulations demonstrate the reasonable performance of the estimation procedure on finite samples. {\textcopyright} 2012 The Author(s). The Econometrics Journal {\textcopyright} 2012 Royal Economic Society.},
	author = {Florens, Jean Pierre and Johannes, Jan and {Van Bellegem}, S{\'{e}}bastien},
	doi = {10.1111/j.1368-423X.2011.00358.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Florens, Johannes, Van Bellegem  (2012) Instrumental regression in partially linear models.pdf:pdf},
	issn = {13684221},
	journal = {Econometrics Journal},
	keywords = {Endogeneity,Ill-posed inverse problem,Instrumental variables,Partially linear model,Root-N consistent estimation,Semi-parametric regression,Tikhonov regularization},
	mendeley-groups = {Semiparametric Estimation/Partially Linear Regression},
	number = {2},
	pages = {304--324},
	title = {{Instrumental Regression in Partially Linear Models}},
	volume = {15},
	year = {2012}
}
@article{Delgado2014,
	abstract = {We propose a simple kernel estimator for semiparametric partial linear models with endogeneity in the nonparametric function. Compared to the existing backfitting estimator, our estimator is notationally simpler and relatively easier to implement. We also discuss data-driven bandwidth selection to implement this estimator in practice. Monte Carlo exercises show that the finite sample performance of these two estimators is similar. {\textcopyright} 2014 Elsevier B.V.},
	author = {Delgado, Michael S. and Parmeter, Christopher F.},
	doi = {10.1016/j.econlet.2014.04.032},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Delgado,  Parmeter  (2014) A simple estimator for partial linear regression with endogenous nonparametric variables..pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {Endogeneity,Instrumental variables,Monte carlo,Partial linear,Semiparametric},
	mendeley-groups = {Semiparametric Estimation/Partially Linear Regression},
	number = {1},
	pages = {100--103},
	publisher = {Elsevier B.V.},
	title = {{A Simple Estimator For Partial Linear Regression with Endogenous Nonparametric Variables}}, 
	volume = {124},
	year = {2014}
}

@article{Wolpin1987,
	abstract = {This paper presents a finite horizon job search model that is econometrically implemented using all of the restrictions implied by the theory. Following a sample of male high school graduates from the youth cohort of the National Longitudinal Surveys from graduation to employment, search parameters such as the cost of search, the probability of receiving an offer, the discount factor, and those from the wage offer distribution are estimated. Reservation wages and offer probabilities are estimated to be quite low. Simulations are performed of the impact of the parameters on the expected duration of unemployment. For example, it is estimated that an offer probability of unity, as opposed to the estimate of approximately only one per cent per week, would reduce the expected duration of unemployment from 46 weeks to 20 weeks.},
	author = {Wolpin, Kenneth I.},
	doi = {10.2307/1911030},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Dynamic/Wolpin (1987) Estimating a Structural Search Model The Transition from School to Work.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {a job search model,are implied by job,cally implemented using all,duration of unemployment,is that it provides,job search,of the restrictions that,paper is to estimate,reservation wages,search,structural approach in general,that is econometri-,the purpose of this,the usefulness of the,theory},
	mendeley-groups = {Discrete Choice/Dynamic},
	number = {4},
	pages = {801},
	title = {{Estimating a Structural Search Model: The Transition from School to Work}},
	volume = {55},
	year = {1987}
}
@article{Eckstein1989,
	abstract = {Our main goal is to quantify the returns to a career in the United States Congress. We specify a dynamic model of career decisions of a member of Congress and estimate this model using a newly collected dataset. Given estimates of the structural model, we ...},
	author = {Eckstein, Zvi and Wolpin, Kenneth I.},
	doi = {10.2307/145996},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Dynamic/Eckstein, Wolpin (1989) The Specification and Estimation of Dynamic Stochastic Discrete Choice Models.pdf:pdf},
	issn = {0022166X},
	journal = {The Journal of Human Resources},
	mendeley-groups = {Discrete Choice/Dynamic},
	number = {4},
	pages = {562},
	title = {{The Specification and Estimation of Dynamic Stochastic Discrete Choice Models: A Survey}},
	volume = {24},
	year = {1989}
}

@article{Chamberlain1980,
	abstract = {Discusses several approaches to the analysis of covariance with qualitative data. Maximization of the joint likelihood function; Application of conditional likelihood function; Equation for multinomial logit for grouped data.},
	author = {Chamberlain, Gary},
	doi = {10.2307/2297110},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Chamberlain (1980) Analysis of Covariance with Qualitative Data.pdf:pdf},
	issn = {00346527},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {1},
	pages = {225},
	title = {{Analysis of Covariance with Qualitative Data}},
	volume = {47},
	year = {1980}
}

@article{Chamberlain1984,
	author = {Chamberlain, Gary},
	doi = {10.1016/S1573-4412(84)02014-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Reviews/Chamberlain (1984) Panel Data.pdf:pdf},
	issn = {15734412},
	journal = {Handbook of Econometrics},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	pages = {1247--1318},
	title = {{Chapter 22 Panel data}},
	volume = {2},
	year = {1984}
}

@article{Hausman1984,
	author = {Hausman, Jerry A. and McFadden, Daniel},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Hausman, McFadden (1984) Specification Tests for The Multinomial Logit Model.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Discrete Choice},
	number = {5},
	pages = {1219--1240},
	title = {{Specification Tests for the Multinomial Logit Model}},
	volume = {52},
	year = {1984}
}


@article{Vijverberg2011,
	abstract = {The Independence of Irrelevant Alternatives assumption inherent in multinomial logit models$\backslash$nis most frequently tested with a Hausman-McFadden test. As is confirmed by many findings$\backslash$nin the literature, this test sometimes produces negative outcomes, in contradiction of its$\backslash$nasymptotic $\chi$2 distribution. This problem is caused by the use of an improper variance matrix$\backslash$nand may lead to an invalid statistical inference even when the test value is positive. With a$\backslash$ncorrect specification of the variance, the sampling distribution for small samples is indeed$\backslash$nclose to a $\chi$2 distribution.},
	author = {Vijverberg, Wim},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Vijverberg (2011) Testing for IIA with the Hausman-McFadden Test.pdf:pdf},
	journal = {IZA Discussion Paper},
	keywords = {Discrete Regression and Qualitative Choice Models,Discrete Regressors,Econometric and Statistical Methods and Methodology: General,Hausman-McFadden test,Hypothesis Testing: General,IIA assumption,Mathematical and Quantitative Methods,Multiple Variables,Multiple or Simultaneous Equation Models,Proportions,multinomial logit},
	mendeley-groups = {Discrete Choice},
	number = {5826},
	title = {{Testing for IIA with the Hausman-McFadden test}},
	year = {2011}
}

@article{Ai2003,
	abstract = {The magnitude of the interaction effect in nonlinear models does not equal the marginal effect of the interaction term, can be of opposite sign, and its statistical significance is not calculated by standard software. We present the correct way to estimate the magnitude and standard errors of the interaction effect in nonlinear models. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
	author = {Ai, Chunrong and Norton, Edward C.},
	doi = {10.1016/S0165-1765(03)00032-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Ai, Norton (2003) Interaction terms in logit and probit models..pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {Interaction effect,Interaction term,Logit,Nonlinear models,Probit},
	mendeley-groups = {Discrete Choice},
	number = {1},
	pages = {123--129},
	title = {{Interaction Terms in Logit and Probit Models}},
	volume = {80},
	year = {2003}
}

@article{Vuong1988,
	abstract = {A two-step maximum likelihood procedure is proposed for estimating simultaneous probit models and is compared to alternative limited information estimators. Conditions under which each estimator attains the Cramer-Rao lower bound are obtained. Simple tests for exogeneity based on the new two-step estimator are proposed and are shown to be asymptotically equivalent to one another and to have the same local asymptotic power as classical tests based on the limited information maximum likelihood estimator. Finite sample comparisons between the new and alternative estimators are presented based on some Monte Carlo evidence. The performance of the proposed tests for exogeneity is also assessed.},
	author = {Vuong, Quang H and Rivers, Douglas},
	doi = {10.1016/0304-4076(88)90063-2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Discrete Choice/Endogeneity/Rivers, Vuong (1988) Limited information estimators and exogeneity tests for simultaneous probit models.pdf:pdf},
	journal = {Journal of Econometrics},
	mendeley-groups = {Discrete Choice/Endogeneity},
	pages = {347--366},
	title = {{Limited Information Estimators and Exogeneity Tests For Simultaneous Probit Models}},
	volume = {39},
	year = {1988}
}
@article{Zhang2018,
	author = {Zhang, Yichong},
	doi = {10.1214/17-AOS1673},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Conditional/Zhang (2018) Extremal quantile treatment effects..pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Quantile/Extreme Value/Regression,Treatment Effects},
	number = {6},
	pages = {3707--3740},
	title = {{Extremal Quantile Treatment Effects}},
	volume = {46},
	year = {2018}
}

@article{Heckman1997MakingMostOut,
  title = {Making the {{Most}} out of {{Programme Evaluations}} and {{Social Experiments}}: {{Accounting}} for {{Heterogeneity}} in {{Programme Impacts}}},
  author = {Heckman, James J. and Smith, Jeffrey and Clements, Nancy},
  year = {1997},
  journal = {Review of Economic Studies},
  volume = {64},
  number = {4},
  pages = {487--535},
  issn = {00346527},
  doi = {10.2307/2971729},
  abstract = {The conventional approach to social programme evaluation focuses on estimating mean impacts of programmes. Yet many interesting questions regarding the political economy of programmes, the distribution of programme benefits and the option values conferred on programme participants require knowledge of the distribution of impacts, or features of it. This paper presents evidence that heterogeneity in response to programmes is empirically important and that classical probability inequalities are not very informative in producing estimates or bounds on the distribution of programme impacts. We explore two methods for supplementing the information in these inequalities based on assumptions about participant decision-making processes and about the strength in dependence between outcomes in the participation and non-participation states. Dependence is produced as a consequence of rational choice by participants. We test for stochastic rationality among programme participants and present and implement methods for estimating the option values of social programmes.},
  file = {D:\Academic Things\Econometrics\Articles\Treatment Effects\Heckman, Smith, Clements (1997) Making the most out of programme evaluations and social.pdf}
}

}

@article{Makarov1981,
	author = {Makarov, G.D.},
	doi = {10.1137/1126086},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Inequalities/Makarov (1982) Estimates for DF Of a Sum with Fixed Marginal Distributions.pdf:pdf},
	journal = {Theory of Probability Amd Its Applications},
	mendeley-groups = {Random Useful Math/Probability},
	number = {4},
	pages = {803--806},
	title = {{Estimates for the Distribution Function of a Sum of Two Random Variables When the Marginal Distributions are Fixed}},
	volume = {26},
	year = {1981}
}

@article{Fan2010,
	abstract = {In this paper, we propose nonparametric estimators of sharp bounds on the distribution of treatment effects of a binary treatment and establish their asymptotic distributions. We note the possible failure of the standard bootstrap with the same sample size and apply the fewer-than-n bootstrap to making inferences on these bounds. The finite sample performances of the confidence intervals for the bounds based on normal critical values, the standard bootstrap, and the fewer-than-n bootstrap are investigated via a simulation study. Finally we establish sharp bounds on the treatment effect distribution when covariates are available. {\textcopyright} 2009 Cambridge University Press.},
	author = {Fan, Yanqin and Park, Sang Soo},
	doi = {10.1017/S0266466609990168},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Fan, Park (2010) SHARP BOUNDS ON THE DISTRIBUTION OF TREATMENT EFFECTS AND THEIR INference.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {Treatment Effects},
	number = {3},
	pages = {931--951},
	title = {{Sharp bounds on the distribution of treatment effects and their statistical inference}},
	volume = {26},
	year = {2010}
}

@article{Hosking1987,
	author = {Hosking, J. R. M. and Wallis, J. R.},
	doi = {10.2307/1269343},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Hosking, Wallis (1987) Parameter and Quantile Estimation for the Generalized Pareto Distribution.pdf:pdf},
	journal = {Technometrics},
	keywords = {maximum likelihood,method of moments,probability-weighted moments},
	mendeley-groups = {Quantile/Extremal},
	number = {3},
	pages = {339--349},
	title = {{Parameter and Quantile Estimation for the Generalized Pareto Distribution}},
	volume = {29},
	year = {1987}
}

@article{Cai2013,
	abstract = {Applying extreme value statistics in meteorology and environmental science requires accurate estimators on extreme value indices that can be around zero. Without having prior knowledge on the sign of the extreme value indices, the probability weighted moment (PWM) estimator is a favorable candidate. As most other estimators on the extreme value index, the PWM estimator bears an asymptotic bias. In this paper, we develop a bias correction procedure for the PWM estimator. Moreover, we provide bias-corrected PWM estimators for high quantiles and, when the extreme value index is negative, the endpoint of a distribution. The choice of k, the number of high order statistics used for estimation, is crucial in applications. The asymptotically unbiased PWM estimators allows the choice of higher level k, which results in a lower asymptotic variance. Moreover, since the bias-corrected PWM estimators can be applied for a wider range of k compared to the original PWM estimator, one gets more flexibility in choosing k for finite sample applications. All advantages become apparent in simulations and an environmental application on estimating "once per 10,000 years" still water level at Hoek van Holland, The Netherlands. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
	author = {Cai, Juan Juan and de Haan, Laurens and Zhou, Chen},
	doi = {10.1007/s10687-012-0158-x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Cai, de Haan, Zhou (2013) Bias correction in extreme value statistics with index.pdf:pdf},
	issn = {13861999},
	journal = {Extremes},
	keywords = {Bias correction,Endpoint estimation,Extreme value index,High quantile estimation,The probability weighted moment estimator},
	mendeley-groups = {Quantile/Extremal},
	number = {2},
	pages = {173--201},
	title = {{Bias correction in extreme value statistics with index around zero}},
	volume = {16},
	year = {2013}
}
@article{FragaAlves2014,
	abstract = {A simple estimator for the finite right endpoint of a distribution function in the Gumbel max-domain of attraction is proposed. Large sample properties such as consistency and the asymptotic distribution are derived. A simulation study is presented.},
	author = {{Fraga Alves}, M. I. and Neves, Claudia},
	doi = {10.5705/ss.2013.183},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Fraga Alves, Neves (2014) Estimation of the finite right endpoint in the Gumbel Domain.pdf:pdf},
	issn = {10170405},
	journal = {Statistica Sinica},
	keywords = {Endpoint estimation,Extreme value theory,Statistical inference},
	mendeley-groups = {Quantile/Extremal},
	number = {4},
	pages = {1811--1835},
	title = {{Estimation of the finite right endpoint in the Gumbel domain}},
	volume = {24},
	year = {2014}
}

@book{DeHaan2006,
	author = {de Haan, Laurens and Ferreira, Ana},
	doi = {10.1007/0-387-34471-3},
	isbn = {978-0-387-23946-0},
	mendeley-groups = {Quantile/Extremal},
	publisher = {Springer},
	title = {{Extreme Value Theory}},
	year = {2006}
}

@book{Leadbetter1983,
	author = {Leadbetter, M.R. and Lindgren, Georg and Rootz{\'{e}}n, Holger},
	doi = {10.1007/978-1-4612-5449-2},
	isbn = {978-1-4612-5451-5},
	mendeley-groups = {Quantile/Extremal},
	publisher = {Springer Series in Statistics},
	title = {{Extremes and Related Properties of Random Sequences and Processes}},
	year = {1983}
}

@article{Gnedenko1943,
	author = {Gnedenko, Boris V.},
	doi = {10.2307/1968974},
	issn = {0003486X},
	journal = {Annals of Mathematics},
	mendeley-groups = {Quantile/Extremal},
	number = {3},
	pages = {423--453},
	publisher = {Annals of Mathematics},
	title = {{Sur La Distribution Limite Du Terme Maximum D'Une Serie Aleatoire}},
	volume = {44},
	year = {1943}
}

@book{Resnick1987,
	author = {Resnick, Sidney I.},
	doi = {10.1007/978-0-387-75953-1},
	isbn = {978-0-387-75952-4},
	mendeley-groups = {Random Useful Math},
	publisher = {Springer},
	title = {{Extreme Values, Regular Variation and Point Processes}},
	year = {1987}
}

@article{Chernozhukov2005,
	abstract = {Quantile regression is an important tool for estimation of conditional quantiles of a response Y given a vector of covariates X. It can be used to measure the effect of covariates not only in the center of a distribution, but also in the upper and lower tails. This paper develops a theory of quantile regression in the tails. Specifically, it obtains the large sample properties of extremal (extreme order and intermediate order) quantile regression estimators for the linear quantile regression model with the tails restricted to the domain of minimum attraction and closed under tail equivalence across regressor values. This modeling setup combines restrictions of extreme value theory with leading homoscedastic and heteroscedastic linear specifications of regression analysis. In large samples, extreme order regression quantiles converge weakly to arg min functionals of stochastic integrals of Poisson processes that depend on regressors, while intermediate regression quantiles and their functionals converge to normal vectors with variance matrices dependent on the tail parameters and the regressor design. {\textcopyright} Institute of Mathematical Statistics, 2005.},
	author = {Chernozhukov, Victor},
	doi = {10.1214/009053604000001165},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Chernozhukov (2005) Extremal Quantile Regression.pdf:pdf},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Conditional quantile estimation,Extreme value theory,Regression},
	mendeley-groups = {Quantile/Extremal},
	number = {2},
	pages = {806--839},
	title = {{Extremal Quantile Regression}},
	volume = {33},
	year = {2005}
}

@article{Hirano2020,
	abstract = {Statistical decision rules map data into actions. Point estimators, inference procedures, and forecasting methods can be viewed as statistical decision rules. However, other types of rules are possible, such as rules for assigning individuals to treatments based on covariates, and methods for designing auctions. We discuss heuristics for constructing statistical decision rules, and survey results that characterize the properties of various classes of decision rules. Particular attention is paid to developing large-sample approximations to the distributions and associated risk properties of statistical decision rules.},
	author = {Hirano, Keisuke and Porter, Jack R.},
	doi = {10.1016/bs.hoe.2020.09.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Hirano, Porter (2020) Evaluating Statistical Decision Procedures.pdf:pdf},
	isbn = {9780444636492},
	issn = {15734412},
	journal = {Handbook of Econometrics},
	keywords = {Limit experiments,Risk,Statistical decision theory,Treatment assignment rules},
	mendeley-groups = {Limit Theory/Efficiency},
	pages = {283--354},
	title = {{Asymptotic Analysis of Statistical Decision Rules in Econometrics}},
	volume = {7},
	year = {2020}
}

@article{Fama2014TwoPillarsAsset,
  title = {Two {{Pillars}} of {{Asset Pricing}}},
  author = {Fama, Eugene F.},
  year = {2014},
  month = jun,
  journal = {American Economic Review},
  volume = {104},
  number = {6},
  pages = {1467--1485},
  issn = {0002-8282},
  doi = {10.1257/aer.104.6.1467},
  urldate = {2025-05-07},
  abstract = {The Nobel Foundation asks that the Nobel lecture cover the work for which the Prize is awarded. The announcement of this year's Prize cites empirical work in asset pricing. I interpret this to include work on efficient capital markets and work on developing and testing asset pricing models---the two pillars, or perhaps more descriptive, the Siamese twins of asset pricing. I start with efficient markets and then move on to asset pricing models.},
  langid = {english}, 
}


@article{Chernozhukov2017,
	abstract = {In 1895, the Italian econometrician Vilfredo Pareto discovered that the power law describes well the tails of income and wealth data. This simple observation stimulated further applications of the power law to economic data, including Zipf (1949), Mandelbrot (1963), Fama (1965), Praetz (1972), Sen (1973), and Longin (1996), among many others. It also led to a theory to analyze the properties of the tails of the distributions, so-called extreme value (EV) theory, which was developed by Gnedenko (1943) and deHaan (1970). Jansen and de Vries (1991) applied this theory to analyze the tail properties of US financial returns and concluded that the 1987 market crash was not an outlier; rather, it was a rare event whose magnitude could have been predicted by prior data. This work stimulated numerous other studies that rigorously documented the tail properties of economic data (Embrechts et al., 1997).},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1612.06850v2},
	author = {Chernozhukov, Victor and Fern{\'{a}}ndez-Val, Iv{\'{a}}n and Kaji, Tetsuya},
	doi = {10.1201/9781315120256},
	eprint = {arXiv:1612.06850v2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Quantile/Extremal/Chernozhukov et al. (2016) Extremal Quantile Regression Overview.pdf:pdf},
	isbn = {9781498725293},
	journal = {Handbook of Quantile Regression},
	mendeley-groups = {Quantile/Extremal},
	number = {2005},
	pages = {333--362},
	title = {{Extremal Quantile Regression}},
	year = {2017}
}
 
@article{Pesaran2008,
	abstract = {This paper proposes a standardized version of Swamy's test of slope homogeneity for panel data models where the cross section dimension (N) could be large relative to the time series dimension (T). The proposed test, denoted by over($\Delta$, ̃), exploits the cross section dispersion of individual slopes weighted by their relative precision. In the case of models with strictly exogenous regressors, but with non-normally distributed errors, the test is shown to have a standard normal distribution as (N, T)over(→, j) ∞ such that sqrt(N) / T2 → 0. When the errors are normally distributed, a mean-variance bias adjusted version of the test is shown to be normally distributed irrespective of the relative expansion rates of N and T. The test is also applied to stationary dynamic models, and shown to be valid asymptotically so long as N / T → $\kappa$, as (N, T) over(→, j) ∞, where 0 ≤ $\kappa$ {\textless} ∞. Using Monte Carlo experiments, it is shown that the test has the correct size and satisfactory power in panels with strictly exogenous regressors for various combinations of N and T. Similar results are also obtained for dynamic panels, but only if the autoregressive coefficient is not too close to unity and so long as T ≥ N. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Pesaran, M. Hashem and Yamagata, Takashi},
	doi = {10.1016/j.jeconom.2007.05.010},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Pesaran, Yamagata (2008). Testing slope homogeneity in large panels.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Dispersion tests,Large panels,Monte Carlo results,Tests of slope homogeneity},
	mendeley-groups = {Panel Data/Heterogeneous Panels,Panel Data/Heterogeneous Panels/Testing},
	number = {1},
	pages = {50--93},
	title = {{Testing slope homogeneity in large panels}},
	volume = {142},
	year = {2008}
}

@article{Su2013,
	author = {Su, Liangjun and Chen, Qihui},
	doi = {10.1017/S0266466613000017},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Su, Chen (2012) Testing Homogeneity Interactive Effects.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Testing},
	number = {6},
	pages = {1079--1135},
	title = {{Testing Homogeneity in Panel Data Models With Interactive Fixed Effects}},
	volume = {29},
	year = {2013}
}
@article{Blomquist2013,
	abstract = {Pesaran and Yamagata (Pesaran, M.H., Yamagata, T., Testing slope homogeneity in large panels, Journal of Econometrics 142, 50-93, 2008) propose a test for slope homogeneity in large panels, which has become very popular in the literature. However, the test cannot deal with the practically relevant case of heteroskedastic and/serially correlated errors. The present note proposes a generalized test that accommodates both features. {\textcopyright} 2013 Elsevier B.V.},
	author = {Blomquist, Johan and Westerlund, Joakim},
	doi = {10.1016/j.econlet.2013.09.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Blomquist, Westerlund (2013). Testing slope homogeneity in large panels with serial correlation.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {C32,C33,Heteroskedasticity,Homogeneity,Panel data,Serial correlation},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Testing},
	number = {3},
	pages = {374--378},
	publisher = {Elsevier B.V.},
	title = {{Testing slope homogeneity in large panels with serial correlation}}, 
	volume = {121},
	year = {2013}
}
@article{Campello2019,
	abstract = {Standard econometric methods can overlook individual heterogeneity in empirical work, generating inconsistent parameter estimates in panel data models. We propose the use of methods that allow researchers to easily identify, quantify, and address estimation issues arising from individual slope heterogeneity. We first characterize the bias in the standard fixed effects estimator when the true econometric model allows for heterogeneous slope coefficients. We then introduce a new test to check whether the fixed effects estimation is subject to heterogeneity bias. The procedure tests the population moment conditions required for fixed effects to consistently estimate the relevant parameters in the model. We establish the limiting distribution of the test and show that it is very simple to implement in practice. Examining firm investment models to showcase our approach, we show that heterogeneity bias-robust methods identify cash flow as a more important driver of investment than previously reported. Our study demonstrates analytically, via simulations, and empirically the importance of carefully accounting for individual specific slope heterogeneity in drawing conclusions about economic behavior.},
	author = {Campello, Murillo and Galvao, Antonio F. and Juhl, Ted},
	doi = {10.1080/07350015.2017.1421545},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Campello, Galvao, Juhl (2018). Testing for Slope Heterogeneity Bias in Panel Data Models.pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Bias,Corporate investment,Fixed effects,Individual heterogeneity,Slope heterogeneity,Testing},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Testing},
	number = {4},
	pages = {749--760},
	publisher = {Taylor {\&} Francis},
	title = {{Testing for Slope Heterogeneity Bias in Panel Data Models}}, 
	volume = {37},
	year = {2019}
}


@article{Guggenberger2010,
	abstract = {This paper investigates the asymptotic size properties of a two-stage test in the linear instrumental variables model when in the first stage a Hausman (1978) specification test is used as a pretest of exogeneity of a regressor. In the second stage, a simple hypothesis about a component of the structural parameter vector is tested, using a t-statistic that is based on either the ordinary least squares (OLS) or the two-stage least squares estimator (2SLS), depending on the outcome of the Hausman pretest. The asymptotic size of the two-stage test is derived in a model where weak instruments are ruled out by imposing a positive lower bound on the strength of the instruments. The asymptotic size equals 1 for empirically relevant choices of the parameter space. The size distortion is caused by a discontinuity of the asymptotic distribution of the test statistic in the correlation parameter between the structural and reduced form error terms. The Hausman pretest does not have sufficient power against correlations that are local to zero while the OLS-based t-statistic takes on large values for such nonzero correlations. Instead of using the two-stage procedure, the recommendation then is to use a t-statistic based on the 2SLS estimator or, if weak instruments are a concern, the conditional likelihood ratio test by Moreira (2003). {\textcopyright} Cambridge University Press, 2009.},
	author = {Guggenberger, Patrik},
	doi = {10.1017/S0266466609100026},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Guggenberger (2010) The Impact of a Hausman Pretest on the Asymptotic Size of a Hypothesis Test.pdf:pdf},
	issn = {02664666},
	journal = {Econometric Theory},
	mendeley-groups = {Limit Theory/Two Step Procedures},
	number = {2},
	pages = {369--382},
	title = {{The Impact of a Hausman Pretest On the Asymptotic Size of a Hypothesis Test}},
	volume = {26},
	year = {2010}
}

@incollection{Pesaran1996,
	abstract = {This chapter is concerned with the problem of heterogeneity across groups (individuals, firms, regions or countries) in dynamic panels.1 While it is widely recognised that parameter heterogeneity can have important consequences for estimation and inference, most attempts at dealing with it have focused on allowing for intercept variation, and in comparison little attention has been paid to the implications of variation in slopes. There are a number of justifications that can be advanced for this neglect.},
	author = {Pesaran, Hashem and Smith, Ron and Im, Kyung So},
	booktitle = {The Econometrics of Panel Data},
	doi = {10.1007/978-94-009-0137-7_8},
	editor = {Matyas, L. and Sevestre, P.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Pesaran, Smith, Im (1996) Dynamic Linear Models for Heterogenous Panels..pdf:pdf},
	mendeley-groups = {Panel Data/Heterogeneous Panels/Testing},
	pages = {145--195},
	title = {{Dynamic Linear Models for Heterogenous Panels}},
	year = {1996}
}

@book{Politis1999,
	author = {Politis, Dimitris N. and Romano, Joseph P. and Wolf, Michael},
	doi = {10.1007/978-1-4612-1554-7},
	isbn = {978-0-387-98854-2},
	mendeley-groups = {Bootstrap},
	publisher = {Springer Series in Statistics},
	title = {{Subsampling}},
	year = {1999}
}

@article{Wang2016,
	abstract = {In this paper we follow Hansen (2015a) and propose a Stein-like estimator for linear panel data models. Our estimator takes a weighted average of the fixed effects estimator and the random effects estimator using the weights constructed from Hausman's (1978) testing statistic. We establish the asymptotic distribution of the Stein-like estimator and show its asymptotic risk being strictly smaller than the fixed effects estimator within a local asymptotic framework.},
	author = {Wang, Yun and Zhang, Yonghui and Zhou, Qiankun},
	doi = {10.1016/j.econlet.2016.02.016},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Wang et al (2015) A Stein-like estimator for linear panel data models.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	keywords = {Asymptotic risk,Fixed effects,Hausman pretest,Panel data,Random effects,Stein estimator},
	mendeley-groups = {Model Selection and Averaging/Panel,Model Selection and Averaging/Shrinkage Averages},
	pages = {156--161},
	publisher = {Elsevier B.V.},
	title = {{A Stein-Like Estimator for Linear Panel Data Models}}, 
	volume = {141},
	year = {2016}
}
@article{Hansen2017,
	abstract = {Maasoumi (1978) proposed a Stein-like estimator for simultaneous equations and showed that his Stein shrinkage estimator has bounded finite sample risk, unlike the three-stage least square estimator. We revisit his proposal by investigating Stein-like shrinkage in the context of two-stage least square (2SLS) estimation of a structural parameter. Our estimator follows Maasoumi (1978) in taking a weighted average of the 2SLS and ordinary least square estimators, with the weight depending inversely on the Hausman (1978) statistic for exogeneity. Using a local-to-exogenous asymptotic theory, we derive the asymptotic distribution of the Stein estimator and calculate its asymptotic risk. We find that if the number of endogenous variables exceeds 2, then the shrinkage estimator has strictly smaller risk than the 2SLS estimator, extending the classic result of James and Stein (1961). In a simple simulation experiment, we show that the shrinkage estimator has substantially reduced finite sample median squared error relative to the standard 2SLS estimator.},
	author = {Hansen, Bruce E.},
	doi = {10.1080/07474938.2017.1307579},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Hansen (2015) Stein-like 2SLS estimator.pdf:pdf},
	issn = {15324168},
	journal = {Econometric Reviews},
	keywords = {2SLS,Endogeneous,IV,OLS,Stein,shrinkage},
	mendeley-groups = {Model Selection and Averaging,Model Selection and Averaging/Shrinkage Averages},
	number = {6-9},
	pages = {840--852},
	publisher = {2017},
	title = {{Stein-like 2SLS estimator}}, 
	volume = {36},
	year = {2017}
}

@article{Breusch1989,
	author = {Breusch, Trevor S. and Mizon, Grayham E. and Schmidt, Peter},
	doi = {10.2307/1911060},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-Invarianct And Slow Moving Variables/Breusch, Mizon, Schmidt (1989) Efficient Estimtino Using Panel Daata.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/IV Estimation},
	number = {3},
	pages = {695--700},
	title = {{Efficient Estimation Using Panel Data}},
	volume = {57},
	year = {1989}
}

@article{Amemiya1986,
	author = {Amemiya, Takeshi and MaCurdy, Thomas E.},
	doi = {10.2307/1912840},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-Invarianct And Slow Moving Variables/Amemiya, MaCurdy (1987) Instrumental-Variable Estimation of an Error-Components Model.pdf:pdf},
	journal = {Econometrica},
	keywords = {b,components model given by,consider procedures for estimating,e1,error components,in this paper we,instrumental variables,parameters of an error-,simultaneous equations,x1f3,y1,y18,zy},
	mendeley-groups = {Panel Data/Time-Invariant And Slow Moving},
	number = {4},
	pages = {869--880},
	title = {{Instrumental-Variable Estimation of an Error-Components Model}},
	volume = {54},
	year = {1986}
}

@article{Hausman1981,
	author = {Hausman, Jerry A. and Taylor, William E.},
	doi = {10.2307/1911406},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-Invarianct And Slow Moving Variables/Hausman, Taylor (1980) Panel Data and Unobservable Individual Effects.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Time-Invariant And Slow Moving},
	number = {6},
	pages = {1377--1398},
	title = {{Panel Data and Unobservable Individual Effects}},
	volume = {49},
	year = {1981}
}
@article{Bickel1982,
	author = {Bickel, Peter J.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Bickel (1982) On Adaptive Estimation.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Semiparametric Estimation},
	number = {3},
	pages = {647--671},
	title = {{On Adaptive Estimation}},
	volume = {10},
	year = {1982}
}

@article{White1980,
	author = {White, Halbert},
	doi = {10.2307/2526245},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/White (1980) Using Least Squares to Approximate Unknown Regression Functions.pdf:pdf},
	journal = {International Economic Review},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	number = {1},
	pages = {149--170},
	title = {{Using Least Squares to Approximate Unknown Regression Functions}},
	volume = {21},
	year = {1980}
}

@article{Andrews1994b,
	abstract = {The effects of firm pension plan provisions on the retirement decisions of older employees are analyzed. The empirical results are based on data from a large firm, with a typical defined benefit pension plan. The "option value" of continued work is the central feature of the analysis. Estimation relies on a retirement decision rule that is close in spirit to the dynamic programming rule but is considerably less complex than a compre- hensive implementation of that rule, thus greatly facilitating the numerical analysis},
	author = {Andrews, Donald W.K.},
	doi = {10.2307/2951475},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Andrews (1994) Asymptotics for Semiparametric Econometric Models Via Stochastic Equicontinuity.pdf:pdf},
	issn = {00130133},
	journal = {Econometrica},
	mendeley-groups = {Semiparametric Estimation},
	number = {1},
	pages = {43--72},
	title = {{Asymptotics for Semiparametric Econometric Models Via Stochastic Equicontinuity}},
	volume = {62},
	year = {1994}
}

@article{Galvao2014,
	abstract = {This article considers fixed effects (FE) estimation for linear panel data models under possible model misspecification when both the number of individuals, n, and the number of time periods, T, are large. We first clarify the probability limit of the FE estimator and argue that this probability limit can be regarded as a pseudo-true parameter. We then establish the asymptotic distributional properties of the FE estimator around the pseudo-true parameter when n and T jointly go to infinity. Notably, we show that the FE estimator suffers from the incidental parameters bias of which the top order is O(T− 1), and even after the incidental parameters bias is completely removed, the rate of convergence of the FE estimator depends on the degree of model misspecification and is either (nT)− 1/2 or n− 1/2. Second, we establish asymptotically valid inference on the (pseudo-true) parameter. Specifically, we derive the asymptotic properties of the clustered covariance matrix (CCM) estimator and the cross-section bootstrap, and show that they are robust to model misspecification. This establishes a rigorous theoretical ground for the use of the CCM estimator and the cross-section bootstrap when model misspecification and the incidental parameters bias (in the coefficient estimate) are present. We conduct Monte Carlo simulations to evaluate the finite sample performance of the estimators and inference methods, together with a simple application to the unemployment dynamics in the U.S.},
	author = {Galvao, Antonio F. and Kato, Kengo},
	doi = {10.1080/07350015.2013.875473},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Large Panels/Galvao, Kato (2014) Estimation and Inference for Linear Panel Data Models Under Misspecification When Both n and T are Large..pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Cross-section bootstrap,Fixed effects estimator,Incidental parameters problem},
	mendeley-groups = {Panel Data/Large Panels},
	number = {2},
	pages = {285--309},
	title = {{Estimation and Inference for Linear Panel Data Models Under Misspecification When Both n and T are Large}},
	volume = {32},
	year = {2014}
}

@article{Anderson1949,
	abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
	author = {Anderson, T. W. and Rubin, Herman},
	doi = {10.1214/aoms/1177730090},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Weak IV/Anderson, Rubin (1949) Estimation of the Parameters of a Single Equation in a Complete System of Stochastic Equations.pdf:pdf},
	issn = {0003-4851},
	journal = {The Annals of Mathematical Statistics},
	mendeley-groups = {Small Sample Results},
	number = {1},
	pages = {46--63},
	title = {{Estimation of the Parameters of a Single Equation in a Complete System of Stochastic Equations}},
	volume = {20},
	year = {1949}
}
@article{Halcrow1949,
	author = {Halcrow, Harold G},
	doi = {10.2307/1232330},
	issn = {10711031},
	journal = {Journal of Farm Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {418--443},
	publisher = {[Oxford University Press, Agricultural {\&} Applied Economics Association]},
	title = {{Actuarial Structures for Crop Insurance}}, 
	volume = {31},
	year = {1949}
}
@article{Botts1958,
	author = {Botts, Ralph R and Boles, James N},
	doi = {10.2307/1235383},
	issn = {10711031},
	journal = {Journal of Farm Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {733--740},
	publisher = {[Oxford University Press, Agricultural {\&} Applied Economics Association]},
	title = {{Use of Normal-Curve Theory in Crop Insurance Ratemaking}}, 
	volume = {40},
	year = {1958}
}
@article{Borch1960,
	annote = {doi: 10.1080/03461238.1960.10410587},
	author = {Borch, Karl},
	doi = {10.1080/03461238.1960.10410587},
	issn = {0346-1238},
	journal = {Scandinavian Actuarial Journal},
	mendeley-groups = {My Research/Theta/Review},
	month = {jul},
	number = {3-4},
	pages = {163--184},
	publisher = {Taylor {\&} Francis},
	title = {{The safety loading of reinsurance premiums}}, 
	volume = {1960},
	year = {1960}
}
@article{Johnston1961,
	author = {Johnston, By Bruce F and Mellor, John W},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnston, Mellor - 1961 - The Role of Agriculture in Economic Development.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {My Teaching/Agro},
	number = {4},
	pages = {566--593},
	title = {{The Role of Agriculture in Economic Development}},
	volume = {51},
	year = {1961}
}
@article{Arrow1961,
	author = {Arrow, K. J. and Chenery, H. B. and Minhas, B. S. and Solow, R. M.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arrow et al. - 1961 - Capital-Labor Substitution and Economic Efficiency.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	mendeley-groups = {My Research/Alpha},
	number = {3},
	pages = {225--250},
	title = {{Capital-Labor Substitution and Economic Efficiency}},
	volume = {43},
	year = {1961}
}
@article{Zellner1962,
	author = {Zellner, Arnold and Theil, H},
	doi = {10.2307/1911287},
	issn = {00129682, 14680262},
	journal = {Econometrica},
	mendeley-groups = {My Research,My Research/Prima},
	number = {1},
	pages = {54--78},
	publisher = {[Wiley, Econometric Society]},
	title = {{Three-Stage Least Squares: Simultaneous Estimation of Simultaneous Equations}}, 
	volume = {30},
	year = {1962}
}
@article{Zellner1962,
	abstract = {Abstract In this paper a method of estimating the parameters of a set of regression equations is reported which involves application of Aitken's generalized least-squares [1] to the whole system of equations. Under conditions generally encountered in practice, it is found that the regression coefficient estimators so obtained are at least asymptotically more efficient than those obtained by an equation-by-equation application of least squares. This gain in efficiency can be quite large if ?independent? variables in different equations are not highly correlated and if disturbance terms in different equations are highly correlated. Further, tests of the hypothesis that all regression equation coefficient vectors are equal, based on ?micro? and ?macro? data, are described. If this hypothesis is accepted, there will be no aggregation bias. Finally, the estimation procedure and the ?micro-test? for aggregation bias are applied in the analysis of annual investment data, 1935?1954, for two firms.},
	annote = {doi: 10.1080/01621459.1962.10480664},
	author = {Zellner, Arnold},
	doi = {10.1080/01621459.1962.10480664},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {My Research/Prima},
	month = {jun},
	number = {298},
	pages = {348--368},
	publisher = {Taylor {\&} Francis},
	title = {{An Efficient Method of Estimating Seemingly Unrelated Regressions and Tests for Aggregation Bias}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1962.10480664},
	volume = {57},
	year = {1962}
}
@article{Inada1963,
	author = {Inada, Ken-Ichi},
	doi = {10.2307/2295809},
	issn = {00346527, 1467937X},
	journal = {The Review of Economic Studies},
	mendeley-groups = {My Research/Theta},
	number = {2},
	pages = {119--127},
	publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
	title = {{On a Two-Sector Model of Economic Growth: Comments and a Generalization}},
	url = {http://www.jstor.org/stable/2295809},
	volume = {30},
	year = {1963}
}
@article{Lanczos1964,
	author = {Lanczos, C},
	issn = {0887459X},
	journal = {Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis},
	mendeley-groups = {My Research/Theta},
	pages = {86--96},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {{A Precision Approximation of the Gamma Function}},
	url = {http://www.jstor.org/stable/2949767},
	volume = {1},
	year = {1964}
}
@book{Olson1965,
	address = {Cambridge},
	author = {Olson, Mancur},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olson - 1965 - The Logic of Collective Action.pdf:pdf},
	isbn = {0674537513},
	pages = {186},
	publisher = {Harvard Economic Studies},
	title = {{The Logic of Collective Action}},
	year = {1965}
}
@article{Rander1966,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	author = {Rander, Roy},
	doi = {10.2307/2525366},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rander - 1966 - Optimal Growth in a Linear-Logarithmic Economy.pdf:pdf},
	journal = {International Economic Review},
	keywords = {icle},
	number = {1},
	pages = {1--33},
	title = {{Optimal Growth in a Linear-Logarithmic Economy}},
	volume = {7},
	year = {1966}
}
@article{Cox1968,
	abstract = {Residuals are usually defined in connection with linear models. Here a more general definition is given and some asymptotic properties found. Some illustrative examples are discussed, including a regression problem involving exponentially distributed errors and some problems concerning Poisson and binomially distributed observations.},
	author = {Cox, David R. and Snell, E. Joyce},
	doi = {10.1111/j.2517-6161.1968.tb00724.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/MLE/Cox, Snell (1968) A General Definition of Residuals.pdf:pdf},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	mendeley-groups = {Macrofinance/Maximum Likelihood},
	number = {2},
	pages = {248--265},
	title = {{A General Definition of Residuals}},
	volume = {30},
	year = {1968}
}
@book{Billingsley1968,
	author = {Billingsley, Patrick},
	isbn = {9781118625965},
	mendeley-groups = {Random Useful Math/Probability},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Statistics},
	title = {{Convergence of Probability Measures}},
	url = {https://books.google.es/books?id=6ItqtwaWZZQC},
	year = {1968}
}
@article{Pollak1968,
	author = {Pollak, Robert A and Phelps, Edmund S.},
	doi = {10.2307/2296547},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollak, Phelps - 1968 - On Second-Best National Saving and Growth ' Game-Equilibrium.pdf:pdf},
	journal = {The Review of Economic Studies},
	number = {2},
	pages = {185--199},
	title = {{On Second-Best National Saving and Growth ' Game-Equilibrium}},
	url = {http://www.jstor.org/stable/2296547},
	volume = {35},
	year = {1968}
}
@article{Bates1969,
	author = {Bates, J.M. and Granger, Clive},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Bates, Granger (1969) The Combination of Forecasts.pdf:pdf},
	journal = {Journal of the Operational Research Society},
	mendeley-groups = {Forecasting/Forecast Combination},
	number = {4},
	pages = {451--468},
	title = {{The Combination of Forecasts}},
	volume = {20},
	year = {1969}
}
@article{Sawa1969,
	author = {Sawa, Takamitsu},
	doi = {10.2307/2283473},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Sawa (1969) The Exact Sampling Distribution of Ordinary Least Squares and Two-Stage Least Squares Estimators.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	mendeley-groups = {Small Sample Results,Small Sample Results/Linear},
	number = {327},
	pages = {923--937},
	title = {{The Exact Sampling Distribution of Ordinary Least Squares and Two-Stage Least Squares Estimators}},
	volume = {64},
	year = {1969}
}
@article{Jennrich1969,
	author = {Jennrich, Robert I.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Jennrich (1969) Asymptotic Properties of NLLS.pdf:pdf},
	issn = {0091-1798},
	journal = {The Annals of Mathematical Statistics},
	mendeley-groups = {Limit Theory},
	number = {2},
	pages = {633--643},
	title = {{Asymptotic Properties of Nonlinear Least Squares Estimators}},
	url = {http://projecteuclid.org/euclid.aop/1176996548},
	volume = {40},
	year = {1969}
}
@article{Durbin1970,
	abstract = {The construction of tests of model specification is considered from a general point of view. The results are applied to testing the serial independence of the disturbances in a regression model where some of the regressors are lagged dependent variables. It is shown that the asymptotic distribution of the lag-1 serial correlation coefficient calculated from the least-squares residuals differs from that of the coefficient calculated from the true disturbances. A consequence of this is that tests of serial independence based on the residuals from regression on fixed regressors are invalid when applied to models containing lagged dependent variables even when the null hypothesis of serial independence is true. Tests which are asymptotically valid for the large-sample case are suggested.},
	author = {Durbin, James},
	doi = {10.2307/1909547},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/Serial Correlation/Durbin (1970) Testing for Serial Correlation in Least-Squares Regression When Some of the Regressors are Lagged Dependent Variables.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Time Series/Serial Correlation},
	number = {3},
	pages = {410},
	title = {{Testing for Serial Correlation in Least-Squares Regression When Some of the Regressors are Lagged Dependent Variables}},
	volume = {38},
	year = {1970}
}
@article{Swamy1970,
	author = {Swamy, P.A.V.B.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Swamy (1970) Efficient Inference in a Random Coefficient Regression Model.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {311--323},
	title = {{Efficient Inference in a Random Coefficient Regression Model}},
	volume = {38},
	year = {1970}
}
@article{Mirrlees2014,
	author = {Mirrlees, James A.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirrlees - 1971 - An Exploration in the Theory of Optimum Income Taxation.pdf:pdf},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Fiscal Policy},
	number = {2},
	pages = {175--208},
	title = {{An Exploration in the Theory of Optimum Income Taxation}},
	year = {1971}
}
@article{Allingham1972,
	author = {Allingham, Michael G and Sandmo, Agnar},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Allingham, Sandmo - 1972 - Income Tax Evasion A Theoretical Analisys.pdf:pdf},
	journal = {Journal of Public Economics},
	pages = {323--338},
	title = {{Income Tax Evasion: A Theoretical Analisys}},
	volume = {1},
	year = {1972}
}
@article{Cox1972,
	author = {Cox, D.R.},
	doi = {https://www.jstor.org/stable/2985181},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cox - 1972 - Regression Models and Life-Tables.pdf:pdf},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {although possibly,asymptotically efficient estimators,em algorithm,incomplete data,maximum likelihood estimation,others are developed which,recursive estimation,stochastic approximation},
	number = {2},
	pages = {187--220},
	title = {{Regression Models and Life-Tables}},
	volume = {24},
	year = {1972}
}
 
@article{Foutz1977,
	author = {Foutz, Robert V and Srivastava, R C},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Foutz, Srivastava - 1977 - The Asymptotic Distribution of the Likelihood Ratio When the Model Is Incorrect.pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {ams 1970 subject classifications,and phrases,model},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {2},
	pages = {273--279},
	title = {{The Asymptotic Distribution of the Likelihood Ratio When the Model Is Incorrect}},
	volume = {6},
	year = {1977}
}
  
@article{Bhagwat1978,
	author = {Bhagwat, K. V. and Subramanian, R.},
	doi = {10.1017/S0305004100054670},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Functional Analysis and Operator Theory/Bhagwat, Subramanian, (1978) Inequalities between means of positive operators.pdf:pdf},
	issn = {14698064},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	mendeley-groups = {Random Useful Math/Operator Theory and Functional Analysis},
	number = {3},
	pages = {393--401},
	title = {{Inequalities between means of positive operators}},
	volume = {83},
	year = {1978}
}
@article{Holmstrom1979,
	abstract = {tra i fondatori della teoria dell'agenzia Bengt H. in Foss fotocopiato tutto},
	author = {H{\"{o}}lmstrom, Bengt},
	doi = {10.2307/3003320},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{o}}lmstrom - 1979 - Moral Hazard and Observability.pdf:pdf},
	isbn = {0361915X},
	issn = {0361-915X},
	journal = {Bell Journal of Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {1},
	pages = {74--91},
	pmid = {19928367},
	title = {{Moral Hazard and Observability}},
	volume = {10},
	year = {1979}
}
@article{Lancaster1979,
	author = {Lancaster, Tony},
	doi = {https://www.jstor.org/stable/2985181},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lancaster - 1979 - Econometric Methods for the Duration of Unemployment.pdf:pdf},
	journal = {Econometrica},
	number = {4},
	pages = {939--956},
	title = {{Econometric Methods for the Duration of Unemployment}},
	volume = {47},
	year = {1979}
}
@article{Harris1979,
	author = {Harris, Milton and Raviv, Artur},
	doi = {http://dx.doi.org/10.1016/0022-0531(79)90073-5},
	issn = {0022-0531},
	journal = {Journal of Economic Theory},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {231--259},
	title = {{Optimal Incentive Contracts with Imperfect Information}},
	url = {http://www.sciencedirect.com/science/article/pii/0022053179900735},
	volume = {20},
	year = {1979}
}
@book{Berry1979,
	author = {Berry, R A and Cline, W R},
	isbn = {9780801821905},
	mendeley-groups = {My Teaching/Agro},
	publisher = {John Hopkins University Press},
	title = {{Agrarian Structure and Productivity in Developing Countries: A Study Prepared for the International Labour Office Within the Framework of the World Employment Programme}},
	url = {https://books.google.ru/books?id=i{\_}4DAAAAMAAJ},
	year = {1979}
}
@article{Quiggin1979,
	author = {Quiggin, John C. and Anderson, Jock R.},
	doi = {10.1111/j.1467-8489.1979.tb00243.x},
	issn = {00049395},
	journal = {Australian Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta},
	month = {dec},
	number = {3},
	pages = {191--206},
	title = {{Stabilisation and Risk Reduction in Australian Agriculture}},
	url = {http://doi.wiley.com/10.1111/j.1467-8489.1979.tb00243.x},
	volume = {23},
	year = {1979}
}
@article{Deaton1980,
	abstract = {Ever since Richard Stone first estimated a system of demand equations derived explicitly from consumer theory, there has been a continuing search for alternative specifications and functional forms. Many models have been proposed, but perhaps the most important in current use, apart from the original linear expenditure system, are the Rotterdam model and the translog model. Both of these models have been extensively estimated and have, in addition, been used to test the homogeneity and symmetry restrictions of demand theory. In this paper, we propose and estimate a new model which is of comparable generality to the Rotterdam and translog models but which has considerable advantages over both. The model, which we call the Almost Ideal Demand System (AIDS), gives an arbitrary first-order approximation to any demand system; it satisfies the axioms of choice exactly; it aggregates perfectly over consumers without invoking parallel linear Engel curves; it has a functional form which is consistent with known household-budget data; it is simple to estimate, largely avoiding the need for non-linear estimation; and it can be used to test the restrictions of homogeneity and symmetry through linear restrictions on fixed parameters.},
	author = {Deaton, Angus and Muellbauer, John},
	doi = {10.1016/0014-2921(94)90008-6},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deaton, Muellbauer - 1980 - An almost ideal demand system.pdf:pdf},
	isbn = {0002-8282},
	issn = {0002-8282},
	journal = {American Economic Review},
	mendeley-groups = {My Research/Alpha},
	number = {3},
	pages = {312--326},
	title = {{An almost ideal demand system}},
	url = {http://www.jstor.org/stable/1805222},
	volume = {70},
	year = {1980}
}
@article{Haff1980,
	author = {Haff, L.R.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haff - 1980 - Empirical Bayes Estimation of the Multivariate Normal Covariance Matrix.pdf:pdf},
	journal = {Annals of Statistics},
	mendeley-groups = {UPF/ADTSI},
	number = {3},
	pages = {586--597},
	title = {{Empirical Bayes Estimation of the Multivariate Normal Covariance Matrix}},
	volume = {8},
	year = {1980}
}
@article{Marwell1981,
	abstract = {Eleven closely related experiments testing the free rider hypothesis under different conditions, and sampling various subpopulations, are reported. Results question the empirical validity and generality of a strong version of the hypothesis. Some reasons for its failure are discussed. ?? 1981.},
	author = {Marwell, Gerald and Ames, Ruth E.},
	doi = {10.1016/0047-2727(81)90013-X},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marwell, Ames - 1981 - Economists Free Ride, Does Anyone Else Experiments on the Provision of Public Goods.pdf:pdf},
	isbn = {0047-2727},
	issn = {00472727},
	journal = {Journal of Public Economics},
	number = {3},
	pages = {295--310},
	title = {{Economists Free Ride, Does Anyone Else? Experiments on the Provision of Public Goods}},
	volume = {15},
	year = {1981}
}
@article{Anderson1981,
	author = {Anderson, T. W. and Hsiao, Cheng},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Anderson, Hsiao (1981) Estimation of Dynamic Models with Error Components.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	keywords = {b,cross-section time series,dynamic models,error components,initial conditions,the asymptotic properties of,the estimators under var-},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {375},
	pages = {598--606},
	title = {{Estimation of Dynamic Models with Error Components}},
	volume = {76},
	year = {1981}
}
@article{Nickell1981,
	author = {Nickell, Stephen},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Nickell (1981) Biases With Fixed Effects.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {6},
	pages = {1417--1426},
	title = {{Biases In Dynamic Models With Fixed Effects}},
	volume = {49},
	year = {1981}
}
@article{Anderson1982,
	abstract = {This paper presents a statistical analysis of time series regression models for longitudinal data with and without lagged dependent variables under a variety of assumptions about the initial conditions of the processes being analyzed. The analysis demonstrates how the asymptotic properties of estimators of longitudinal models are critically dependent on the manner in which samples become large: by expanding the number of observations per person, holding the number of people fixed, or by expanding the number of persons, holding the number of observations per person fixed. The paper demonstrates which parameters can and cannot be identified from data produced by different sampling plans. {\textcopyright} 1982.},
	author = {Anderson, T. W. and Hsiao, Cheng},
	doi = {10.1016/0304-4076(82)90095-1},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Anderson, Hsiao (1982) Formulation and estimation of dynamic models using panel data.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {47--82},
	title = {{Formulation and estimation of dynamic models using panel data}},
	volume = {18},
	year = {1982}
}
@article{Ahsan1982,
	author = {Ahsan, Syed M. and Ali, Ali A. G. and Kurian, N. John},
	doi = {10.2307/1240644},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahsan, Ali, Kurian - 1982 - Toward a Theory of Agricultural Insurance.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {a major role played,affected by natural,by insurance programs is,competitive markets,crop insurance,individuals,public provision of insurance,the indemnification of risk-averse,who might be adversely},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {520},
	title = {{Toward a Theory of Agricultural Insurance}},
	url = {http://www.jstor.org/stable/1240644},
	volume = {64},
	year = {1982}
}
@article{Kent1982,
	abstract = {The usual asymptotic chi-squared distribution for the likelihood ratio test statistic is based on the assumptions that the data come from the parametric model under consideration and that the parameter satisfies the null hypothesis. In this paper we examine the distribution of the likelihood ratio statistic when the data do not come from the parametric model, but when the `nearest' member of the parametric family still satisfies the null hypothesis. In general, the likelihood ratio statistic no longer follows an asymptotic chi-squared distribution, and an alternative statistic based on the union-intersection approach is proposed.},
	author = {Kent, John T.},
	doi = {10.1093/biomet/69.1.19},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kent - 1982 - Robust Properties of Likelihood Ratio Tests.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Information,Likelihood ratio test,Robustness,Union-intersection test},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {1},
	pages = {19--27},
	title = {{Robust Properties of Likelihood Ratio Tests}},
	volume = {69},
	year = {1982}
}
@article{Holmstrom1982,
	abstract = {This article studies moral hazard with many agents. The focus is on two features that are novel in a multiagent setting: free riding and competition. The free-rider problem implies a new role for the principal: administering incentive schemes that do not balance the budget. This new role is essential for controlling incentives and suggests that firms in which ownership and labor are partly separated will have an advantage over partnerships in which output is distributed among agents. A new characterization of informative (hence valuable) monitoring is derived and applied to analyze the value of relative performance evaluation. It is shown that competition among agents (due to relative evaluations) has merit solely as a device to extract information optimally. Competition per se is worthless. The role of aggregate measures in relative performance evaluation is also explored, and the implications for investment rules are discussed.},
	author = {Holmstrom, Bengt},
	doi = {10.2307/3003457},
	issn = {0361915X},
	journal = {The Bell Journal of Economics},
	number = {2},
	pages = {324--340},
	publisher = {[RAND Corporation, Wiley]},
	title = {{Moral Hazard in Teams}},
	url = {http://www.jstor.org/stable/3003457},
	volume = {13},
	year = {1982}
}
@article{Carroll1982,
	abstract = {A general weak convergence theory is developed for time-sequential censored rank statistics in the two-sample problem of comparing time to failure between two treatment groups, such as in the case of a clinical trial in which patients enter serially and, after being randomly allocated to one of two treatments, are followed until they fail or withdraw from the study or until the study is terminated. Applications of the theory to time-sequential tests based on these censored rank statistics are also discussed.},
	author = {Carroll, Raymond J.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Carroll (1982) Adapting for Heteroskedasticity in Linear Models.pdf:pdf},
	journal = {The Annals of Statistics},
	mendeley-groups = {Semiparametric Estimation},
	number = {4},
	pages = {1224--1233},
	title = {{Adapting for Heteroscedasticity in Linear Models}},
	volume = {10},
	year = {1982}
}
@article{Kydland1982,
	abstract = {The equilibrium growth model is modified and used to explain the cyclical variances of a set of economic time series, the covariances between real output and the other series, and the autocovariance of output. The model is fitted to quarterly data for the post-war U.S. economy. Crucial features of the model are the assumption that more than one time period is required for the construction of new productive capital, and the non-time-separable utility function that admits greater intertemporal substitution of leisure. The fit is surprisingly good in light of the model's simplicity and the small number of free parameters.},
	author = {Kydland, Finn E and Prescott, Edward C},
	doi = {10.2307/1913386},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kydland, Prescott - 1982 - Time-to-Build and Aggregate Fluctuations.pdf:pdf},
	journal = {Econometrica},
	number = {6},
	pages = {1345--1370},
	title = {{Time-to-Build and Aggregate Fluctuations}},
	volume = {50},
	year = {1982}
}
@misc{Kerr1983,
	abstract = {Three experiments tested the hypothesis that group members exert less effort as the perceived dispensability of their efforts for group success increases. The resultant motivation losses were termed "free-rider effects." In Exp I, 189 undergraduates of high or low ability performed in 2-, 4-, or 8-person groups at tasks with additive, conjunctive, or disjunctive demands. As predicted, member ability had opposite effects on effort under disjunctive and conjunctive task demands. The failure to obtain a relationship between group size and member effort in Exp I was attributed to a procedural artifact eliminated in Exp II (73 Ss). As predicted, as groups performing conjunctive and disjunctive tasks increased in size, member motivation declined. This was not a social loafing effect; group members were fully identifiable at every group size. Exp III (108 Ss) explored the role that performance feedback plays in informing group members of the dispensability of their efforts and encouraging free riding. Results are generally consistent with those of Exps I and II. (24 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	address = {US},
	author = {Kerr, Norbert L and Bruun, Steven E},
	booktitle = {Journal of Personality and Social Psychology},
	doi = {10.1037/0022-3514.44.1.78},
	isbn = {1939-1315(Electronic);0022-3514(Print)},
	keywords = {*Feedback,*Group Dynamics,*Group Problem Solving,*Group Size,Motivation},
	number = {1},
	pages = {78--94},
	publisher = {American Psychological Association},
	title = {{Dispensability of member effort and group motivation losses: Free-rider effects.}},
	volume = {44},
	year = {1983}
}
@article{White1982,
	author = {White, Halbert},
	doi = {10.2307/1912004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/MLE/White (1982) Misspecified MLE.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Macrofinance/Maximum Likelihood},
	number = {1},
	pages = {1--25},
	title = {{Maximum Likelihood Estimation of Misspecified Models}},
	volume = {50},
	year = {1982}
}

@misc{Kerr1983,
	abstract = {A theory developed to account for behavior in social dilemmas—situations in which the rational pursuit of self-interest can lead to collective disaster—was applied to the analysis of group motivation losses. Two group motivation loss effects demonstrated in previous research, the social-loafing effect and the free-rider effect, are shown to follow from social dilemma theories. An experiment with 75 undergraduates was performed to empirically demonstrate a 3rd motivation loss effect, termed the "sucker" effect. It was hypothesized that group members would reduce their efforts if they had a capable partner who free-rode on their efforts, that is, who was capable of contributing to the group but would not. This prediction was confirmed. The effect was particularly strong in males. Potential remedies for such motivation losses are discussed. (23 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	address = {US},
	author = {Kerr, Norbert L},
	booktitle = {Journal of Personality and Social Psychology},
	doi = {10.1037/0022-3514.45.4.819},
	isbn = {1939-1315(Electronic);0022-3514(Print)},
	keywords = {*Group Participation,*Group Performance,*Intergroup Dynamics,Motivation},
	number = {4},
	pages = {819--828},
	publisher = {American Psychological Association},
	title = {{Motivation Losses in Small Groups: A Social Dilemma Analysis.}},
	volume = {45},
	year = {1983}
}
@article{LongJohn1983,
	abstract = {In this paper we demonstrate how certain very ordinary economic principles lead maximizing individuals to choose consumption-production plans that display many of the characteristics commonly associated with business cycles. Our explanation is entirely consistent with (i) rational expectations, (ii) complete current information, (iii) stable preferences, (iv) no frictions or adjustment costs, (vii) no government, (viii) no money, and (ix) no serial dependence in the stochastic elements of the environment. We also provide a completely worked out example of the type of artificial economy we have in mind. The time-series properties of the example exhibit some major features of observed business cycles. Although this type of model may not be capable of explaining all of the regularities in actual business cycles, we believe that it provides a useful, well-defined benchmark for assesing the relative importance of factors (e.g., monetary disturbances) that we have deliberately ignored.},
	author = {Long, John Jr. and Plosser, Charles I.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Plosser - 1983 - Real Business Cycles.pdf:pdf},
	journal = {Journal of Political Economy},
	number = {1},
	pages = {39--69},
	title = {{Real Business Cycles}},
	url = {http://people.bu.edu/rking/EC702/lpjpe83.pdf},
	volume = {91},
	year = {1983}
}
@article{Rubinstein1983,
	author = {Rubinstein, Ariel and Yaari, Menahem E},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubinstein, Yaari - 1983 - Repeated Insurance Contracts and Moral Hazard.pdf:pdf},
	journal = {Journal of Economic Theory},
	mendeley-groups = {My Research/Theta/Review},
	pages = {74--97},
	title = {{Repeated Insurance Contracts and Moral Hazard}},
	volume = {97},
	year = {1983}
}
@incollection{Phillips1983,
	author = {Phillips, Peter C.B.},
	booktitle = {Handbook of Econometrics},
	chapter = {8},
	doi = {10.1016/S1573-4412(83)01012-0},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Phillips (1983)  Exact small sample theory in the simultaneous equations model.pdf:pdf},
	mendeley-groups = {Small Sample Results},
	publisher = {North-Holland},
	title = {{Exact Small Sample Theory in Simultaneous Equations}},
	volume = {I},
	year = {1983}
}
@article{Hamermesh1984,
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Hamermesh, Daniel},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamermesh - 1984 - Consumption during retirement The mssing link in the life cycle.pdf:pdf},
	isbn = {9788578110796},
	issn = {1098-6596},
	journal = {Journal of Chemical Information and Modeling},
	keywords = {icle},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {1--7},
	pmid = {25246403},
	title = {{Consumption during retirement: The mssing link in the life cycle}},
	volume = {66},
	year = {1984}
}
@article{Granger1984,
	author = {Granger, Clive and Ramanathan, Ramu},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Granger, Ramanathan (1984) Improved methods of combining forecasts.pdf:pdf},
	journal = {Journal of Forecasting},
	keywords = {combining arma models},
	mendeley-groups = {Forecasting/Forecast Combination},
	number = {April/June 1984},
	pages = {197--204},
	title = {{Improved Methods for Combining Forecasts}},
	volume = {3},
	year = {1984}
}
@article{Pagan1984,
	author = {Pagan, Adrian},
	doi = {10.2307/2648877},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Pagan (1984) Econometric Issues in the Analysis of Regressions with Generated Regressors.pdf:pdf},
	journal = {International Economic Review},
	mendeley-groups = {Limit Theory,Limit Theory/Two Step Procedures},
	number = {1},
	pages = {221--247},
	title = {{Econometric Issues in the Analysis of Regressions with Generated Regressors}},
	volume = {25},
	year = {1984}
}
@article{Stone1984,
	author = {Stone, Charles A.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Smoothing Parameter Choice/Stone (1984) An Asymptotically Optimal Window Selection Rule for Kernel Density Estimates.pdf:pdf},
	journal = {Annals of Statistics},
	mendeley-groups = {Nonparametric Estimation/Smoothing Parameter Choice},
	number = {4},
	pages = {1285--1297},
	title = {{An Asymptotically Optimal Window Selection Rule for Kernel Density Estimates}},
	volume = {12},
	year = {1984}
}
@article{Dey1985,
	author = {Dey, Dipak K. and Srinivasan, C.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey, Srinivasan - 1985 - Estimation of a Covariance Matrix under Stein' s Loss.pdf:pdf},
	journal = {Annals of Statistics},
	mendeley-groups = {UPF/ADTSI},
	number = {4},
	pages = {1581--1591},
	title = {{Estimation of a Covariance Matrix under Stein' s Loss}},
	volume = {13},
	year = {1985}
}
@article{Berliner1985,
	author = {Berliner, Baruch},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berliner - 1985 - Large Risks and Limits of Insurability.pdf:pdf},
	journal = {The Geneva Papers of Risk and Insurance},
	mendeley-groups = {My Research/Theta/Review},
	number = {37},
	pages = {313--329},
	title = {{Large Risks and Limits of Insurability}},
	volume = {10},
	year = {1985}
}
@incollection{Gardner1986,
	address = {Baltimore-London},
	author = {Gardner, Bruce L and Kramer, Randall A.},
	booktitle = {Crop Insurance for Agricultural Development: Issues and Experience},
	editor = {Hazell, Peter and Pomareda, Carlos and Valdes, Alberto},
	mendeley-groups = {My Research/Prima,My Research/Theta/Review},
	pages = {195--222},
	publisher = {Johns Hopkins University Press},
	title = {{Experience with Crop Insurance Programs in the United States}},
	year = {1986}
}
@article{Tauchen1986,
	author = {Tauchen, George},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tauchen - 1986 - Finite State Markov-Chain Approximations to Univariate and Vector Autoregressions.pdf:pdf},
	journal = {Economics Letters},
	mendeley-groups = {Trash},
	pages = {177--181},
	title = {{Finite State Markov-Chain Approximations to Univariate and Vector Autoregressions}},
	volume = {20},
	year = {1986}
}
@article{Skees1986,
	author = {Skees, Jerry R and Reed, M.R. Michael R},
	doi = {10.2307/1241549},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skees, Reed - 1986 - Rate making for farm-level crop insurance implications for adverse selection.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {adverse},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {653--659},
	title = {{Rate making for farm-level crop insurance: implications for adverse selection}},
	url = {http://www.jstor.org/stable/10.2307/1241549},
	volume = {68},
	year = {1986}
}
@article{Dasgupta1986,
	abstract = {In the predecessor to this article (Dasgupta and Ray, I986; hereafter DR), we developed a theory which provides a link between persistent involuntary unemployment and the incidence of undernourishment, relates them in turn to the production and distribution of ...$\backslash$n},
	author = {Dasgupta, Partha and Ray, Debraj},
	doi = {10.2307/2233329},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasgupta, Ray - 1986 - Inequality as a Determinant of Malnutrition and Unemployment Theory.pdf:pdf},
	isbn = {0013-0133},
	issn = {00130133},
	journal = {The Economic Journal},
	keywords = {1011-1034,1986,384,96,author,blackwell publishing for the,dec,no,of malnutrition and unemployment,partha dasgupta and debraj,pp,published by,quality as a determinant,ray,royal economic society,s,source,the economic journal,theory,vol},
	number = {384},
	pages = {1011--1034},
	title = {{Inequality as a Determinant of Malnutrition and Unemployment: Theory}},
	url = {http://www.jstor.org/stable/2233329?origin=crossref{\%}5Cnpapers3://publication/doi/10.2307/2233329},
	volume = {96},
	year = {1986}
}
@article{Prescott1986,
	abstract = {Recent developments in business cycle theory are reviewed. The principal finding is that the growth model, which was developed to account for the secular patterns in important economic aggregates, displays the business cycle phenomena once it incorporates the observed randomness in the rate of technological advance. The amplitudes and serial correlation properties of fluctuations in output and employment that the growth model predicts match those historically experienced in the United States. Further, the model continues to display the growth facts it was developed to explain. (This abstract was borrowed from another version of this item.)},
	author = {Prescott, Edward C},
	doi = {10.1016/0167-2231(86)90036-9},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Prescott - 1986 - Theory ahead of business cycle measurement.pdf:pdf},
	isbn = {1111111111},
	issn = {01672231},
	journal = {Quarterly Review, Federal Reserve Bank of Minneapolis},
	number = {Fall},
	pages = {9--22},
	title = {{Theory ahead of business cycle measurement}},
	year = {1986}
}
@book{Hazell1986,
	abstract = {Le cr{\'{e}}dit rural, et les assurances sont faiblement implant{\'{e}}s dans les zones rurales. Binswanger fait une pr{\'{e}}sentation synth{\'{e}}tique des diff{\'{e}}rentes raisons: il concentre son expos{\'{e}} sur les probl{\`{e}}mes d'assym{\'{e}}trie d'information, qui sont {\`{a}} l'origine du faible d{\'{e}}veloppement du cr{\'{e}}dit rural et des assurances.},
	address = {Baltimore-London},
	author = {Hazell, Peter and Pomareda, Carlos and Valdes, Alberto},
	booktitle = {Crop Insurance for Agricultural Development},
	doi = {10.1016/0306-9192(88)90038-3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hazell, Pomareda, Valdes - 1986 - Crop Insurance for Agricultural Development Issues and Experience.pdf:pdf},
	isbn = {080182673X},
	issn = {03069192},
	keywords = {ADVERSE SELECTION,AGRICULTURAL POLICIES,CROP INSURANCE,INFORMATION,MORAL HAZARD,RISK BIBRISK,UNCERTAINTY},
	mendeley-groups = {My Research/Theta/Review},
	pages = {1--322},
	publisher = {Johns Hopkins University Press},
	title = {{Crop Insurance for Agricultural Development: Issues and Experience}},
	year = {1986}
}
@article{Robinson1987,
	abstract = {In a multiple regression model the residual variance is an unknown function of the explanatory variables, and estimated by nearest neighbor nonparametric regression. The resulting weighted least squares estimator of the regression coefficients is shown to be adaptive, in the sense of having the same asymptotic distribution, to first order, as estimators based on knowledge of the actual variance function or a finite parameterization of it. A similar result was established by Carroll (1982) using kernel estimation and under substantially more restrictive conditions on the data generating process than ours. Extensions to various other models seem to be possible.},
	author = {Robinson, P.M.},
	doi = {10.2307/1911033},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Robinson (1987) Asymptotically Efficient Estimation in the Presence of Heteroskedasticity of Unknown Form.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {adaptive estimation,asymptotic efficiency,heteroskedasticity,nearest neighbor regression},
	mendeley-groups = {Semiparametric Estimation},
	number = {4},
	pages = {875},
	title = {{Asymptotically Efficient Estimation in the Presence of Heteroskedasticity of Unknown Form}},
	volume = {55},
	year = {1987}
}
@article{Manski1987,
	abstract = {Andersen (1970) considered the problem of inference on random effects linear models from binary response panel data. He showed that inference is possible if the disturbances for each panel member are known to be white noise with the logistic distribution and if the observed explanatory variables vary over time. A conditional maximum likelihood estimator consistently estimates the model parameters up to scale. The present paper shows that inference remains possible if the disturbances for each panel member are known only to be time-stationary with unbounded support and if the explanatory variables vary enough over time. A conditional version of the maximum score estimator (Manski, 1975, 1985) consistently estimates the model parameters up to scale.},
	author = {Manski, Charles F.},
	doi = {10.2307/1913240},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Manski (1987) Semiparametric Analysis of Random Effects Linear Models from Binary Panel Data..pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {binary response,maximum score,panel data,semiparametric estimation},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {2},
	pages = {357},
	title = {{Semiparametric Analysis of Random Effects Linear Models from Binary Panel Data}},
	volume = {55},
	year = {1987}
}
@article{Newey1987,
	abstract = {This paper discusses asymptotically efficient estimation of the parameters of limited dependent variable models with endogenous explanatory variables. General results on asymptotic efficiency of two-stage and Amemiya GLS estimators are derived and used to obtain a simple, asymptotically efficient estimator of the structural coefficients. This estimator can be calculated by applying GLS to estimates of the reduced form coefficients that are obtained by using reduced form residuals as additional explanatory variables. It is also shown that it is possible to obtain asymptotically efficient estimators of the other coefficients by a modified minimum chi-square method.},
	author = {Newey, Whitney K.},
	doi = {http://dx.doi.org/10.1016/0304-4076(87)90001-7},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {My Research/Zeta},
	number = {3},
	pages = {231--250},
	title = {{Efficient estimation of limited dependent variable models with endogenous explanatory variables}},
	url = {http://www.sciencedirect.com/science/article/pii/0304407687900017},
	volume = {36},
	year = {1987}
}
@article{Andrews1987,
	author = {Andrews, Donald W.K.},
	doi = {10.2307/1913568},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Andrews (1987) Consistency in Nonlinear Econometric Models A Generic Uniform Law of Large Numbers.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Limit Theory},
	number = {6},
	pages = {1465--1471},
	title = {{Consistency in Nonlinear Econometric Models: A Generic Uniform Law of Large Numbers}},
	volume = {55},
	year = {1987}
}
@incollection{Bierens1987,
	author = {Bierens, Herman J},
	booktitle = {Advances in Econometrics: Fifth World Congress},
	doi = {10.1017/CCOL0521344301.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Bierens, (1987) Kernel estimators of regression functions.pdf:pdf},
	isbn = {0521344301},
	mendeley-groups = {Nonparametric Estimation/Nonparametric Regression},
	pages = {99--194},
	title = {{Kernel Estimators of Regression Functions}},
	year = {1987}
}
@article{Chamberlain1987,
	author = {Chamberlain, Gary},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Chamberlain (1987) Asymptotic efficiency in estimation with conditional moment restrictions..pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Limit Theory/Efficiency,Efficiency},
	pages = {305--334},
	title = {{Asymptotic efficiency in estimation with conditional moment restrictions.}},
	volume = {34},
	year = {1987}
}
@article{Diebold1987,
	author = {Diebold, Francis X and Pauly, Peter},
	doi = {10.1002/for.3980060103},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Diebold,  Pauly (1987) Structural change and the combination of forecasts..pdf:pdf},
	journal = {Journal of Forecasting},
	keywords = {changes in attitudes,constant changes,economy,in reality thcre are,structural change forecast combination,structural shifts in the,varying-parameter models},
	mendeley-groups = {Forecasting/Forecast Combination},
	number = {December 1986},
	pages = {21--40},
	title = {{Structural Change and the Combination of Forecasts}},
	volume = {40},
	year = {1987}
}
@article{Nelson1987,
	abstract = {The economic theory of contracts is applied to agricultural insurance to show that, given full information, Pareto-optimal insurance contracts are actuarially fair, provide full coverage, and differ for each individual. The information problems of moral hazard and adverse selection prevent Pareto optimality from being attained. Several "second-best" solutions to these problems are applied to agricultural insurance. It is shown that information collection and the application of contract design principles are "second-best" solutions which may achieve the benefits of insurance at less cost than the current practice of public subsidies.},
	author = {Nelson, Carl H and Loehman, Edna T},
	doi = {10.2307/1240644},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nelson, Loehman - 1987 - Further toward a Theory of Agricultural Insurance.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {adverse selection,agricultural insurance,moral hazard,principal-agent theory},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {523--531},
	title = {{Further toward a Theory of Agricultural Insurance}},
	url = {http://www.jstor.org/stable/1241688},
	volume = {69},
	year = {1987}
}
@article{Elder1988,
	abstract = {A major task for research on the social costs of economic stress is to trace how macrosocial changes affect increasingly smaller social units and ultimately those microsocial phenomena that directly influence children in their families. In this paper, we specify linkages between macroeconomic change and children's development by tracing deprivational effects through family adaptations in the household economy and in personal relationships. Our findings from research on children and families of the Great Depression are discussed in relation to an interactional model of the process by which families adapt to stressful times.},
	author = {Elder, Glen H and Caspi, Avshalom},
	doi = {10.1111/j.1540-4560.1988.tb02090.x},
	issn = {1540-4560},
	journal = {Journal of Social Issues},
	mendeley-groups = {My Research/Zeta},
	month = {jan},
	number = {4},
	pages = {25--45},
	publisher = {Blackwell Publishing Ltd},
	title = {{Economic Stress in Lives: Developmental Perspectives}},
	url = {http://dx.doi.org/10.1111/j.1540-4560.1988.tb02090.x},
	volume = {44},
	year = {1988}
}
@article{Abreu1988,
	author = {Abreu, Dilip},
	doi = {10.2307/1911077},
	journal = {Econometrica},
	number = {2},
	pages = {383--396},
	title = {{On the Theory of Infinitely Repeated Games with Discounting}},
	url = {https://www.jstor.org/stable/1911077},
	volume = {56},
	year = {1988}
}
@article{Holtz-Eakin1988,
	author = {Holtz-Eakin, Douglas and Newey, Whitney K. and Rosen, Harvey S.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Panel ARs/Holtz-Eakin, Newey, Rosen (1988) - Panel VARs.pdf:pdf},
	journal = {Econometrica},
	keywords = {causality tests,labor supply,panel data,vector autoregression},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {6},
	pages = {1371--1395},
	title = {{Estimating Vector Autoregressions with Panel Data}},
	volume = {56},
	year = {1988}
}
@article{Robinson1988,
	author = {Robinson, P.M.},
	doi = {10.2307/1912705},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Robinson  (1988) Root-N-Consistent Semiparametric Regression.pdf:pdf},
	journal = {Econometrica},
	keywords = {Regression,Root-n-consistent Semiparametric},
	mendeley-groups = {Semiparametric Estimation},
	number = {4},
	pages = {931--954},
	title = {{Root-N-Consistent Semiparametric Regression}},
	volume = {56},
	year = {1988}
}
@article{Vuong1989,
	author = {Vuong, Quang H.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuong - 1989 - Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses.pdf:pdf},
	journal = {Econometrica},
	keywords = {likelihood ratio tests,misspecified,model selection,models,non-nested hypotheses,weighted sums of chi-squares},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {4},
	pages = {671--684},
	title = {{Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses}},
	volume = {33},
	year = {1989}
}
@article{Arellano1989,
	abstract = {The asymptotic variances of the IV estimators for dynamic panel data proposed by Anderson and Hsiao (1982) are obtained for some simple models. With an autoregressive exogenous variable, the estimator that uses differenced instruments has a singularity point and very large variances over a significant range of parameter values. On the contrary, the estimator that uses instruments in levels has no singularities and much smaller variances. {\textcopyright} 1989.},
	author = {Arellano, Manuel},
	doi = {10.1016/0165-1765(89)90025-6},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Arellano (1990) A Note on the Anderson-Hsiao Estimator for Panel Data.pdf:pdf},
	issn = {01651765},
	journal = {Economics Letters},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {4},
	pages = {337--341},
	title = {{A note on the Anderson-Hsiao estimator for panel data}},
	volume = {31},
	year = {1989}
}
@article{Bernanke1989,
	author = {Bernanke, Ben and Gertler, Mark},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernanke, Gertler - 1989 - Agency Costs, Net Worth, and Business Fluctuations.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {1},
	pages = {14--31},
	title = {{Agency Costs, Net Worth, and Business Fluctuations}},
	url = {http://www.jstor.org/stable/1804770},
	volume = {79},
	year = {1989}
}
@techreport{Manski1990,
	author = {Manski, Charles},
	keywords = {econometrics,indicators,information,probability},
	mendeley-groups = {Sample Selection},
	publisher = {Wisconsin Madison - Social Systems},
	series = {Working papers},
	title = {{The Selection Problem}},
	url = {https://econpapers.repec.org/RePEc:att:wimass:90-12},
	year = {1990}
}
@article{Nelson1990,
	author = {Nelson, Charles R. and Startz, Richard},
	doi = {10.2307/2938359},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Nelson,  Startz,  (1990) Some Further Results on the Exact Small Sample Properties of the Instrumental Variable Estimator.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Small Sample Results,Small Sample Results/Linear},
	number = {4},
	pages = {967--976},
	title = {{Some Further Results on the Exact Small Sample Properties of the Instrumental Variable Estimator}},
	volume = {58},
	year = {1990}
}
@article{Friedman1990,
	abstract = {The fact that most elderly U. S. individuals maintain a flat age-wealth profile, rather than buy individual life annuities, contradicts the standard life-cycle consumption model. Average expected yields on individual life annuities in the United States during 1968-1983 were lower by 4.21-6.13 percent, or 2.43-4.35 percent after allowing for adverse selection, than yields on plausible alternative investments. Simulations of a model of saving and portfolio allocation show that during the early retirement years such yield differentials can account for the absence of annuity purchases even without a bequest motive. At older ages the combination of such yield differentials and a bequest motive can do so. [ABSTRACT FROM AUTHOR]},
	annote = {NULL},
	author = {Friedman, Benjamin M. and Warshawsky, Mark J.},
	doi = {10.2307/2937822},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Warshawsky - 1990 - The Cost of Annuities Implications for Saving Behavior and Bequests.pdf:pdf},
	isbn = {00335533},
	issn = {0033-5533},
	journal = {Quarterly Journal of Economics},
	number = {1},
	pages = {135--154},
	title = {{The Cost of Annuities : Implications for Saving Behavior and Bequests}},
	url = {http://www.nber.org/papers/w1682},
	volume = {105},
	year = {1990}
}
@article{Newey1990,
	author = {Newey, Whitney K.},
	doi = {10.1002/jae.3950050202},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Efficiency/Newey (1990)  Semiparametric Efficiency Bounds.pdf:pdf;:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newey - 1990 - Semiparametric Efficiency Bounds.pdf:pdf;:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newey - 1990 - Semiparametric Efficiency Bounds(2).pdf:pdf},
	journal = {Journal of Applied Econometrics},
	mendeley-groups = {Efficiency},
	number = {2},
	pages = {99--135},
	title = {{Semiparametric Efficiency Bounds}},
	volume = {5},
	year = {1990}
}
@article{Abreu1990,
	author = {Abreu, Dilip and Pearce, David and Stacchetti, Ennio},
	doi = {10.2307/2938299},
	journal = {Econometrica},
	keywords = {algorithm,asymmetric repeated games,bang reward functions,bang-,extremal equilibria,self-generation},
	number = {5},
	pages = {1041--1063},
	title = {{Toward a Theory of Discounted Repeated Games with Imperfect Monitoring}},
	url = {http://www.jstor.org/stable/2938299},
	volume = {58},
	year = {1990}
}
@misc{Hastie1990,
	abstract = {Generalized additive models (GAMs) have distinct advantages over generalized linear models as they allow investigators to make inferences about associations between outcomes and predictors without placing parametric restrictions on the associations. The variable of interest is often smoothed using a locally weighted regression (LOESS) and the optimal span (degree of smoothing) can be determined by minimizing the Akaike Information Criterion (AIC). A natural hypothesis when using GAMs is to test whether the smoothing term is necessary or if a simpler model would suffice. The statistic of interest is the difference in deviances between models including and excluding the smoothed term. As approximate chi-square tests of this hypothesis are known to be biased, permutation tests are a reasonable alternative. We compare the type I error rates of the chi-square test and of three permutation test methods using synthetic data generated under the null hypothesis. In each permutation method a distribution of differences in deviances is obtained from 999 permuted datasets and the null hypothesis is rejected if the observed statistic falls in the upper 5{\%} of the distribution. One test is a conditional permutation test using the optimal span size for the observed data; this span size is held constant for all permutations. This test is shown to have an inflated type I error rate. Alternatively, the span size can be fixed a priori such that the span selection technique is not reliant on the observed data. This test is shown to be unbiased; however, the choice of span size is not clear. A third method is an unconditional permutation test where the optimal span size is selected for observed and permuted datasets. This test is unbiased though computationally intensive.},
	author = {Hastie, Trevor and Tibshirani, Robert},
	booktitle = {Statistical Science},
	doi = {10.1016/j.csda.2010.05.004},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Tibshirani - 1990 - Generalized additive models.pdf:pdf},
	isbn = {0412343908},
	issn = {01679473},
	number = {3},
	pages = {297--318},
	pmid = {20948974},
	title = {{Generalized additive models}},
	url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Generalized+additive+models{\#}0},
	volume = {1},
	year = {1990}
}
@article{Gaubatz1991,
	abstract = {Insight into the effect of domestic factors on the international conflict behavior of democratic states can be garnered from treating electoral cycles as cycles in the relative power of state and society. This article shows that there is a discernible relationship between election cycles and the behavior of democratic states in international conflicts. In the past 200 years, democratic states have tended to get into relatively more wars early in the election cycle and fewer wars late in the cycle. Interestingly, this result holds regardless of whether it was a democracy or a nondemocracy which initiated the war. The author argues, however, that this pattern at the international level is insufficient for drawing firm conclusions about the preferences of democratic states and societies at the domestic level. Finally, the author suggests that although election dynamics may mitigate against war entry in the short run, it is plausible that they have made war more likely or more severe in the long run.},
	author = {Gaubatz, Kurt Taylor},
	doi = {10.1177/0022002791035002004},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaubatz - 1991 - Election Cycles {\&} War.pdf:pdf},
	journal = {Journal of Conflict Resolution},
	number = {2},
	pages = {212--244},
	title = {{Election Cycles {\&} War}},
	volume = {35},
	year = {1991}
}
@article{Arellano1991,
	author = {Arellano, Manuel and Bond, Stephen},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Arellano, Bond (1991).pdf:pdf},
	journal = {Review of Economic Studies},
	mendeley-groups = {Panel Data/Dynamic Panels},
	pages = {277--297},
	title = {{Some Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations}},
	volume = {58},
	year = {1991}
}
@article{Rice1991,
	abstract = {We develop methods for the analysis of a collection of curves which are stochastically modelled as independent realizations of a random function with an unknown mean and covariance structure. We propose a method of estimating the mean function non- parametrically under the assumption that it is smooth. We suggest a variant on the usual form of cross-validation for choosing the degree of smoothing to be employed. This method of cross-validation, which consists of deleting entire sample curves, has the advantage that it does not require that the covariance structure be known or estimated. In the estimation of the covariance structure, we are primarily concerned with models in which the first few eigenfunctions are smooth and the eigenvalues decay rapidly, so that the variability is predominantly of large scale. We propose smooth nonparametric estimates of the eigen- functions and a suitable method of cross-validation to determine the amount of smoothing. Our methods are applied to data on the gaits of a group of 5-year-old children.},
	author = {Rice, John A. and Silverman, B. W.},
	doi = {10.1111/j.2517-6161.1991.tb01821.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Rice, Silverman (1991) Estimating the Mean and Covariance Structure Nonparametrically When the Data are Curves.pdf:pdf},
	issn = {0035-9246},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	keywords = {analysis,cross-validation,curve estimation,eigenfunction expansions,gait,principal components,smoothing},
	mendeley-groups = {Nonparametric Estimation},
	number = {1},
	pages = {233--243},
	title = {{Estimating the Mean and Covariance Structure Nonparametrically When the Data are Curves}},
	volume = {53},
	year = {1991}
}
@article{Miranda1991,
	author = {Miranda, Mario J},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miranda - 1991 - Area yield crop insurance reconsidered.pdf:pdf},
	journal = {American Journal of Agricultural Economics},
	keywords = {crop insurance,optimal hedging,risk and uncertainty},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {233--242},
	title = {{Area yield crop insurance reconsidered}},
	volume = {73},
	year = {1991}
}
@article{Holdren1991,
	abstract = {When energy is scarce or expensive, people can suffer material deprivation and economic hardship. When it is obtained in ways that fail to minimize environmental and political costs, these too can threaten human wellbeing in fundamental and pervasive ways. The energy problem today combines these syndromes: much of the world's population has too little energy to meet basic human needs; the monetary costs of energy are rising nearly everywhere; the environmental impacts of energy supply are growing and already dominant contributors to local, regional, and global environmental problems (including air pollution, water pollution, ocean pollution, and climate change); and the sociopolitical risks of energy supply (above all the danger of conflict over oil and the links between nuclear energy and nuclear weapons) are growing too. This predicament has many causes, but predominant among them are the nearly 20-fold increase in world energy use since 1850 and the cumulative depletion of the most convenient oil and gas deposits that this growth has entailed, resulting in increasing resort to costlier and/or environmentally more disruptive energy sources. The growth of world population in this period was responsible for 52{\%} of the energy growth, while growth in per capita energy use was responsible for 48{\%} (excluding causal connections between population and energy use per capita). In the United States in the same period, population growth accounted for 66{\%} of the 36-fold increase in energy use. In the late 1980s, population growth was still accounting for a third of energy growth both in the United States and worldwide. Coping with global energy problems will require greatly increased investment in improving the efficiency of energy enduse and in reducing the environmental impacts of contemporary energy technologies, and it will require financing a transition over the next several decades to a set of more sustainable (but probably also more expensive) energy sources. The difficulty of implementing these measures will be greatest by far in the developing countries, not least because of their high rates of population growth and the attendant extra pressures on economic and managerial resources. If efficiency improvements permit delivering the high standard of living to which the world aspires based on a per capita rate of energy use as low as 3 kilowatts—about a quarter of the current U.S. figure—then a world population stabilized at 10 billion people would be using energy at a rate of 30 terawatts, and a population of 14 billion would imply 42 terawatts (compare 13.2 terawatts in 1990). Delivering even the lower figure at tolerable monetary and environmental costs will be difficult; each additional billion people added to the world population will compound these difficulties and increase energy's costs, making everyone poorer.},
	author = {Holdren, John P.},
	doi = {10.1007/BF01357916},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holdren - 1991 - Population and the energy problem.pdf:pdf},
	isbn = {0199-0039},
	issn = {01990039},
	journal = {Population and Environment},
	mendeley-groups = {My Teaching/Energy},
	number = {3},
	pages = {231--255},
	title = {{Population and the energy problem}},
	volume = {12},
	year = {1991}
}
@article{Newey1991,
	author = {Newey, Whitney K.},
	doi = {10.2307/2938179},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Newey (1991) Uniform Convergence in Probability and Stochastic Equicontinuity.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Limit Theory},
	number = {4},
	pages = {1161--1167},
	title = {{Uniform Convergence in Probability and Stochastic Equicontinuity}},
	volume = {59},
	year = {1991}
}
@article{Horowitz1992,
	abstract = {Manski (1985) has shown that the maximum score estimator of the coefficient vector of a binary response model is consistent under weak distributional assumptions. Cavanagh (1987) and Kim and Pollard (1989) have shown that N{\textless}sup{\textgreater}1/3{\textless}/sup{\textgreater} times the centered maximum score estimator converges in distribution to the random variable that maximizes a certain Gaussian process. The properties of the limiting distribution are largely unknown, and the result of Cavanagh and Kim and Pollard cannot be used for inference in applications. This paper describes a modified maximum score estimator that is obtained by maximizing a smoothed version of Manski's score function. Under distributional assumptions that are somewhat stronger than Manski's but still very weak, the centered smoothed estimator is asymptotically normal with a convergence rate that is at least N{\textless}sup{\textgreater}-2/5{\textless}/sup{\textgreater} and can be made arbitrarily close to N{\textless}sup{\textgreater}-1/2{\textless}/sup{\textgreater}, depending on the strength of certain smoothness assumptions. The estimator's rate of convergence is the fastest possible under the assumptions that are made. The parameters of the limiting distribution can be estimated consistently from data, thereby making statistical inference based on the smoothed estimator possible with samples that are sufficiently large.},
	author = {Horowitz, Joel L.},
	doi = {10.2307/2951582},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Horowitz (1992) A Smoothed Maximum Score Estimator for the Binary Response Model.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {3},
	pages = {505},
	title = {{A Smoothed Maximum Score Estimator for the Binary Response Model}},
	volume = {60},
	year = {1992}
}
@article{Diaz-Gimenez1992,
	author = {D{\'{i}}az-Gim{\'{e}}nez, Javier and Prescott, Edward C and Fitzgerald, Terry and Alvarez, Fernando},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D{\'{i}}az-Gim{\'{e}}nez et al. - 1992 - Banking in computable general equilibrium economies.pdf:pdf},
	journal = {Journal of Economic Dynamics and Control},
	mendeley-groups = {Financial Intermediaries},
	pages = {533--559},
	title = {{Banking in computable general equilibrium economies *}},
	volume = {16},
	year = {1992}
}
@article{Chamberlain1992,
	author = {Chamberlain, Gary},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Chamberlain (1992) Efficiency Bounds for Semiparametric Regression.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Heterogeneous Panels,Limit Theory/Efficiency},
	number = {3},
	pages = {567--596},
	title = {{Efficiency Bounds for Semiparametric Regression}},
	volume = {60},
	year = {1992}
}
@article{Loehman1992,
	author = {Loehman, Edna and Nelson, Carl H},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loehman, Nelson - 1992 - Optimal Risk Management , Risk Aversion , and Production Function Properties.pdf:pdf},
	journal = {Journal of Agricultural and Resource Economics},
	keywords = {production risk,risk aversion,risk management},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {219--231},
	title = {{Optimal Risk Management , Risk Aversion , and Production Function Properties}},
	volume = {17},
	year = {1992}
}
@article{Angrist1992,
	author = {Angrist, Joshua and Imbens, Guido W},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Angrist, Imbens - 1992 - Identification and Estimation of Local Average Treatment Effects.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects},
	number = {2},
	pages = {476--475},
	title = {{Identification and Estimation of Local Average Treatment Effects}},
	volume = {62},
	year = {1992}
}
@article{Obstfeld1992,
	abstract = {The assumption that economic activity takes place continuously is a convenient abstraction in many applications. In others, such as the study of financial-market equilibrium, the assumption of continuous trading corresponds closely to reality. Regardless of motivation, ... $\backslash$n},
	author = {Obstfeld, Maurice},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Obstfeld - 1992 - Dynamic Optimization in Continuous-Time Economic Models (A Guide for the Perplexed).pdf:pdf},
	journal = {Unpublished},
	number = {April},
	pages = {21--47},
	title = {{Dynamic Optimization in Continuous-Time Economic Models (A Guide for the Perplexed)}},
	year = {1992}
}
@article{Munnell1992,
	author = {Munnell, Alicia H},
	issn = {08953309},
	journal = {The Journal of Economic Perspectives},
	number = {4},
	pages = {189--198},
	publisher = {American Economic Association},
	title = {{Policy Watch: Infrastructure Investment and Economic Growth}},
	url = {http://www.jstor.org/stable/2138275},
	volume = {6},
	year = {1992}
}
@article{ODonoghue1992,
	author = {O'Donoghue, Ted and Rabit, Mathhew},
	doi = {10.1257/aer.89.1.103},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Donoghue, Rabit - 1992 - Doing It Now or Later.pdf:pdf},
	journal = {American Economic Review},
	number = {1},
	pages = {103--124},
	title = {{Doing It Now or Later}},
	volume = {89},
	year = {1992}
}
@article{Morris1992,
	abstract = {To assess the effect of unemployment and early retirement on cigarette smoking, alcohol consumption, and body weight in middle aged British men.},
	author = {Morris, J K and Cook, D.G. and Shaper, A. G.},
	doi = {10.1136/bmj.304.6826.536},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morris, Cook, Shaper - 1992 - Non-employment and changes in smoking, drinking, and body weight.pdf:pdf},
	isbn = {0959-8138},
	issn = {0959-8138},
	journal = {British Medical Journal},
	keywords = {Adult,Alcohol Drinking,Alcohol Drinking: epidemiology,Body Weight,Cohort Studies,Great Britain,Humans,Male,Middle Aged,Physical Fitness,Prospective Studies,Retirement,Retirement: statistics {\&} numerical data,Smoking,Smoking: epidemiology,Social Class,Time Factors,Unemployment,Unemployment: statistics {\&} numerical data},
	mendeley-groups = {My Research/Zeta},
	number = {February},
	pages = {536--41},
	pmid = {1559056},
	title = {{Non-employment and changes in smoking, drinking, and body weight.}},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1881409{\&}tool=pmcentrez{\&}rendertype=abstract},
	volume = {304},
	year = {1992}
}
@article{Honore1992,
	author = {Honore, Bo},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Honore (1992) Trimmed Lad and Least Squares Estimation of Truncated and Censored Regression Models with Fixed Effects..pdf:pdf},
	journal = {Econometrica},
	keywords = {fixed effects,panel data,truncated},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {3},
	pages = {533--565},
	title = {{Trimmed LAD and Least Squares Estimation of Truncated and Censored Regression Models with Fixed Effects}},
	volume = {60},
	year = {1992}
}
@article{Huggett1993,
	author = {Huggett, Mark},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huggett - 1993 - The risk-free rate in heterogeneous-agent incomplete-insurance economies.pdf:pdf},
	journal = {Journal of Economic Dynamics and Control},
	mendeley-groups = {Trash},
	pages = {953--969},
	title = {{The risk-free rate in heterogeneous-agent incomplete-insurance economies}},
	volume = {17},
	year = {1993}
}
@article{Lian1993,
	abstract = {It is conventional wisdom that the public rallies 'round the president when military force is used abroad. Indeed, this belief has encouraged the view that presidents are apt to rattle the saber to divert attention from domestic problems. The rally effect is assessed by measuring the change in the president's popularity following all major uses of force by the United States from 1950 through 1984. Surprisingly, for these 102 cases, the mean change in the president's approval rating is 0{\%}, even among the members of his party. Even well-publicized uses of force during a crisis boost the president's standing only 2{\%}-3{\%} on average. Regression analyses confirm that the rallying effect of a use of force is greater in a crisis and when the action is prominently reported by the media. In addition, rallies are greater when the president enjoys bipartisan support, his initial popularity is low, and the country is not at war or fatigued by war.},
	author = {Lian, Bradley and Oneal, John R.},
	doi = {10.1177/0022002793037002003},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lian, Oneal - 1993 - Presidents, the Use of Military Force, and Public Opinion.pdf:pdf},
	issn = {0022-0027},
	journal = {Journal of Conflict Resolution},
	number = {2},
	pages = {277--300},
	title = {{Presidents, the Use of Military Force, and Public Opinion}},
	volume = {37},
	year = {1993}
}
@article{Manski1993,
	abstract = {The aim of methodological research in the social sciences is to learn what conclusions can and cannot be drawn given empiri- cally relevant combinations of assumptions and data. Method- ologists have long found it useful to separate inferential prob- lems into statistical and identification components. Studies of identification seek to characterize the conclusions that could be drawn if the researcher had available a sample of unlimited size. Studies of statistical inference seek to characterize the generally weaker conclusions that can be drawn given a sample of positive butfinite size. Statistical and identification problems limit in distinct ways the conclusions that may be drawn in empirical research. Statistical problems are most severe when the available sample is small. Identification problems are most severe when the researcher knows little about the population under study and the sampling process yields only weak data on the population. This chapter synthesizes some of my recent research and thinking on identification problems in the social sciences. Four problems are discussed: extrapolation of regres- sions, the selection problem, identification of endogenous so- cial effects, and identification of subjective phenomena. These problems arise regularly in social science research and are the source of many substantive disputes.},
	author = {Manski, Charles F.},
	doi = {10.2307/271005},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Sample Selection/Manski (1993) Identification Problems in the Social Sciences.pdf:pdf},
	issn = {00811750},
	journal = {Sociological Methodology},
	mendeley-groups = {Sample Selection},
	number = {1993},
	pages = {1},
	title = {{Identification Problems in the Social Sciences}},
	volume = {23},
	year = {1993}
}
@article{Heckathorn1993,
	abstract = {Douglas D .},
	author = {Heckathorn, Douglas D .},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckathorn - 1993 - Collective Action and Group Heterogeneity Voluntary Provision Versus Selective Incentives.pdf:pdf},
	journal = {American Sociological Review},
	number = {3},
	pages = {329--350},
	title = {{Collective Action and Group Heterogeneity: Voluntary Provision Versus Selective Incentives}},
	url = {http://www.jstor.org/stable/2095904},
	volume = {58},
	year = {1993}
}
@article{Horowitz1993,
	abstract = {This paper examines how crop insurance affects corn farmers' fertilizer and pesticide use in the U.S. Midwest. Crop insurance might be expected to affect chemical use because of "moral hazard"; insured farmers may undertake riskier production than do uninsured farmers. Results suggest that insurance exerts considerable influence on corn farmers' chemical use decisions. Those purchasing insurance applied significantly more nitrogen per acre (19{\%}), spent more on pesticides (21{\%}), and treated more acreage with both herbicides and insecticides (7{\%} and 63{\%}) than did those not purchasing insurance. These results suggest that both fertilizer and pesticides may be risk-increasing inputs.},
	author = {Horowitz, John K and Lichtenberg, Erik},
	doi = {10.2307/1243980},
	issn = {00029092, 14678276},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {4},
	pages = {926--935},
	publisher = {[Agricultural {\&} Applied Economics Association, Oxford University Press]},
	title = {{Insurance, Moral Hazard, and Chemical Use in Agriculture}},
	url = {http://www.jstor.org/stable/1243980},
	volume = {75},
	year = {1993}
}
@article{Rosenzweig1993,
	abstract = {This paper utilizes panel data from rural India to examine how the composition of asset holdings varies across farmers with different levels of total wealth and across farmers facing different degrees of weather risk. In particular, the riskiness of farmers' asset portfolios are measured in terms of their sensitivity to weather variation and a test is developed and implemented of risk aversion based on the association between the average returns to individual production assets and their sensitivity to weather variability. How the responsiveness of portfolio riskiness and farm profitability to the influence of exogenous weather risk varies with wealth is also estimated.},
	author = {Rosenzweig, Mark R and Binswanger, Hans P},
	doi = {10.2307/2234337},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenzweig, Binswanger - 1993 - Wealth, Weather Risk and the Composition and Profitability of Agricultural Investments.pdf:pdf},
	isbn = {00130133},
	issn = {00130133},
	journal = {The Economic Journal},
	keywords = {Department of Agricultural and Applied Economics -,Department of Economics - Minneapolis,Economic Development Center,University of Minnesota},
	mendeley-groups = {My Teaching/Agro},
	number = {416},
	pages = {56--78},
	title = {{Wealth, Weather Risk and the Composition and Profitability of Agricultural Investments}},
	url = {http://www.jstor.org/stable/2234337?origin=crossref},
	volume = {103},
	year = {1993}
}
@article{Goodwin1993,
	abstract = {Knowledge of factors affecting farmer purchases of crop insurance is essential for evaluating the soundness and profitability of crop insurance programs. Despite this importance, the demand for crop insurance has received limited empirical attention. The present paper reports on an empirical assessment of the demand for crop insurance by Iowa corn producers. Adverse selection in the insured pool suggests that producers with differing levels of loss-risk have different demand elasticities. Loss-risk is included in the empirical analysis and is found to influence the elasticity of demand. Results show average demand elasticities of about -0.32 for relative insured acres and -0.73 for liability per planted acre. Implications for the actuarial soundness of the industry are provided.},
	author = {Goodwin, Barry K.},
	doi = {10.2307/1242927},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodwin - 1993 - An empirical analysis of the demand for multiple peril crop insurance.pdf:pdf},
	isbn = {0002-9092},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {act of 1980 ex-,act was to,adverse selection,crop,crop insurance,demand,insurance programs,objectives of u,panded the scope and,s,the federal crop insurance,the goal of the},
	mendeley-groups = {My Research/Theta/Review},
	pages = {425--434},
	title = {{An empirical analysis of the demand for multiple peril crop insurance}},
	url = {http://www.jstor.org/stable/1242927},
	volume = {75},
	year = {1993}
}
@article{Dick1994,
	abstract = {Customer loyalty is viewed as the strength of the relationship between an individual's relative attitude and repeat patronage. The relationship is seen as mediated by social norms and situational factors. Cognitive, affective, and conative antecedents of relative attitude are identified as contributing to loyalty, along with motivational, perceptual and behavioral consequences. Implications for it- search and for the management of loyally are derived.},
	archivePrefix = {arXiv},
	arxivId = {0803973233},
	author = {Dick, Alan S. and Basu, Kuna},
	doi = {10.1177/0092070394222001},
	eprint = {0803973233},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dick, Basu - 1994 - Customer Loyalty Toward an Integrated Conceptual Framework.pdf:pdf},
	isbn = {00920703},
	issn = {0092-0703},
	journal = {Journal of the Academy of Marketing Science},
	mendeley-groups = {My Research/Alpha},
	number = {2},
	pages = {99--113},
	pmid = {867488},
	title = {{Customer Loyalty: Toward an Integrated Conceptual Framework}},
	volume = {22},
	year = {1994}
}
@incollection{Newey1994,
	abstract = {Asymptotic distribution theory is the primary method used to examine the properties of econometric estimators and tests. We present conditions for obtaining cosistency and asymptotic normality of a very general class of estimators (extremum estimators). Consistent asymptotic variance estimators are given to enable approximation of the asymptotic distribution. Asymptotic efficiency is another desirable property then considered. Throughout the chapter, the general results are also specialized to common econometric estimators (e.g. MLE and GMM), and in specific examples we work through the conditions for the various results in detail. The results are also extended to two-step estimators (with finite-dimensional parameter estimation in the first step), estimators derived from nonsmooth objective functions, and semiparametric two-step estimators (with nonparametric estimation of an infinite-dimensional parameter in the first step). Finally, the trinity of test statistics is considered within the quite general setting of GMM estimation, and numerous examples are given. {\textcopyright} 1994 Elsevier Science B.V. All rights reserved.},
	author = {Newey, Whitney K. and McFadden, Daniel},
	booktitle = {Handbook of Econometrics},
	chapter = {36},
	doi = {10.1016/S1573-4412(05)80005-4},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newey, McFadden - 1994 - Large Sample Estimation and Hypothesis Testing.pdf:pdf},
	issn = {15734412},
	mendeley-groups = {Limit Theory},
	pages = {2111--2245},
	title = {{Large Sample Estimation and Hypothesis Testing}},
	volume = {4},
	year = {1994}
}
@incollection{Andrews1994,
	author = {Andrews, Donald W.K.},
	booktitle = {Handbook of Econometrics, vol.4},
	chapter = {37},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Limit Theory/Andrews (1994) Chapter 37 Empirical process methods in econometrics.pdf:pdf},
	mendeley-groups = {Limit Theory},
	number = {1},
	publisher = {North-Holland},
	title = {{Empirical Process Methods In Econometrics}},
	year = {1994}
}
@article{Vercammen1994,
	abstract = {r. (I, 176-5)},
	author = {Vercammen, James and van Kooten, GC},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vercammen, Kooten - 1994 - Moral hazard cycles in individual-coverage crop insurance.pdf:pdf},
	journal = {American Journal of Agricultural Economics},
	keywords = {and,crop insurance,differ for each individual,fair,full coverage,given full information,individual coverage,moral hazard cycles,pareto optimal crop in-,provide,surance contracts are actuarially},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {250--261},
	title = {{Moral hazard cycles in individual-coverage crop insurance}},
	url = {http://ajae.oxfordjournals.org/content/76/2/250.short},
	volume = {76},
	year = {1994}
}
@article{Gramlich1994,
	author = {Gramlich, Edward M},
	issn = {00220515},
	journal = {Journal of Economic Literature},
	number = {3},
	pages = {1176--1196},
	publisher = {American Economic Association},
	title = {{Infrastructure Investment: A Review Essay}},
	url = {http://www.jstor.org/stable/2728606},
	volume = {32},
	year = {1994}
}
@article{Campbell1994,
	abstract = {This paper argues that a clear understanding of the stochastic growth model can best be achieved by working out an approximate analytical solution. The proposed solution method replaces the true budget constraints and Euler equations of economic agents with loglinear approximations. The model then becomes a system of loglinear expectational difference equations, which can be solved by the method of undetermined coefficients. The paper uses this technique to study shocks to techno- logy and shocks to government spending financed by lump-sum or distortionary taxation. It emphasizes that the persistence of shocks is an important determinant of their macroeconomic effects. {\textcopyright} 1994.},
	author = {Campbell, John Y.},
	doi = {10.1016/0304-3932(94)90040-X},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell - 1994 - Inspecting the mechanism An analytical approach to the stochastic growth model.pdf:pdf},
	issn = {03043932},
	journal = {Journal of Monetary Economics},
	keywords = {Analytical solution,Loglinear approximation,Stochastic growth model},
	number = {3},
	pages = {463--506},
	title = {{Inspecting the mechanism: An analytical approach to the stochastic growth model}},
	volume = {33},
	year = {1994}
}
@article{Kiviet1995,
	abstract = {When a model for panel data includes lagged dependent explanatory variables, then the habitual estimation procedures are asymptotically valid only when the number of observations in the time dimension (T) gets large. Usually, however, such datasets have substantial sample size in the cross-section dimension (N), whereas T is often a single-digit number. Results on the asymptotic bias (N → ∞) in this situation have been published a decade ago, but, hence far, analytic small sample assessments of the actual bias have not been presented. Here we derive a formula for the bias of the Least-Squares Dummy Variable (LSDV) estimator which has a O(N-1 T- 3 2) approximation error. In a simulation study this is found to be remarkably accurate. Due to the small variance of the LSDV estimator, which is usually much smaller than the variance of consistent (Generalized) Method of Moments estimators, a very efficient procedure results when we remove the bias from the LSDV estimator. The simulations contain results for a particular operational corrected LSDV estimation procedure which in many situations proves to be (much) more efficient than various instrumental variable type estimators. {\textcopyright} 1995.},
	author = {Kiviet, Jan F.},
	doi = {10.1016/0304-4076(94)01643-E},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Kiviet (1995) - On Bias, Efficiency.pdf:pdf},
	isbn = {0304407694016},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Bias correction,Dummy variables estimator,Dynamic panel data model,GMM estimators,Monte Carlo simulation,Unobserved heterogeneity},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {53--78},
	title = {{On Bias, Inconsistency, and Efficiency of Various Estimators in Dynamic Panel Data Models}},
	volume = {68},
	year = {1995}
}
@article{Andrews1995,
	author = {Andrews, Donald W.K.},
	doi = {10.1017/S0266466600009427},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Andrews (1995) Nonparametric Kernel Estimation for Semiparametric Models.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Semiparametric Estimation},
	pages = {560--586},
	title = {{Nonparametric Kernel Estimation For Semiparametric Models}},
	volume = {11},
	year = {1995}
}
@article{Pesaran1995,
	abstract = {In panel data four procedures are widely used: pooling, aggregating, averaging group estimates, and cross-section regression. In the static case, if the coefficients differ ran- domly, all four procedures give unbiased estimates of coefficient means. In the dynamic case, when the coefficients differ across groups, pooling and aggregating give inconsistent and potentially highly misleading estimates of the coefficients, though the cross-section can provide consistent estimates of the long-run effects. The theoretical results on the properties of the four procedures are illustrated by UK labour demand functions for 38 industries over 30 years. Key},
	author = {Pesaran, M. Hashem and Smith, Ron P.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Large Panels/Pesaran, Smith (1995) LR relationships.pdf:pdf},
	isbn = {0304407694016},
	issn = {0045-9801},
	journal = {Journal of Econometrics},
	keywords = {data fields,dynamic panels,parameter heterogeneity,sectoral employment},
	mendeley-groups = {Panel Data/Large Panels,Panel Data/Heterogeneous Panels},
	pages = {473--477},
	title = {{Estimating long-run relationships from dynamic heterogeneous panels}},
	volume = {6061},
	year = {1995}
}
@article{Arellano1995,
	author = {Arellano, Manuel and Bover, Olympia},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Arellano, Bover (1995) Another Look at IV.pdf:pdf},
	isbn = {0304407694016},
	journal = {Journal of Econometrics},
	keywords = {orthogonal,variables},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {29--51},
	title = {{Another Look at the Instrumental Variable Estimation of Error-Components Models}},
	volume = {68},
	year = {1995}
}
@article{Pakes1995,
	abstract = {We consider an econometric model based on a set of moment conditions which are indexed by both a finite-dimensional vector of interest, $\theta$, and an infinite-dimensional parameter, h, which in turn depends upon both $\theta$ and another infinite-dimensional parameter, $\tau$. The population moment conditions equal zero at $\theta$ = $\theta$0. Estimators of $\theta$0 are obtained by forming nonparametric estimates of h and $\tau$, substituting them into the sample analog of the moment conditions, and choosing that value of $\theta$ that makes the sample moments as 'close as possible' to zero. Using independence and smoothness assumptions the paper provides consistency, √n consistency, and asymptotic normality proofs for the resultant estimator. As an example, we consider Olley and Pakes' (1991) use of semiparametric techniques to control for both simultaneity and selection biases in estimating production functions. The example illustrates how semiparametric techniques an be used to overcome both computational problems and the need for strong functional form restrictions in obtaining estimates from structural models. It also illustrates the impacts of 1. (i) alternative estimators for the nonparametric components of the problem and 2. (ii) alternative estimators for the standard errors of the estimated $\theta$. {\textcopyright} 1995.},
	author = {Pakes, Ariel and Olley, Steven},
	doi = {10.1016/0304-4076(94)01605-Y},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Pakes, Olley (1995) A limit theorem for a smooth class of semiparametric estimators.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Selection and simultaneity biases in production fu,Semiparametric m-estimators},
	mendeley-groups = {Semiparametric Estimation},
	number = {1},
	pages = {295--332},
	title = {{A limit theorem for a smooth class of semiparametric estimators}},
	volume = {65},
	year = {1995}
}
@article{Ahn1995,
	abstract = {In this paper we consider a dynamic model for panel data. We show that, under standard assumptions, there are more moment conditions than are currently exploited in the literature. Some of these are linear, but others are quadratic, so that nonlinear GMM is required. We also show that exogenous regressors generate a larger number of relevant moment conditions in a dynamic model than they would in a static model. {\textcopyright} 1995.},
	author = {Ahn, Seung Chan and Schmidt, Peter},
	doi = {10.1016/0304-4076(94)01641-C},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Ahn, Schmidt (1995) -- Efficient Estimation of Dynamic Panels.pdf:pdf},
	isbn = {0304407694016},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Dynamic model,Fixed effects,Panel data,Random effects},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {5--27},
	title = {{Efficient Estimation of Models for Dynamic Panel Data}},
	volume = {68},
	year = {1995}
}
@article{Rilstone1996,
	abstract = {Despite the now widespread use of nonlinear estimators, their finite-sample properties have received very little attention in either the statistics or econometrics literature. We partially redress this problem by deriving and examining the second-order bias and mean squared error of a fairly wide class of nonlinear estimators which includes Nonlinear Least Squares, Maximum Likelihood, and many Generalized Method of Moments estimators as special cases. A number of examples are provided. The results from a Monte Carlo exercise demonstrate how the results can be applied for improved inferences.},
	author = {Rilstone, Paul and Srivastava, V. K. and Ullah, Aman},
	doi = {10.1016/0304-4076(96)89457-7},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Rilstone, Srivastava, Ullah (1996) The second-order bias and mean squared error of nonlinear estimators.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Higher-order moments,Stochastic expansions},
	mendeley-groups = {Small Sample Results/Nonlinear},
	number = {2},
	pages = {369--395},
	title = {{The Second-Order Bias and Mean Squared Error of Nonlinear Estimators}},
	volume = {75},
	year = {1996}
}
@article{Huggett1996,
	author = {Huggett, Mark},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huggett - 1996 - Wealth distribution in life-cycle economies.pdf:pdf},
	journal = {Journal of Monetary Economics},
	keywords = {d30,el3,jel classification,wealth distribution},
	mendeley-groups = {Trash},
	pages = {469--494},
	title = {{Wealth distribution in life-cycle economies}},
	volume = {38},
	year = {1996}
}
@article{Smith1996,
	abstract = {An econometric analysis of the demand for multiple peril crop insurance is carried out for a sample of 370 Montana wheat farms. The study is the first to model the farm's participation and coverage-level decisions separately through Heckman two-stage estimation procedures. These decisions are shown to be determined in different ways. In addition, farms with positive expected returns from insurance make coverage-level decisions in different ways from farms with negative expected returns. These differences indicate that adverse selection effects limit the efficacy of across-the-board premium rate increases as mechanisms for reducing loss ratios.},
	author = {Smith, Vincent H. and Baquet, Alan E.},
	doi = {10.2307/1243790},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith, Baquet - 1996 - The Demand for Multiple Peril Crop Insurance Evidence from Montana Wheat Farms.pdf:pdf},
	issn = {0002-9092},
	journal = {American Journal of Agricultural Economics},
	keywords = {adverse selection,ance,crop insurance,demand,eral crop insurance corporation,fcic,fed-,has re-,model specification,mpci,peril crop insur-,program operated by the,s,the federally subsidized multiple,u},
	mendeley-groups = {My Research/Theta/Review},
	number = {1},
	pages = {189--201},
	title = {{The Demand for Multiple Peril Crop Insurance: Evidence from Montana Wheat Farms}},
	url = {http://ajae.oxfordjournals.org/content/78/1/189.abstract},
	volume = {78},
	year = {1996}
}
@misc{Tibshirani1996,
	abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
	archivePrefix = {arXiv},
	arxivId = {1369–7412/11/73273},
	author = {Tibshirani, Robert},
	booktitle = {Journal of the Royal Statistical Society B},
	doi = {10.2307/2346178},
	eprint = {11/73273},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tibshirani - 1996 - Regression Selection and Shrinkage via the Lasso.pdf:pdf},
	isbn = {0849320240},
	issn = {00359246},
	number = {1},
	pages = {267--288},
	pmid = {16272381},
	primaryClass = {1369–7412},
	title = {{Regression Selection and Shrinkage via the Lasso}},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
	volume = {58},
	year = {1996}
}
@article{Tirupattur1996,
	author = {Tirupattur, Viswanath and Wayne, Fort and Hauser, Robert J},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tirupattur, Wayne, Hauser - 1996 - Crop Yield and Price Distributional Effects on Revenue Hedging.pdf:pdf},
	journal = {OFOR Paper},
	mendeley-groups = {My Research/Theta/Review},
	number = {96},
	pages = {1--18},
	title = {{Crop Yield and Price Distributional Effects on Revenue Hedging}},
	year = {1996}
}
@article{Young1996,
	abstract = {Changing patterns of demand for agricultural products have prompted agricultural economists to consider the causes of such changes. However, the standard theory of consumer behaviour which forms the basis of their analyses is arguably ill-designed to deal with such issues. It is argued that there are some fundamental deficiencies with the conventional approach to consumer behaviour. Many of these arise from the reliance upon a particular conception (or construct) of the individual within mainstream theory, which is open to severe criticism from social theory/ philosophy. Some illustrations of these problems, which are encountered when attempting to explain changes in preferences, are discussed and the importance of alternative approaches is suggested. The implications for modelling changes in demand are indicated and it is suggested that agricultural economists should either limit the types of questions which they ask or give serious consideration to alternative approaches.},
	annote = {10.1093/erae/23.3.281},
	author = {Young, David},
	doi = {10.1093/erae/23.3.281},
	journal = {European Review of Agricultural Economics},
	mendeley-groups = {My Research/Alpha},
	month = {jan},
	number = {3},
	pages = {281--300},
	title = {{Changing tastes and endogenous preferences: Some issues in modelling the demand for agricultural products}},
	url = {http://erae.oxfordjournals.org/content/23/3/281.abstract},
	volume = {23},
	year = {1996}
}
@article{Smith1996,
	abstract = {This study examines the relationship between chemical input use and crop insurance purchase decisions for a sample of Kansas dryland wheat farmers. Recent research by Horowitz and Lichtenberg indicated that, contrary to conventional wisdom, farmers that purchased insurance tended to use relatively more chemical inputs than farmers who did not insure. In contrast, our results confirm the conventional view that moral hazard incentives lead insured farmers to use fewer chemical inputs. Implications for the joint determination of insurance and input use decisions and appropriate estimation techniques are discussed.},
	author = {Smith, Vincent H. and Goodwin, Barry K.},
	doi = {10.2307/1243714},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith, Goodwin - 1996 - Crop Insurance, Moral Hazard and Agricultural Chemical Use.pdf:pdf},
	isbn = {00029092},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {crop insurance,input use,moral hazard},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {428--438},
	title = {{Crop Insurance, Moral Hazard and Agricultural Chemical Use}},
	volume = {78},
	year = {1996}
}
@book{VanderVaart1996,
	author = {{Van der Vaart}, Aad and Wellner, Jon A.},
	doi = {10.1007/978-1-4757-2545-2},
	isbn = {978-1-4757-2547-6},
	mendeley-groups = {Random Useful Math/Probability},
	publisher = {Springer},
	title = {{Weak Convergence and Empirical Processes}},
	year = {1996}
}
@article{Sin1996,
	abstract = {We consider penalized likelihood criteria for selecting models of dependent processes. The models may be strictly nested, overlapping or nonnested, linear or nonlinear, and correctly specified or misspecified. We provide sufficient conditions on the penalty to guarantee the selection, with probability one (or with probability approaching one), of the model attaining the lower average Kullback-Leibler Information Criterion (KLIC) or, when both have the same KLIC, the more parsimonious model. As special cases, our results describe the Akaike, Schwarz, and Hannan-Quinn information criteria. As examples, we consider selection of ARMAX-GARCH and STAR models.},
	author = {Sin, Chor Yiu and White, Halbert},
	doi = {10.1016/0304-4076(94)01701-8},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Sin, White  (1996) Information criteria for selecting possibly misspecified parametric models.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Akaike information criterion,Hannan-Quinn information criterion,Kullback-Leibler information criterion,Penalized likelihood,Schwarz information criterion},
	mendeley-groups = {Model Selection and Averaging},
	number = {1-2},
	pages = {207--225},
	title = {{Information criteria for selecting possibly misspecified parametric models}},
	volume = {71},
	year = {1996}
}
@article{Olley1996,
	abstract = {Technological change and deregulation have caused a major restructuring of the telecommunications equipment industry over the last two decades. Our empirical focus is on estimating the parameters of a production function for the equipment industry, and then using those estimates to analyze the evolution of plant-level productivity. The restructuring involved significant entry and exit and large changes in the sizes of incum bents. Firms' choices on whether to liquidate, and on input quantities should they continue, depended on their productivity. This generates a selection and a simultaneity problem when estimating production functions. Our theoretical focus is on providing an estimation algorithm which takes explicit account of these issues. We find that our algorithm produces markedly different and more plausible estimates of production func tion coefficients than do traditional estimation procedures. Using our estimates we find increases in the rate of aggregate productivity growth after deregulation. Since we have plant-level data we can introduce indices which delve deeper into how this productivity growth occurred. These indices indicate that productivity increases were primarily a result of a reallocation of capital towards more productive establishments.},
	author = {Olley, G. Steven and Pakes, Ariel},
	doi = {10.2307/2171831},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Olley, Pakes (1996) Dynamics of Productivity In Telecoms.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {6},
	pages = {1263},
	title = {{The Dynamics of Productivity in the Telecommunications Equipment Industry}},
	volume = {64},
	year = {1996}
}
@article{Cameron1990,
	abstract = {Using the AIDS model, we show that there exists for the UK a stable long-run relationship between expenditure shares on beer, cider, spirits and wine, alcohol prices, total alcohol expenditure and a range of non-economic variables relating to advertising, licensing, the employment, social class and demographic characteristics of consumers, and climate. Our estimates of key price and income elasticities generally lie between those found from other time-series studies (which exclude most of these non-economic variables) and those found from cross-section studies (which generally include them). However, the restrictions required for separability, homegeneity and symmetry (although not those for perfect price aggregation) are decisively rejected.},
	author = {Blake, David and Nied, Angelika},
	doi = {10.1080/00036849700000041},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blake, Nied - 1997 - The Demand for Alcohol in the United Kingdom.pdf:pdf},
	journal = {Applied Economics},
	mendeley-groups = {My Research/Zeta},
	number = {12},
	pages = {1655--1672},
	title = {{The Demand for Alcohol in the United Kingdom}},
	volume = {29},
	year = {1997}
}
@article{Champ1997,
	abstract = {This paper discusses an experiment in which the value for an unfamiliar environmental good, whose total value has a large nonuse component, is verified using a revealed-preference method. As we were unable to observe preferences via an incentive compatible mechanism, we collected voluntary contributions toward the provision of the good. We make a case for interpreting these contributions as a “theoretical lower bound” on the value of the public good and estimate the lower bound. We also investigate whether we can use contingent donation data to estimate such lower bounds on values. We used a follow-up to the contingent donation question about the respondent's level of certainty with respect to her response to the contingent donation question. The results of this study suggest that use of the follow-up certainty question to differentiate respondents who would actually donate from those who would not is a promising approach to estimating a lower bound to Hicksian surplus measures.},
	author = {Champ, Patricia A and Bishop, Richard C and Brown, Thomas C and McCollum, Daniel W},
	doi = {http://dx.doi.org/10.1006/jeem.1997.0988},
	issn = {0095-0696},
	journal = {Journal of Environmental Economics and Management},
	number = {2},
	pages = {151--162},
	title = {{Using Donation Mechanisms to Value Nonuse Benefits from Public Goods}},
	url = {http://www.sciencedirect.com/science/article/pii/S0095069697909888},
	volume = {33},
	year = {1997}
}
@article{10.2307/2950855,
	abstract = {This paper measures the effect on the federal funds rate of an open-market operation. The paper deals with simultaneous-equations bias by developing a proxy for the errors the Federal Reserve makes in forecasting the extent to which Treasury operations will add or drain reserves available to private banks. These errors induce fluctuations in bank reserves which have measurable consequences for the federal funds rate. The paper estimates that a reduction in nonborrowed reserves of 30 million, if sustained for an entire 14-day reserve maintenance period, will cause the federal funds rate to rise by 10 basis points.},
	author = {Hamilton, James D},
	issn = {00028282},
	journal = {American Economic Review},
	number = {1},
	pages = {80--97},
	publisher = {American Economic Association},
	title = {{Measuring the Liquidity Effect}},
	url = {http://www.jstor.org/stable/2950855},
	volume = {87},
	year = {1997}
}
@article{Miranda1997,
	abstract = {Without affordable reinsurance, private crop insurance markets are doomed to fail because systemic weather effects induce high correlation among farm-level yields, defeating insurer efforts to pool risks across farms. Using an empirical model of the U.S. crop insurance market, we find that U.S. crop insurer portfolios are twenty to fifty times riskier than they would be otherwise if yields were stochastically independent across farms. We also find that area yield reinsurance contracts would enable crop insurers to cover most of their systemic crop loss risk, reducing their risk exposure to levels typically experienced by more conventional property liability insurers.},
	author = {Miranda, Mario J and Glauber, Joseph W.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miranda, Glauber - 1997 - Systemic Risk, Reinsurance, and the Failure of Crop Insurance Markets.pdf:pdf},
	issn = {0002-9092},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta,My Research/Theta/Review},
	month = {feb},
	number = {1},
	pages = {206--215},
	title = {{Systemic Risk, Reinsurance, and the Failure of Crop Insurance Markets}},
	url = {http://dx.doi.org/10.2307/1243954},
	volume = {79},
	year = {1997}
}
@book{Potscher1997,
	author = {P{\"{o}}tscher, Benedikt M. and Prucha, Ingmar R.},
	isbn = {3662034867},
	mendeley-groups = {Limit Theory},
	publisher = {Springer},
	title = {{Dynamic Nonlinear Econometric Models: Asymptotic Theory}},
	year = {1997}
}
@article{Maddala1997,
	author = {Maddala, G. S. and Trost, Robert P. and Li, Hongyi and Joutz, Frederick},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Maddala et al. (1997) Estimation of Short-Run and Long-Run Elasticities of Energy Demand From Panel Data Using Shrinkage Estimators.pdf:pdf},
	journal = {Journal of Business and Economic Statistics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {90--100},
	title = {{Estimation of Short-Run and Long-Run Elasticities of Energy Demand From Panel Data Using Shrinkage Estimators}},
	volume = {15},
	year = {1997}
}
@article{Wooldridge1997,
	author = {Wooldridge, Jeffrey M.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wooldridge - 1997 - On two stage least squares estimation of the average treatment effect in a random coefficient model.pdf:pdf},
	journal = {Economics Letters},
	keywords = {average treatment effect,c21,jel classification,random coefficient,two stage least squares,unobserved heterogeneity},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects},
	pages = {129--133},
	title = {{On two stage least squares estimation of the average treatment effect in a random coefficient model}},
	volume = {56},
	year = {1997}
}
@article{Acemoglu1997,
	author = {Acemoglu, Daron and Zilibotti, Fabrizio},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Acemoglu, Zilibotti - 1997 - Was Prometheus Unbound by Chance Risk , Diversification , and Growth.pdf:pdf},
	journal = {Journal of Political Economy},
	mendeley-groups = {UPF/Adv. Macro III},
	number = {4},
	pages = {709--751},
	title = {{Was Prometheus Unbound by Chance? Risk , Diversification , and Growth}},
	volume = {105},
	year = {1997}
}
@article{Skees1997,
	abstract = {This article documents the design and rate-making procedures used in the development of the Group Risk Plan (GRP)-the new federal crop insurance product that insures based on area yield. The authors of this article worked closely with personnel in the Federal Crop insurance Corporation and others in developing methodological and practical constraints needed in implementing a workable area yield contract. GRP indemnity payments are made based on percentage shortfalls in actual county yields relative to a forecasted yield. Historical county yield data are used to develop forecasted yields and premium rates},
	author = {Skees, Jerry R and Black, J Roy and Barnett, Barry J.},
	doi = {10.2307/1244141},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skees, Black, Barnett - 1997 - Designing and Rating an Area Yield Crop Insurance Contract.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {1949,agricultural policy,area yield,area yield insurance,crop insurance,first such article in,halcrow published the,miranda revisited the,several articles on,this journal has published},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {430--438},
	title = {{Designing and Rating an Area Yield Crop Insurance Contract}},
	volume = {79},
	year = {1997}
}
@unpublished{Varian1997,
	author = {Varian, Hal R.},
	doi = {10.1108/14684520910985729},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varian - 1997 - Versioning information goods.pdf:pdf},
	institution = {University of California, Berkeley},
	isbn = {978-0-262-09041},
	issn = {1468-4527},
	mendeley-groups = {My Research/Alpha},
	number = {January},
	pages = {1--13},
	title = {{Versioning information goods}},
	url = {https://www-inst.cs.berkeley.edu/{~}eecsba1/sp97/reports/eecsba1b/Final/version.pdf},
	year = {1997}
}
@article{Kiyotaki1997,
	abstract = {We construct a model of a dynamic economy in which lenders can- not force borrowers to repay their debts unless the debts are se- cured. In such an economy, durable assets play a dual role: not only are they factors of production, but they also serve as collateral for loans. The dynamic interaction between credit limits and asset prices turns out to be a powerful transmission mechanism by which the effects of shocks persist, amplify, and spill over to other sectors. We show that small, temporary shocks to technology or income distribution can generate large, persistent fluctuations in output and asset prices. I.},
	author = {Kiyotaki, Nobuhiro and Moore, John},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiyotaki, Moore - 1997 - Credit Cycles.pdf:pdf},
	journal = {Journal of Political Economy},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {2},
	pages = {211--248},
	title = {{Credit Cycles}},
	volume = {105},
	year = {1997}
}
@inproceedings{Rao1997,
	author = {Rao, J. Sunil and Potts, William J.E.},
	booktitle = {Proceedings of the Third International Conference on Knowledge Discovery and Data Mining},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rao, Potts - 1997 - Visualizing Bagged Decision Trees.pdf:pdf},
	pages = {243--246},
	title = {{Visualizing Bagged Decision Trees}},
	url = {http://www.aaai.org/Papers/KDD/1997/KDD97-050.pdf},
	year = {1997}
}
@article{Kenc1997,
	author = {Kenc, Turalay and Perraudin, William},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kenc, Perraudin - 1997 - European Pension Systems A Sim u latiun A nuly s is.pdf:pdf},
	mendeley-groups = {My Research/Pensions},
	number = {3},
	pages = {249--277},
	title = {{European Pension Systems : A Sim u latiun A nuly s is}},
	volume = {18},
	year = {1997}
}
@book{Joe1997,
	abstract = {This book on multivariate models, statistical inference, and data analysis contains deep coverage of multivariate non-normal distributions for modeling of binary, count, ordinal, and extreme value response data. It is virtually self-contained, and includes many exercises and unsolved problems.},
	author = {Joe, Harry},
	booktitle = {Analysis},
	doi = {Export Date 21 May 2013},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joe - 1997 - Multivariate models and dependence concepts.pdf:pdf},
	isbn = {0412073315},
	number = {1960},
	pages = {418},
	pmid = {4019430},
	title = {{Multivariate models and dependence concepts}},
	year = {1997}
}
@article{Blundell1998,
	author = {Blundell, Richard and Bond, Stephen},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Blundell, Bond (1998) System GMM.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {dynamic panel data,error components,initial condi-,weak instruments},
	mendeley-groups = {Panel Data/Dynamic Panels},
	pages = {115--143},
	title = {{Initial Conditions and Moment Restrictions in Dynamic Panel Data Models}},
	volume = {87},
	year = {1998}
}
@article{Heckman1998,
	abstract = {This paper considers the use of instrumental variables to identify a correlated random coefficients model in which coefficients are correlated with (or stochastically dependent on) the regressors. A correlated random coefficients model is central to the human capital earnings model. Conditions are given under which instrumental variables identify the average rate of return. These conditions are applied to David Card's version of Gary Becker's Woytinsky lecture.},
	author = {Heckman, James and Vytlacil, Edward},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Heckman, Vytlacil (1998) Instrumental Variables Methods for the Correlated Random Coefficient Model.pdf:pdf},
	journal = {Journal of Human Resources},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {4},
	pages = {974--987},
	title = {{Instrumental variables methods for the correlated random coefficient model}},
	volume = {33},
	year = {1998}
}
@article{Azariadis1998,
	author = {Azariadis, Costas and Smith, Bruce},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azariadis, Smith - 1998 - Financial Intermediation and Regime Switching in Business Cycles.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {3},
	pages = {516--536},
	title = {{Financial Intermediation and Regime Switching in Business Cycles}},
	url = {http://www.jstor.org/stable/116847},
	volume = {88},
	year = {1998}
}
@book{Pagan1999,
	abstract = {This book systematically and thoroughly covers the vast literature on the nonparametric and semiparametric statistics and econometrics that has evolved over the last five decades. Within this framework this is the first book to discuss the principles of the nonparametric approach to the topics covered in a first year graduate course in econometrics, e.g. regression function, heteroskedasticity, simultaneous equations models, logit-probit and censored models. Nonparametric and semiparametric methods potentially offer considerable reward to applied researchers, owing to the methods' ability to adapt to many unknown features of the data. Professors Pagan and Ullah provide intuitive explanations of difficult concepts, heuristic developments of theory, and empirical examples emphasizing the usefulness of the modern nonparametric approach. The book should provide a new perspective on teaching and research in applied subjects in general and econometrics and statistics in particular.},
	address = {Cambridge},
	author = {Pagan, Adrian and Ullah, Aman},
	booktitle = {Themes in Modern Econometrics},
	doi = {DOI: 10.1017/CBO9780511612503},
	isbn = {9780521355643},
	mendeley-groups = {Nonparametric Estimation,Nonparametric Estimation/Books and Notes},
	publisher = {Cambridge University Press},
	title = {{Nonparametric Econometrics}},
	url = {https://www.cambridge.org/core/books/nonparametric-econometrics/554113E1A7283B03AF28A1D6D28CB452},
	year = {1999}
}
@incollection{Hsiao1999,
	author = {Hsiao, Cheng and Pesaran, M. Hashem and Tahmiscioglu, A. Kamil},
	booktitle = {Analysis of Panels and Limited Dependent Variable Models,},
	doi = {10.1017/cbo9780511493140.013},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Hsiao, Pesaran, Tahmiscioglu (1999) Bayes Estimation of Short-Run Coefficients in Dynamic Panel Data Models.pdf:pdf},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {268--296},
	title = {{Bayes Estimation of Short-Run Coefficients in Dynamic Panel Data Models}},
	year = {1999}
}
@article{Pesaran1999,
	abstract = {It is now quite common to have panels in which both T, the number of time series observations, and N, the number of groups, are quite large and of the same order of magnitude. The usual practice is either to estimate N separate regressions and calculate the coefficient means, which we call the mean group (MG) estimator, or to pool the data and assume that the slope coefficients and error variances are identical. In this article we propose an intermediate procedure, the pooled mean group (PMG) estimator, which constrains long-run coefficients to be identical but allows short-run coefficients and error variances to differ across groups. We consider both the case where the regressors are stationary and the case where they follow unit root processes, and for both cases derive the asymptotic distribution of the PMG estimators as T tends to infinity. We also provide two empirical applications: Aggregate consumption functions for 24 Organization for Economic Cooperation and Development economies over the period 1962–1993, and energy demand functions for 10 Asian developing economies over the period 1974–1990. {\textcopyright} 1999 Taylor {\&} Francis Group, LLC.},
	author = {Pesaran, M. Hashem and Shin, Yongcheol and Smith, Ron P.},
	doi = {10.1080/01621459.1999.10474156},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Pesaran, Shin, Smith (1999) Pooled Mean Group Estimation of Dynamic Heterogeneous Panels.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Consumption functions,Energy demand,Heterogeneous dynamic panels,I(0) and I(1) regressors,Pooled mean group estimator},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {446},
	pages = {621--634},
	title = {{Pooled Mean Group Estimation of Dynamic Heterogeneous Panels}},
	volume = {94},
	year = {1999}
}
@article{Just1999,
	abstract = {Adverse selection is often blamed for crop insurance indemnities exceeding premiums plus subsidies. However, nationwide empirical evidence has been lacking or based on inadequate county-level data. This article uses nationwide farm-level data on corn and soybeans to decompose incentives for participation in U.S. multiple peril crop insurance into a risk-aversion incentive (the conventional justification for insurance), an actuarial or subsidy incentive (reflecting government subsidization), and an asymmetric information incentive (which reflects farmers' information advantage). Results show that the risk-aversion incentive is small. Farmers participate in crop insurance primarily to receive the subsidy or because of adverse selection possibilities. },
	annote = {10.2307/1244328},
	author = {Just, Richard E and Calvin, Linda and Quiggin, John},
	doi = {10.2307/1244328},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Just, Calvin, Quiggin - 1999 - Adverse Selection in Crop Insurance Actuarial and Asymmetric Information Incentives.pdf:pdf},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta,My Research/Theta/Review},
	month = {nov},
	number = {4 },
	pages = {834--849},
	title = {{Adverse Selection in Crop Insurance: Actuarial and Asymmetric Information Incentives}},
	url = {http://ajae.oxfordjournals.org/content/81/4/834.abstract},
	volume = {81 },
	year = {1999}
}
@article{Skees1999,
	author = {Skees, Jerry and Hazell, Peter and Miranda, Mario J},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skees, Hazell, Miranda - 1999 - New Approaches To Crop Yield Insurance in Developing Countries.pdf:pdf},
	journal = {International Food Policy Research Institute Environment and Production Technology Discussion Paper},
	mendeley-groups = {My Research/Theta/Review},
	number = {55},
	title = {{New Approaches To Crop Yield Insurance in Developing Countries}},
	year = {1999}
}
@article{Phillips1999,
	abstract = {This paper develops a regression limit theory for nonstationary panel data with large numbers of cross section (n) and time series (T) observations. The limit theory allows for both sequential limits, wherein T c-oo followed by n -* oo, and joint limits where T, n -c00 simultaneously; and the relationship between these multidimensional limits is explored. The panel structures considered allow for no time series cointegration, heterogeneous cointegration, homogeneous cointegration, and near-homogeneous cointegration. The paper explores the existence of long-run average relations between integrated panel vectors when there is no individual time series cointegration and when there is heteroge- neous cointegration. These relations are parameterized in terms of the matrix regression coefficient of the long-run average covariance matrix. In the case of homogeneous and near homogeneous cointegrating panels, a panel fully modified regression estimator is developed and studied. The limit theory enables us to test hypotheses about the long run average parameters both within and between subgroups of the full population.},
	author = {Phillips, Peter and Moon, Hyungsik Roger},
	doi = {10.1111/1468-0262.00070},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Large Panels/Phillips, Moon (1999) Linear regression limit theory for nonstationary panel data.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Long-run average relations,Multidimensional limits,Nonstationary panel data,Panel cointegration regression,Panel spurious regression},
	mendeley-groups = {Panel Data/Large Panels},
	number = {5},
	pages = {1057--1111},
	title = {{Linear regression limit theory for nonstationary panel data}},
	volume = {67},
	year = {1999}
}
@article{Agrawal1999,
	abstract = {The poor conservation outcomes that followed decades of intrusive resource management strategies and planned development have forced policy makers and scholars to reconsider the role of community in resource use and conservation. In a break from previous work on development which considered communities a hindrance to progressive social change, current writings champion the role of community in bringing about decentralization, meaningful participation, and conservation. But despite its recent popularity, the concept of community is rarely defined or carefully examined by those concerned with resource use and management. We seek to redress this omission by investigating 'community' in work concerning resource conservation and management. We explore the conceptual origins of the community, and the ways the term has been deployed in writings on resource use. We then analyze those aspects of community most important to advocates for community's role in resource management - community as a small spatial unit, as a homogeneous social structure, and as shared norms - and indicate the weaknesses of these approaches. Finally, we suggest a more political approach: community must be examined in the context of development and conservation by focusing on the multiple interests and actors within communities, on how these actors influence decision-making, and on the internal and external institutions that shape the decision-making process. A focus on institutions rather than 'community' is likely to be more fruitful for those interested in community-based natural resource management.},
	author = {Agrawal, Arun and Gibson, Clark C.},
	doi = {10.1016/S0305-750X(98)00161-2},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Gibson - 1999 - Enchantment and disenchantment The role of community in natural resource conservation.pdf:pdf},
	isbn = {0305-750X},
	issn = {0305750X},
	journal = {World Development},
	number = {4},
	pages = {629--649},
	pmid = {495177},
	title = {{Enchantment and disenchantment: The role of community in natural resource conservation}},
	volume = {27},
	year = {1999}
}
@article{Sarlio-Lahteenkorva1999,
	abstract = {BACKGROUND: Although an inverse relationship between socioeconomic status and body mass index (BMI) is well documented, broad population studies focusing on the association between BMI and various forms of disadvantage such as unemployment, low income or social isolation are rare. METHODS: A nationwide, representative sample of 25-64-year-old Finnish subjects (n = 6016) was classified according to their BMI into four groups: 'thin' (BMI {\textless} 20), 'normal' (BMI 20-24.9), 'overweight' (BMI 25-29.9) and 'obese' (BMI {\textgreater} or = 30). Multivariable analyses using logistic regression were conducted with this BMI-grouping as an independent variable to predict social and economic disadvantage, controlling simultaneously for age, educational attainment, region of residence, and limiting long-standing illness. RESULTS: In women, overweight was associated with current unemployment and obesity with long-term unemployment as well as absence of close friends outside the family circle. Both overweight and obesity were associated with low individual earnings. Obese women were also most likely to have low household disposable and individual incomes; a similar pattern was seen among thin women. A small subgroup of thin men were socially and economically disadvantaged with all our indicators whereas excess body weight was not problematic for men. CONCLUSIONS: Deviant body weight is associated with social and economic disadvantage in a gender-specific and partly curvilinear way. In particular, obese women face multiple social and economic disadvantage.},
	author = {Sarlio-L{\"{a}}hteenkorva, S and Lahelma, E},
	doi = {10.1093/ije/28.3.445},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarlio-L{\"{a}}hteenkorva, Lahelma - 1999 - The association of body mass index with social and economic disadvantage in women and men.pdf:pdf},
	isbn = {0300-5771 (Print)$\backslash$r0300-5771 (Linking)},
	issn = {0300-5771},
	journal = {International journal of epidemiology},
	mendeley-groups = {My Research/Zeta},
	number = {3},
	pages = {445--449},
	pmid = {10405846},
	title = {{The association of body mass index with social and economic disadvantage in women and men.}},
	volume = {28},
	year = {1999}
}
@incollection{Stock1999,
	author = {Stock, James H and Watson, Mark W},
	booktitle = {Cointegration, Causality and Forecasting: A Festschrift for Clive W.J. Granger},
	editor = {Engle, Robert F. and White, Halbert},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Stock, Watson (2001) A comparison of linear and nonlinear univariate models for forecasting.pdf:pdf},
	mendeley-groups = {Forecasting/Forecast Combination},
	pages = {1--44},
	publisher = {Oxford University Press},
	title = {{A Comparison of Linear and Nonlinear Univariate Models for Forecasting Macroeconomic Time Series}},
	year = {1999}
}
@article{Merlo2000,
	abstract = {This paper considers public goods and/or economic externalities (off-site and off-market effects) of Mediterranean forests, referred as MEDFOREXs. It also reports possible market failures and/or distortions. MEDFOREX can be valued in monetary terms as already undertaken in several Mediterranean forest sites. Multifunctional indicators are also being developed. Policy tools aimed at promotion of positive MEDFOREX, and prevention of negative ones, can be classified as mandatory or voluntary. Mandatory tools have been much used in the Mediterranean context, and now represent the core of forest policies. More recently financial or economic tools, mainly based on payments, and internalisation of externalities, have emerged. Also market options are now envisaged and applied as shown by recent surveys. Policy effectiveness is improved by the use of a combined set of tools. The most appropriate mix has not yet been formalised and this represents a policy challenge.},
	author = {Merlo, Maurizio and {Rojas Briales}, Eduardo},
	doi = {http://dx.doi.org/10.1016/S0264-8377(00)00017-X},
	issn = {0264-8377},
	journal = {Land Use Policy},
	keywords = {Externalities,Mediterranean forests,Policy tools,Public goods},
	month = {jul},
	number = {3},
	pages = {197--208},
	title = {{Public goods and externalities linked to Mediterranean forests: economic nature and policy}},
	url = {http://www.sciencedirect.com/science/article/pii/S026483770000017X},
	volume = {17},
	year = {2000}
}
@book{Severini2000,
	author = {Severini, Thomas A.},
	isbn = {9780198506508},
	mendeley-groups = {Macrofinance/Maximum Likelihood},
	pages = {1--392},
	publisher = {Oxford University Press},
	title = {{Likelihood Methods in Statistics}},
	year = {2000}
}
@article{Cheng2000,
	abstract = {Overcoming the potential dilemma of awarding the same grade to a group of students for group work assignments, regardless of the contribution made by each group member, is a problem facing teachers who ask their students to work collaboratively together on assessed group tasks. In this paper, we report on the procedures to factor in the contributions of individual group members engaged in an integrated group project using peer assessment procedures. Our findings demonstrate that the method we used resulted in a substantially wider spread of marks being given to individual students. Almost every student was awarded a numerical score which was higher or lower than a simple group project mark would have been. When these numerical scores were converted into the final letter grades, approximately one-third of the students received a grade for the group project that was different from the grade that they would have received if the same grade had been awarded to all group members. Based on these preliminary {\textregistered} ndings we conclude that peer assessment can be usefully and meaningfully employed to factor individual contributions into the grades awarded to students engaged in collaborative group work.},
	author = {Cheng, Winnie and Warren, Martin},
	doi = {10.1080/135625100114885},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Warren - 2000 - Making a Difference Using peers to assess individual students' contributions to a group project.pdf:pdf},
	isbn = {13562517},
	issn = {1356-2517},
	journal = {Teaching in Higher Education},
	number = {2},
	pages = {243--255},
	title = {{Making a Difference: Using peers to assess individual students' contributions to a group project}},
	volume = {5},
	year = {2000}
}
@article{Oliver2000,
	abstract = {Objective: This study investigated experimentally whether acute stress alters food choice during a meal. The study was designed to test claims of selective effects of stress on appetite for specific sensory and nutritional categories of food and interactions with eating attitudes.
	Methods: Sixty-eight healthy men and women volunteered for a study on “the effects of hunger on physiology, performance, and mood.” Eating attitudes and food preferences were measured on entry to the study. The stressed group prepared a 4-minute speech, expecting it to be filmed and assessed after a midday meal, although in fact speeches were not performed. The ad libitum meal included sweet, salty, or bland high- and low-fat foods. The control group listened to a passage of neutral text before eating the meal. Blood pressure, heart rate, mood, and hunger were measured at baseline and after the 10-minute preparatory period, when appetite for 34 foods and food intake were recorded.
	Results: Increases in blood pressure and changes in mood confirmed the effectiveness of the stressor. Stress did not alter overall intake, nor intake of, or appetite for the six food categories. However, stressed emotional eaters ate more sweet high-fat foods and a more energy-dense meal than unstressed and nonemotional eaters. Dietary restraint did not significantly affect appetitive responses to stress.
	Conclusions: Increased eating of sweet fatty foods by emotional eaters during stress, found here in a laboratory setting, may underlie the previously reported finding that dietary restraint or female gender predicts stress-induced eating. Stress may compromise the health of susceptible individuals through deleterious stress-related changes in food choice.},
	author = {Oliver, Georgina and Wardle, Jane and Gibson, E Leigh},
	issn = {0033-3174},
	journal = {Psychosomatic Medicine},
	keywords = {appetite,,dietary restraint,,emotional eating,,food choice,,nutrition.,stress,},
	mendeley-groups = {My Research/Zeta},
	number = {6},
	title = {{Stress and Food Choice: A Laboratory Study}},
	url = {http://journals.lww.com/psychosomaticmedicine/Fulltext/2000/11000/Stress{\_}and{\_}Food{\_}Choice{\_}{\_}A{\_}Laboratory{\_}Study.16.aspx},
	volume = {62},
	year = {2000}
}
@book{Nemirovski2000,
	author = {Nemirovski, Arkadii},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nemirovski - 2000 - Topics in Non-parametric Statistics. Ecole d'Et{\'{e}} de Probabilit{\'{e}}s de Saint-Flour XXVIII.djvu:djvu},
	mendeley-groups = {UPF/Research Seminar},
	publisher = {Springer},
	title = {{Topics in Non-parametric Statistics. Ecole d'Et{\'{e}} de Probabilit{\'{e}}s de Saint-Flour XXVIII}},
	year = {2000}
}
@article{Hansen2000,
	author = {Hansen, Bruce E.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Threshold Models/Hansen (2000) Sample Splitting and Threshold Estimation.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Threshold Models},
	number = {3},
	pages = {575--603},
	title = {{Sample Splitting and Threshold Estimation}},
	volume = {68},
	year = {2000}
}
@incollection{Holly2000,
	author = {Holly, Alberto and Gardiol, Lucien},
	booktitle = {Panel Data Econometrics: Future Directions},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holly, Gardiol - 2000 - A score test for individual heteroscedasticity in one-way error components model.pdf:pdf},
	mendeley-groups = {Panel Data,Panel Data/Heteroskedasticity},
	number = {January},
	pages = {199--211},
	title = {{A score test for individual heteroscedasticity in one-way error components model.}},
	year = {2000}
}
@article{Klooster2000,
	abstract = {Forest management is an important carbon mitigation strategy for developing countries. As demonstrated by the case of Mexico, community forest management is especially effective because it offers tangible local benefits while conserving forests and sequestering carbon. Community forestry receives minimal government support now, but the clean development mechanism (CDM) of the Kyoto Protocol could leverage additional resources to promote the approach in Mexico and elsewhere. We argue that adequately designed and implemented, community forestry management projects can avoid deforestation and restore forest cover and forest density. They comprise promising options for providing both carbon mitigation and sustainable rural development. These kinds of projects should be included in the CDM.},
	author = {Klooster, Daniel and Masera, Omar},
	doi = {http://dx.doi.org/10.1016/S0959-3780(00)00033-9},
	issn = {0959-3780},
	journal = {Global Environmental Change},
	keywords = {Clean Development Mechanism,Climate change,Community forestry,Latin America,Mexico,Mitigation options},
	month = {dec},
	number = {4},
	pages = {259--272},
	title = {{Community forest management in Mexico: carbon mitigation and biodiversity conservation through rural development}},
	url = {http://www.sciencedirect.com/science/article/pii/S0959378000000339},
	volume = {10},
	year = {2000}
}
@article{Ker2000,
	abstract = {With the crop insurance program becoming the cornerstone of U.S. agricultural policy, recovering accurate rates is of paramount interest. Lack of yield data presents, by far, the most fundamental obstacle to recovery of accurate rates. This article employs new methodology to estimate conditional yield densities and derive the insurance rates. In our application, we find the nonparametric kernel density estimator requires an additional twenty-six years of yield data to estimate the shape of the conditional yield densities as accurately as the recently developed empirical Bayes nonparametric kernel density estimator. Such methodological improvements can significantly aid in ameliorating the data problem. Copyright 2000, Oxford University Press.},
	author = {Ker, Alan P. and Goodwin, Barry K.},
	doi = {10.1111/0002-9092.00039},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ker, Goodwin - 2000 - Nonparametric Estimation of Crop Insurance Rates Revisited.pdf:pdf},
	isbn = {0420120033},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	keywords = {a program which,crop insurance program,empirical bayes,insurance rating,is,kernel density estimation,paramount to the actuarial,recovering accurate premium rates,soundness of the},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {463--478},
	pmid = {16792878},
	title = {{Nonparametric Estimation of Crop Insurance Rates Revisited}},
	url = {http://econpapers.repec.org/RePEc:oup:ajagec:v:82:y:2000:i:2:p:463-478{\%}5Cnhttp://ajae.oxfordjournals.org/content/82/2/463.short},
	volume = {82},
	year = {2000}
}
@article{Bell2000,
	author = {Bell, Kathleen P and Bockstael, Nancy E},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bell, Bockstael - 2000 - Applying the Generalized Method of Moments Approach to Spatial Problems Involving Micro-Level Data.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	keywords = {GMM,Maryland,Patuxent,generalized method of moments,spatial dependence,spatial econometrics,spatial weight matrix},
	mendeley-groups = {Spatial Econometrics},
	number = {1},
	pages = {72--82},
	title = {{Applying the Generalized Method of Moments Approach to Spatial Problems Involving Micro-Level Data}},
	volume = {82},
	year = {2000}
}
@article{Juditsky2000,
	author = {Juditsky, Anatoli and Nemirovski, Arkadii},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Juditsky, Nemirovski - 2000 - Functional Aggregation for Nonparametric Regression.pdf:pdf},
	journal = {Annals of Statistics},
	mendeley-groups = {UPF/Research Seminar},
	number = {3},
	pages = {681--712},
	title = {{Functional Aggregation for Nonparametric Regression}},
	volume = {28},
	year = {2000}
}
@incollection{Arellano2001,
	author = {Arellano, Manuel and Honore, Bo},
	booktitle = {Handbook of Econometrics, vol. 5},
	chapter = {53},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Reviews/Arellano, Honore (2001) Panel Data Models - Some Recent Developments.pdf:pdf},
	mendeley-groups = {Panel Data/Books and Reviews,Panel Data/Nonlinear Panels},
	title = {{Panel Data Models: Some Recent Developments}},
	year = {2001}
}
@article{Laffont2001,
	author = {Laffont, Jean-Jacques and Marcus, Scott and Rey, Patrick and Tirole, Jean},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laffont et al. - 2001 - Internet Peering.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {IO},
	number = {2},
	title = {{Internet Peering}},
	volume = {91},
	year = {2001}
}
@book{Arnold2001,
	abstract = {As natural forests diminish in parts of the developing world, rural communities widely establish local stocks of trees for soil conservation and to maintain needed goods and services, such as fuelwood. Growing pressures on the resource base of rural communities are making it increasingly difficult to develop appropriate independent tree management practices not requiring external support. Such support needs to be accurately matched to the particular needs and possibilities of each case. Three community forestry strategies are reviewed-communal forestry, farm forestry for the household, and farm forestry for the market.},
	address = {Rome},
	author = {Arnold, J. E. Mike},
	doi = {10.2307/4313337},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arnold - 2001 - Forests and People 25 years of Community Forestry.pdf:pdf},
	isbn = {9781412956970},
	issn = {00447447},
	pages = {134},
	publisher = {FAO Forestry Papers},
	title = {{Forests and People: 25 years of Community Forestry}},
	url = {http://www.fao.org/docrep/012/y2661e/y2661e00.pdf},
	year = {2001}
}
@article{Chen2001,
	abstract = {This paper presents a semiparametric model for large-dimension vector time series whose elements correspond to economic agents. Dependence between agents' variables is characterized using a spatial model. Functions of agents' economic distances provide restrictions that enable estimation of a vector autoregressive specification. We present sufficient conditions for our model to generate stationary, $\beta$-mixing series with finite higher-order moments. We estimate the model using a simple two-step sieve least-squares procedure, where the sieve estimators are constructed to preserve shape restrictions on the functions of economic distance, e.g., positive definiteness of a covariance function. We provide rates of convergence for the sieve estimators, √T limiting distributions for the model's finite-dimensional parameters, and a bootstrap method for inference. In an illustrative application, we use this model to characterize how the comovement in output growth across US industrial sectors depends on the similarity of sectors' technologies. We also present a small Monte Carlo evaluation of our estimators. {\textcopyright} 2001 Elsevier Science S.A. All rights reserved.},
	author = {Chen, Xiaoheng and Conley, Timothy G.},
	doi = {10.1016/S0304-4076(01)00070-7},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Conley - 2001 - A new semiparametric spatial model for panel time series.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Isotropic conditional covariances,Sieve estimator,Space-time model,Time-varying coefficient VAR},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {1},
	pages = {59--83},
	title = {{A new semiparametric spatial model for panel time series}},
	volume = {105},
	year = {2001}
}
@article{Freeman2001,
	abstract = {The beer industry has traditionally been viewed as `recession-proof ` :$\backslash$ndrinkers will have their beer notwithstanding the ecomomy's ups and$\backslash$ndowns. Few empirical studies have examined the veracity of the claim of$\backslash$nbeer's non-cyclicality and none have used monthly data. This paper uses$\backslash$nan error-correction specification to test the sensitivity o mfonthly$\backslash$nbeer consumption to cyclical macro-economic variables over the period$\backslash$nJanuary 1955-December 1994. Strong support was found for the traditional$\backslash$nview: beer is mostly immune to economic cycles.},
	author = {Freeman, Donald},
	doi = {10.1080/135048501750041295},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freeman - 2001 - Beer and the business cycle.pdf:pdf},
	issn = {1350-4851},
	journal = {Applied Economics Letters},
	mendeley-groups = {My Research/Zeta},
	number = {1},
	pages = {51--54},
	title = {{Beer and the business cycle}},
	volume = {8},
	year = {2001}
}
@article{Bonin2001,
	abstract = {The paper examines the intergenerational impact of the Spanish public pension system after the 1997 Pension Reform Act. Working within a Generational Accounting framework, we find that maintaining the new legal setting could leave future generations with liabilities as high as 176 percent of base year GDP. As the recent reform measures have been insufficient to achieve the sustainability of the current pension system, we also analyse the impact of alternative reform strategies. Within the current pay-as-you-go setting, a further improvement to tax-benefit linkage in line with the original spirit of the Toledo Agreement is shown to yield and intergenerationally more balanced outcome, than an increase in the retirement age or an expansion of public subsidies financed through indirect taxes. Finally, we examine the generational impact of a move toward a partially funded pension system which might restore the intergenerational balance.},
	author = {Bonin, Holger and Gil, Joan and Patxot, Concepci{\'{o}}},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonin, Gil, Patxot - 2001 - Beyond the Toledo agreement the intergenerational impact of the Spanish Pension Reform.pdf:pdf},
	journal = {Spanish Economic Review},
	keywords = {genera-,intergenerational redistribution,spanish pension reform},
	mendeley-groups = {My Research/Pensions},
	pages = {111--130},
	title = {{Beyond the Toledo agreement : the intergenerational impact of the Spanish Pension Reform}},
	volume = {130},
	year = {2001}
}
@article{Sarlio-Lahteenkorva2001,
	abstract = {Fears and experiences of food restriction influence eating behavior but the association between past and present economic disadvantage, food insecurity and body size is poorly understood. Therefore, we examined these associations in a nationwide, representative sample of 25- to 64-y-old Finnish men and women (n = 6506). The respondents were classified by their body mass index (BMI) into four groups: thin, normal, overweight and obese. Economic disadvantage was assessed by three indicators including low household income, unemployment during past 5 y and long-term economic problems in childhood. Food insecurity was assessed by five separate items concerning economic fears and experiences related to sufficient supply of food during the past 12 mo, and a combined scale in which those with affirmative responses to four to five items were classified as hungry. Multivariable logistic regression analyses were conducted using both the BMI grouping and indicators of economic disadvantage as independent variables to predict food insecurity, controlling simultaneously for age, educational attainment and sex. The results showed that low household income, recent unemployment and economic problems in childhood were all predictors of food insecurity. Thin people were most likely to be hungry and showed most food insecurity in five separate items. In addition, obese people reported more buying cheaper food due to economic problems and fears or experiences of running out of money to buy food than did normal weight subjects. In conclusion, both past and present economic disadvantage is associated with various aspects of food insecurity. The association between food insecurity and BMI is curvilinear.},
	author = {Sarlio-L{\"{a}}hteenkorva, S and Lahelma, E},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarlio-L{\"{a}}hteenkorva, Lahelma - 2001 - Food insecurity is associated with past and present economic disadvantage and body mass index.pdf:pdf},
	isbn = {0022-3166 (Print)},
	issn = {0022-3166},
	journal = {The Journal of nutrition},
	mendeley-groups = {My Research/Zeta},
	number = {11},
	pages = {2880--2884},
	pmid = {11694612},
	title = {{Food insecurity is associated with past and present economic disadvantage and body mass index.}},
	volume = {131},
	year = {2001}
}
@article{Ahn2001,
	author = {Ahn, Seung Chan and Lee, Young Hoon and Schmidt, Peter},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Ahn et al (2001) Time-Varying Individual Effects.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {ects,generalized method of moments,mle,panel data,time-varying e},
	mendeley-groups = {Panel Data,Panel Data/Factor Models and Interactive Effects},
	pages = {219--255},
	title = {{GMM estimation of linear panel data models with time-varying individual effects}},
	volume = {101},
	year = {2001}
}
@article{Okina2001,
	abstract = {Since the latter half of the 1980s, Japan's economy has experienced the emergence, expansion, and bursting of a bubble economy, characterized by a rapid rise in asset prices, the overheating of economic activity, and the expansion of money supply and credit. This paper examines the mechanism by which the bubble economy was generated and summarizes lessons a central bank should draw from the experience in order to prevent it from happening again. Specifically, by focusing on the intensified bullish expectations that played an important role behind the large fluctuations in asset prices and the economy, the process of the emergence, expansion, and bursting of the bubble is examined in relation to the monetary policy at the time. Based on this analysis, the paper discusses a framework for monetary policy conducive to achieving both price stability and financial system stability.},
	author = {Okina, Kunio and Shirakawa, Masaaki and Shiratsuka, Shigenori},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Okina, Shirakawa, Shiratsuka - 2001 - The Asset Price Bubble and Monetary Policy Japan's Experience in the Late 1980s Background Paper.pdf:pdf},
	journal = {Monetary and Economic Studies},
	keywords = {asset prices,bank of japan,boj,bubble,e-mail,financial,forward-looking monetary policy,intensified bullish expectations,jp,kunio,kunio okina,monetary policy,okina,or,stability,sustained price stability},
	number = {February},
	pages = {395--450},
	title = {{The Asset Price Bubble and Monetary Policy: Japan's Experience in the Late 1980s Background Paper}},
	year = {2001}
}
@article{Poot2001,
	abstract = {Becker and Barro (1988) formulated a theoretical model which identified a range of macroeconomic variables which can temporarily or permanently affect fertility in small open economies. This article tests the Becker-Barro model with relevant data which covers most of the 20th century for two small open economies, namely The Netherlands and New Zealand. The results show that government subsidies for having children have a strong positive effect on fertility, while the provision of public pensions has a strong negative effect. The degree of intergenerational altruism appears to have been declining. Moreover, there is only weak support for the hypothesis that real interest rates positively influence fertility.},
	author = {Poot, Jacques and Siegers, Jacques J.},
	doi = {10.1007/s001480050160},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poot, Siegers - 2001 - The macroeconomics of fertility in small open economies A test of the Becker-Barro model for the Netherlands and.pdf:pdf},
	issn = {09331433},
	journal = {Journal of Population Economics},
	keywords = {Altruism,Dynastic utility,Fertility,Real interest rates,Social security},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {73--100},
	title = {{The macroeconomics of fertility in small open economies: A test of the Becker-Barro model for the Netherlands and New Zealand}},
	volume = {14},
	year = {2001}
}
@article{Henisz2002,
	abstract = {The empirical evidence that links political institutions to economic outcomes has grown dramatically in recent years. However, virtually all of this analysis is undertaken using data from the past three decades. This paper extends this empirical framework by performing a two‐century long historical analysis of the determinants of infrastructure investment in a panel of over 100 countries. The results demonstrate that political environments that limit the feasibility of policy change are an important determinant of investment in infrastructure. },
	annote = {10.1093/icc/11.2.355},
	author = {Henisz, Witold J},
	doi = {10.1093/icc/11.2.355},
	journal = {Industrial and Corporate Change },
	month = {apr},
	number = {2 },
	pages = {355--389},
	title = {{The institutional environment for infrastructure investment}},
	url = {http://icc.oxfordjournals.org/content/11/2/355.abstract},
	volume = {11 },
	year = {2002}
}
@article{Hsiao2002,
	abstract = {A transformed likelihood approach is suggested to estimate fixed effects dynamic panel data models. Conditions on the data generating process of the exogenous variables are given to get around the issue of "incidental parameters". The maximum likelihood (MLE) and minimum distance estimator (MDE) are suggested. Both estimators are shown to be consistent and asymptotically normally distributed. A Hausman-type specification test is suggested to test the fixed versus random effects specification or conditions on the data generating process of the exogenous variables. Monte Carlo studies are conducted to evaluate the finite sample properties of the MLE, MDE, instrumental variable estimator (IV) and linear generalized method of moments estimator (GMM). It is shown that the likelihood approach appears to dominate the GMM approach both in terms of the bias and root mean square error of the estimators and the size and power of the test statistics. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
	author = {Hsiao, Cheng and Pesaran, M. Hashem and {Kamil Tahmiscioglu}, A.},
	doi = {10.1016/S0304-4076(01)00143-9},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Hsiao et al. (2002) - Short T Dynamic Panels.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Dynamic panels,Fixed and random effects,GMM,IV,Maximum likelihood estimators,Minimum distance estimators},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {107--150},
	title = {{Maximum likelihood estimation of fixed effects dynamic panel data models covering short time periods}},
	volume = {109},
	year = {2002}
}
@article{Ruhm2002,
	abstract = {This paper investigates the relationship between macroeconomic conditions and drinking using individual-level data from 1987 to 1999 interview years of the “behavioral risk factor surveillance system” (BRFSS). We confirm the procyclical variation in overall drinking identified in previous research using aggregate sales data and show that this largely results from changes in consumption by existing drinkers, rather than movements into or out of drinking. Moreover, the decrease occurring during bad economic times is concentrated among heavy consumers, with light drinking actually rising. We also find no evidence that the decline in overall alcohol use masks a rise for persons becoming unemployed during contractions. These results suggest that any stress-induced increases in drinking during bad economic times are more than offset by declines resulting from changes in economic factors such as lower incomes.},
	author = {Ruhm, Christopher J. and Black, William E.},
	doi = {10.1016/S0167-6296(02)00033-4},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruhm, Black - 2002 - Does drinking really decrease in bad times.pdf:pdf},
	isbn = {0167-6296},
	issn = {01676296},
	journal = {Journal of Health Economics},
	keywords = {Alcohol consumption,Behavioral risk factor surveillance system (BRFSS),E320,I120,Macroeconomic conditions},
	mendeley-groups = {My Research/Zeta},
	number = {4},
	pages = {659--678},
	pmid = {12146596},
	title = {{Does drinking really decrease in bad times?}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167629602000334},
	volume = {21},
	year = {2002}
}
@book{Gyorfi2002,
	author = {Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Kohler, Michael and Krzyzak, Adam and Walk, Harro},
	doi = {10.1007/b97848},
	isbn = {978-0-387-95441-7},
	mendeley-groups = {Nonparametric Estimation/Books and Notes},
	publisher = {Springer Series in Statistics},
	title = {{A Distribution-Free Theory of Nonparametric Regression}},
	year = {2002}
}
@article{Rochet2002,
	author = {Rochet, Jean-charles and Tirole, Jean},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rochet, Tirole - 2002 - Cooperation among Competitors Some Economics of Payment Card Associations.pdf:pdf},
	journal = {The RAND Journal},
	mendeley-groups = {IO},
	number = {4},
	pages = {549--570},
	title = {{Cooperation among Competitors : Some Economics of Payment Card Associations}},
	volume = {33},
	year = {2002}
}
@article{Cigno2002,
	author = {Cigno, A and Casolaro, L and Rosati, FC},
	doi = {10.1628/0015221032643209},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cigno, Casolaro, Rosati - 2002 - The impact of social security on saving and fertility in Germany.pdf:pdf},
	issn = {00152218},
	journal = {FinanzArchiv/Public Finance Analysis},
	mendeley-groups = {My Research/Pensions},
	number = {2},
	pages = {189--211},
	title = {{The impact of social security on saving and fertility in Germany}},
	url = {http://www.jstor.org/stable/40912996},
	volume = {59},
	year = {2002}
}
@article{Banerjee2002,
	abstract = {The paper analyzes the effect of agricultural tenancy laws offering security of tenure to tenants and regulating the share of output that is paid as rent on farm productivity. Theoretically, the net impact of tenancy reform is shown to be a combination of two effects: a bargaining power effect and a security of tenure effect. Analysis of evidence on how contracts and productivity changed after a tenancy reform program was implemented in the Indian state of West Bengal in the late 1970s suggests that tenancy reform had a positive effect on agricultural productivity there.},
	author = {Banerjee, Abhijit{\&}{\#}xa0;V. and Gertler, Paul{\&}{\#}xa0;J. and Ghatak, Maitreesh},
	doi = {10.1086/338744},
	issn = {00223808, 1537534X},
	journal = {Journal of Political Economy},
	mendeley-groups = {My Teaching/Agro},
	number = {2},
	pages = {239--280},
	publisher = {University of Chicago Press},
	title = {{Empowerment and Efficiency: Tenancy Reform in West Bengal}},
	url = {http://www.jstor.org/stable/10.1086/338744},
	volume = {110},
	year = {2002}
}
@article{Schanzenbach2002,
	author = {Schanzenbach, Max},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schanzenbach - 2002 - Network Effects and Antitrust Law Predation , Affirmative Defenses , and the Case of U . S . v . Microsoft.pdf:pdf},
	journal = {Stanford Technology Law Review},
	mendeley-groups = {IO},
	number = {2001},
	title = {{Network Effects and Antitrust Law : Predation , Affirmative Defenses , and the Case of U . S . v . Microsoft}},
	volume = {91},
	year = {2002}
}
@article{Malecki2002,
	author = {Malecki, Edward J},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malecki - 2002 - The Economic Geography of the Internet's Infrastructure.pdf:pdf},
	journal = {Economic Geography},
	keywords = {hall 1998,icant technology of the,intermillennial era,internet,it fills this role,kondratiev wave,networks,telecommunications,the internet is arguably,the most signif-,urban hierarchy,world cities},
	mendeley-groups = {IO},
	number = {4},
	pages = {399--424},
	title = {{The Economic Geography of the Internet's Infrastructure}},
	volume = {78},
	year = {2002}
}
@article{Mahul2003,
	abstract = {When the indemnity schedule is contingent on the farmer's price and individual yield, an optimal crop revenue insurance contract depends only on the farmer's gross revenue. However, this design is not efficient if, as is the case with available contracts, the coverage function is based on imperfect estimators of individual yield and/or price. The producer's degree of prudence and the extent of basis risks have important influences on the optimal indemnity schedule. In this broader context, optimal protection is not provided by available U.S. crop insurance contracts and may include combinations of revenue insurance, yield insurance, futures, and options contracts. },
	annote = {10.1111/1467-8276.00457},
	author = {Mahul, Olivier and Wright, Brian D},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta},
	month = {aug},
	number = {3 },
	pages = {580--589},
	title = {{Designing Optimal Crop Revenue Insurance}},
	url = {http://ajae.oxfordjournals.org/content/85/3/580.abstract},
	volume = {85 },
	year = {2003}
}
@article{Hjort2003,
	abstract = {The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications.},
	author = {Hjort, Nils Lid and Claeskens, Gerda},
	doi = {10.1198/016214503000000828},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Shrinkage/Hjort,  Claeskens (2003) Frequentist Model Average Estimators.pdf:pdf},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Bias and variance balance,Growing models,Likelihood inference,Model average estimators,Model information criteria,Moderate misspecification},
	mendeley-groups = {Model Selection and Averaging},
	number = {464},
	pages = {879--899},
	title = {{Frequentist Model Average Estimators}},
	volume = {98},
	year = {2003}
}
@book{Gaidar2003,
	address = {Cambridge},
	author = {Gaidar, Yegor},
	editor = {Gaidar, Yegor},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaidar - 2003 - The Economics of Russian Transition.pdf:pdf},
	isbn = {026207219X},
	pages = {1--1057},
	publisher = {The MIT Press},
	title = {{The Economics of Russian Transition}},
	year = {2003}
}
@article{Phillips2003,
	abstract = {This paper deals with cross section dependence, homogeneity restrictions and small sample bias issues in dynamic panel regressions. To address the bias problem we develop a panel approach to median unbiased estimation that takes account of cross section dependence. The estimators given here considerably reduce the effects of bias and gain precision from estimating cross section error correlation. This paper also develops an asymptotic theory for tests of coefficient homogeneity under cross section dependence, and proposes a modified Hausman test to test for the presence of homogeneous unit roots. An orthogonalization procedure, based on iterated method of moments estimation, is developed to remove cross section dependence and permit the use of conventional and meta unit root tests with panel data. Some simulations investigating the finite sample performance of the estimation and test procedures are reported. Keywords:},
	author = {Phillips, Peter C. B. and Sul, Donggyu},
	doi = {10.1111/1368-423x.00108},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Phillips, Sul (2003) Dynamic panel estimation and homogeneity testing under.pdf:pdf},
	issn = {1368-4221},
	journal = {The Econometrics Journal},
	keywords = {autoregression,bias,cross section dependence,dynamic,dynamic factors,gls estimation,hausman tests,homogeneity tests,median unbiased estimation,median unbiased sur estimation,modified,orthogonalization procedure,panel estimation,panel unit},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {217--259},
	title = {{Dynamic panel estimation and homogeneity testing under cross section dependence}},
	volume = {6},
	year = {2003}
}
@article{Ledoit2003,
	abstract = {This paper proposes to estimate the covariance matrix of stock returns by an optimally weighted average of two existing estimators: the sample covariance matrix and single-index covariance matrix. This method is generally known as shrinkage, and it is standard in decision theory and in empirical Bayesian statistics. Our shrinkage estimator can be seen as a way to account for extra-market covariance without having to specify an arbitrary multifactor structure. For NYSE and AMEX stock returns from 1972 to 1995, it can be used to select portfolios with significantly lower out-of-sample variance than a set of existing estimators, including multifactor models. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.1016/S0927-5398(03)00007-0},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2003 - Improved estimation of the covariance matrix of stock returns with an application to portfolio selection.pdf:pdf},
	isbn = {0927-5398},
	issn = {09275398},
	journal = {Journal of Empirical Finance},
	keywords = {Covariance matrix estimation,Factor models,Portfolio selection,Shrinkage method},
	mendeley-groups = {UPF/ADTSI},
	number = {5},
	pages = {603--621},
	pmid = {14755677},
	title = {{Improved estimation of the covariance matrix of stock returns with an application to portfolio selection}},
	volume = {10},
	year = {2003}
}
@article{Lindbeck2003,
	abstract = {We classify social security pension systems in three dimensions: actuarial versus non-actuarial, funded versus unfunded, and defined-benefit versus defined-contribution systems. Recent pension reforms are discussed in terms of these dimensions. Shifting to a more actuarial system reduces labor-market distortions, although limiting the scope for redistribution. Shifting to a funded system may increase saving, redistribute income to future generations and distort contemporary labor supply. A partial shift to a funded system helps individuals diversify their pension assets. A shift from a defined-benefit to a defined-contribution system means that income risk will be shifted from workers to pensioners.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Lindbeck, Assar and Persson, Mats},
	doi = {10.1257/002205103321544701},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindbeck, Persson - 2003 - The Gains from Pension Reform.pdf:pdf},
	isbn = {0022-0515},
	issn = {0022-0515},
	journal = {Journal of Economic Literature},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {74--112},
	pmid = {25246403},
	title = {{The Gains from Pension Reform}}, 
	volume = {41},
	year = {2003}
}
@article{Krishnamurthy2003,
	abstract = {Kiyotaki and Moore (J. Polit. Economy 105 (1997) 211) have offered a theory for how common shocks to credit-constrained firms are amplified through changes in collateral values and transmitted as fluctuations in output. I clarify and extend their model by showing that their collateral amplification mechanism is not robust to the introduction of markets that allow these firms to hedge against common shocks. A theory of incomplete hedging is proposed in which the supply of hedging available in the economy is constrained by the aggregate value of collateral. I illustrate how the constraint reinstates amplification effects and discuss empirical implications of this new mechanism. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
	author = {Krishnamurthy, Arvind},
	doi = {10.1016/S0022-0531(03)00098-X},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishnamurthy - 2003 - Collateral constraints and the amplification mechanism.pdf:pdf},
	isbn = {0022-0531},
	issn = {00220531},
	journal = {Journal of Economic Theory},
	keywords = {Credit market imperfections,Financial crises,Incomplete markets,Insurance,Interest rate swaps,Liquidity},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {2},
	pages = {277--292},
	title = {{Collateral constraints and the amplification mechanism}},
	volume = {111},
	year = {2003}
}
@article{Alvarez2003,
	author = {Alvarez, Javier and Arellano, Manuel},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Alvarez, Arellano (2003) The Time Series and Cross-Section Asymptotics of Dynamic Panel Data Estimators.pdf:pdf},
	journal = {Econometrica},
	keywords = {autoregressive models,panel data,random effects,within-groups},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {4},
	pages = {1121--1159},
	title = {{The Time Series and Cross-Section Asymptotics of Dynamic Panel Data Estimators}},
	volume = {71},
	year = {2003}
}
@article{Levinsohn2003,
	abstract = {We add to the methods for conditioning out serially correlated unobserved shocks to the production technology. We build on ideas first developed in Olley and Pakes (1996). They show how to use investment to control for correlation between input levels and the unobserved firm-specific productivity process. We show that intermediate inputs (those inputs which are typically subtracted out in a value-added production function) can also solve this simultaneity problem. We discuss some theoretical benefits of extending the proxy choice set in this direction and our empirical results suggest these benefits can be important.},
	author = {Levinsohn, James and Petrin, Amil},
	doi = {10.1111/1467-937X.00246},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Levinsohn, Petrin (2003) Estimating Production Functions Using Inputs to Control for Unobservables.pdf:pdf},
	issn = {00346527},
	journal = {Review of Economic Studies},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {2},
	pages = {317--341},
	title = {{Estimating Production Functions Using Inputs to Control for Unobservables}},
	volume = {70},
	year = {2003}
}
@article{Chen2003,
	abstract = {We propose an estimation method for models of conditional moment restrictions, which contain finite dimensional unknown parameters ($\theta$) and infinite dimensional un- known functions (h). Our proposal is to approximate h with a sieve and to estimate $\theta$ and the sieve parameters jointly by applying the method ofminimum distance.We show that: (i) the sieve estimator of h is consistent with a rate faster than n−1/4 under certain metric; (ii) the estimator of $\theta$ is √ n consistent and asymptotically normally distributed; (iii) the estimator for the asymptotic covariance of the $\theta$ estimator is consistent and easy to compute; and (iv) the optimally weighted minimum distance estimator of $\theta$ at- tains the semiparametric efficiency bound. We illustrate our results with two examples: a partially linear regression with an endogenous nonparametric part, and a partially additive IV regression with a link function.},
	author = {Chen, Xiaohong and Ai, Chunrong},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Ai, Chen (2003) Efficient Estimation of Models with Conditional Moment Restrictions Containing Unknown Functions.pdf:pdf},
	journal = {Econometrica},
	keywords = {newidea,semiparametric efficiency,sieve estimators},
	mendeley-groups = {Semiparametric Estimation},
	number = {6},
	pages = {1795--1843},
	title = {{Efficient Estimation of Models with Conditional Moment Restrictions Containing Unknown Functions}},
	volume = {71},
	year = {2003}
}
@article{Wooldridge2003,
	author = {Wooldridge, Jeffrey M.},
	doi = {10+10170S0266466603002081},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Wooldridge (2003). 03.2.1. Fixed Effects Estimation of the Population-Averaged Slopes in a Panel Data Random Coefficient Model.pdf:pdf},
	issn = {00131784},
	journal = {Econometric Theory},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {411--413},
	title = {{Fixed Effects Estimation of the Population-Averaged Slopes in a Panel Data Random Coefficient Model}},
	volume = {19},
	year = {2003}
}
@article{Im2003,
	abstract = {This paper proposes unit root tests for dynamic heterogeneous panels based on the mean of individual unit root statistics. In particular it proposes a standardized t-bar test statistic based on the (augmented) Dickey-Fuller statistics averaged across the groups. Under a general setting this statistic is shown to converge in probability to a standard normal variate sequentially with T (the time series dimension) → ∞, followed by N (the cross sectional dimension) → ∞. A diagonal convergence result with T and N → ∞ while N/T → k, k being a finite non-negative constant, is also conjectured. In the special case where errors in individual Dickey-Fuller (DF) regressions are serially uncorrelated a modified version of the standardized t-bar statistic is shown to be distributed as standard normal as N → ∞ for a fixed T, so long as T {\textgreater} 5 in the case of DF regressions with intercepts and T {\textgreater} 6 in the case of DF regressions with intercepts and linear time trends. An exact fixed N and T test is also developed using the simple average of the DF statistics. Monte Carlo results show that if a large enough lag order is selected for the underlying ADF regressions, then the small sample performances of the t-bar test is reasonably satisfactory and generally better than the test proposed by Levin and Lin (Unpublished manuscript, University of California, San Diego, 1993). {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
	author = {Im, Kyung So and Pesaran, M. Hashem and Shin, Yongcheol},
	doi = {10.1016/S0304-4076(03)00092-7},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Im, Pesaran, Shin (2003) Testing for unit roots in heterogeneous panels.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Finite sample properties,Heterogeneous dynamic panels,T-bar statistics,Tests of unit roots},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {53--74},
	title = {{Testing for unit roots in heterogeneous panels}},
	volume = {115},
	year = {2003}
}
@article{Wooldridge2003b,
	author = {Wooldridge, Jeffrey M.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Wooldridge (2003). Further results on instrumental variables estimation of average treatment effects in the correlated random coefficient model..pdf:pdf},
	journal = {Economics Letters},
	keywords = {average treatment effect,c21,correlated random coefficient model,instrumental variables,jel classification,two-stage least squares,unobserved heterogeneity},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects,Panel Data/Heterogeneous Panels},
	pages = {185--191},
	title = {{Further results on instrumental variables estimation of average treatment effects in the correlated random coefficient model}},
	volume = {79},
	year = {2003}
}
@incollection{Tsybakov2003,
	address = {Heidelberg},
	author = {Tsybakov, Alexandre},
	booktitle = {Computational Learning Theory and Kernel Machines (COLT-2003). Lecture Notes in Artificial Intelligence},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsybakov - 2003 - Optimal Rates of Aggregation.pdf:pdf},
	mendeley-groups = {UPF/Research Seminar},
	publisher = {Springer},
	title = {{Optimal Rates of Aggregation}},
	year = {2003}
}
@article{Rochet2003,
	author = {Rochet, Jean-Charles and Tirole, Jean},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rochet, Tirole - 2003 - Platform Competition in Two-sided Markets.pdf:pdf},
	journal = {Journal of European Economic Review},
	mendeley-groups = {IO},
	number = {4},
	pages = {990--1029},
	title = {{Platform Competition in Two-sided Markets}},
	volume = {1},
	year = {2003}
}
@unpublished{Ledoit2003,
	abstract = {The central message of this paper is that nobody should be using the sample covariance matrix for the purpose of portfolio optimization. It contains estimation error of the kind most likely to perturb a mean-variance optimizer. In its place, we suggest using the matrix obtained from the sample covariance matrix through a transformation called shrinkage. This tends to pull the most extreme coefficients towards more central values, thereby systematically reducing estimation error where it matters most. Statistically, the challenge is to know the optimal shrinkage intensity, and we give the formula for that. Without changing any other step in the portfolio optimization process, we show on actual stock market data that shrinkage reduces tracking error relative to a benchmark index, and substantially increases the realized information ratio of the active portfolio manager.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.2139/ssrn.433840},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2003 - Honey, I Shrunk the Sample Covariance Matrix.pdf:pdf},
	isbn = {0095-4918},
	issn = {0095-4918},
	keywords = {C13,C51,C61,Covariance matrix,G11,G15,Markovitz optimization,shrinkage,tracking error},
	mendeley-groups = {UPF/ADTSI},
	pages = {1--22},
	pmid = {12760391},
	title = {{Honey, I Shrunk the Sample Covariance Matrix}},
	year = {2003}
}
@article{Potscher2003,
	author = {P{\"{o}}tscher, Benedikt M. and Leeb, Hannes},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Leeb, P{\"{o}}tscher (2003) Finite Sample Distribution.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	pages = {100--142},
	title = {{The Finite-Sample Distribution of Post-Model-Selection Estimators and Uniform Versus Nonuniform Approximations}},
	volume = {19},
	year = {2003}
}
@article{Hsieh2003,
	abstract = {con- sumption is constant), 1 should be equal to zero. The first column in Table 2 presents the re- sults of the first set of tests for nondurable . 10 The point estimate of 1 is positive, but economically},
	author = {Hsieh, Chang Tai},
	doi = {10.1257/000282803321455377},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh - 2003 - Do consumers react to anticipated income changes Evidence from the Alaska permanent fund.pdf:pdf},
	isbn = {00028282},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro II/Consumption},
	number = {1},
	pages = {397--405},
	title = {{Do consumers react to anticipated income changes? Evidence from the Alaska permanent fund}},
	volume = {93},
	year = {2003}
}
@article{HollyWang2003,
	abstract = {Risk theory tells us if an insurer can effectively pool a large number of individuals to reduce total risk, the insurer can then provide insurance by charging a premium close to the actuarially fair rate. However, a common belief exists that risk can be effectively pooled only when random loss is independent. Therefore crop insurance markets cannot survive without government subsidy because crop yields are not independent among growers. In this article, we take a spatial statistics approach to examine the effectiveness of risk pooling for crop insurance under correlation. We develop a method for evaluating the effectiveness of risk pooling under correlation and apply the method to three major crops in the United States: wheat, soybeans, and corn. The empirical study shows that yields for the three crops present zero or negative correlation when two counties are far apart, which complies with a weaker condition than independence, finite-range positive dependency. The results show that effective risk pooling is possible and reveal a high possibility of a private crop insurance market in the United States.},
	author = {{Holly Wang}, H. and Zhang, Hao},
	doi = {10.1111/1539-6975.00051},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holly Wang, Zhang - 2003 - On the Possibility of a Private Crop Insurance Market A Spatial Statistics Approach.pdf:pdf},
	issn = {0022-4367},
	journal = {Journal of Risk and Insurance},
	mendeley-groups = {My Research/Theta/Review},
	number = {1},
	pages = {111--124},
	title = {{On the Possibility of a Private Crop Insurance Market: A Spatial Statistics Approach}},
	volume = {70},
	year = {2003}
}
@article{Wansink2003,
	abstract = {Building on findings related to physiological and psychological motivations of food preference, this research develops a framework to examine preferences toward comfort foods. Study 1 used a North American survey of 411 people to determine favored comfort foods, and Study 2 quantified the preferences for these foods across gender and across age groups using a stratified sample of 1005 additional people. Consistent with hypotheses, the findings showed different comfort food preferences across gender and across age. Males preferred warm, hearty, meal-related comfort foods (such as steak, casseroles, and soup), while females instead preferred comfort foods that were more snack related (such as chocolate and ice cream). In addition, younger people preferred more snack-related comfort foods compared to those over 55 years of age. Associations with guilty feelings underscored how these different preferences between males and females may extend to areas of application.},
	author = {Wansink, Brian and Cheney, Matthew M and Chan, Nina},
	doi = {http://dx.doi.org/10.1016/S0031-9384(03)00203-8},
	issn = {0031-9384},
	journal = {Physiology {\&} Behavior},
	keywords = {Age,Chocolate,Comfort foods,Food preference,Gender,Meals,Nutrient density,Snacks},
	mendeley-groups = {My Research/Zeta},
	month = {sep},
	number = {4–5},
	pages = {739--747},
	title = {{Exploring comfort food preferences across age and gender}},
	volume = {79},
	year = {2003}
}
@article{Baltagi1996,
	author = {Baltagi, Badi H},
	doi = {DOI: 10.1017/S0266466600007209},
	edition = {2009/02/11},
	issn = {0266-4666},
	journal = {Econometric Theory},
	mendeley-groups = {Panel Data/Efficiency},
	number = {5},
	pages = {867},
	publisher = {Cambridge University Press},
	title = {{Heteroskedastic Fixed Effects Models}},
	volume = {12},
	year = {1996}
}

@article{Ledoit2004,
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator - the sample covariance matrix - is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.1016/S0047-259X(03)00096-4},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2004 - A well-conditioned estimator for large-dimensional covariance matrices.pdf:pdf},
	isbn = {0047-259X},
	issn = {0047259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Condition number,Covariance matrix estimation,Empirical Bayes,General asymptotics,Shrinkage},
	mendeley-groups = {UPF/ADTSI},
	number = {2},
	pages = {365--411},
	title = {{A well-conditioned estimator for large-dimensional covariance matrices}},
	volume = {88},
	year = {2004}
}
@article{Elliott2004,
	abstract = {Existing results on the properties and performance of forecast combinations have been derived in the context of mean squared error loss. Under this loss function empirical studies have generally found that equally-weighted combined forecasts lead to better performance than estimates of optimal forecast combination weights which in turn outperform the best individual predictions. We show that this and other results can be overturned when asymmetries are introduced in the loss function and the forecast error distribution is skewed. We characterize the optimal combination weights for the most commonly used alternatives to mean squared error loss and demonstrate how the degree of asymmetry in the loss function and skews in the underlying forecast error distribution can significantly change the optimal combination weights. We also propose estimation methods and investigate their small sample properties in simulations and in an inflation forecasting exercise. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
	author = {Elliott, Graham and Timmermann, Allan},
	doi = {10.1016/j.jeconom.2003.10.019},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Elliott, Timmermann (2004) Optimal forecast combinations under general loss functions and forecast error distributions.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Asymmetric loss,Forecast combination,Loss function},
	mendeley-groups = {Forecasting/Forecast Combination},
	number = {1},
	pages = {47--79},
	title = {{Optimal forecast combinations under general loss functions and forecast error distributions}},
	volume = {122},
	year = {2004}
}
@article{Finkelstein2004,
	annote = {NULL},
	author = {Finkelstein, Amy and Mcgarry, Kathleen},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Finkelstein, Mcgarry - 2004 - Multiple dimensions of private information evidence from the long-term care insurance market.pdf:pdf},
	journal = {NBER Working Paper},
	keywords = {adverse selection,asymmetric information,ben olken,casey rothschild,d82,david cutler,david de,g22,ginger jin,i11,janet currie,jeff brown,jel classification,jerry hausman,larry katz,long-term care insurance,meza,pierre-andre chiappori,raj chetty,sarah reber,seema jayachandran,we thank daron acemoglu},
	title = {{Multiple dimensions of private information: evidence from the long-term care insurance market}},
	year = {2004}
}
@article{Coleman2004,
	abstract = {Backing women's rights in developing countries isn't just good ethics; it's also sound economics. Growth and living standards get a dramatic boost when women are given just a bit more education, political clout, and economic opportunity. So the United States should aggressively promote women's rights abroad. And by couching its case in economic terms, it might even overcome the resistance of conservative Muslim countries that have long balked at gender equality.},
	author = {Coleman, Isobel},
	doi = {10.2307/20033977},
	issn = {00157120},
	journal = {Foreign Affairs},
	mendeley-groups = {My Research/Prima},
	number = {3},
	pages = {80--95},
	publisher = {Council on Foreign Relations},
	title = {{The Payoff from Women's Rights}},
	volume = {83},
	year = {2004}
}
@article{Matsuyama2004,
	author = {Matsuyama, Kiminori},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matsuyama - 2004 - Financial Market Globalization, Symmetry-Breaking and Endogenous Inequality of Nations.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {3},
	pages = {853--884},
	title = {{Financial Market Globalization, Symmetry-Breaking and Endogenous Inequality of Nations}},
	volume = {72},
	year = {2004}
}
@article{Sherrick2004a,
	author = {Sherrick, Bruce J and Zanini, Fabio C and Schnitkey, Gary D and Irwin, Scott H and American, Source and Economics, Agricultural and May, No and Irwin, Scort H},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sherrick et al. - 2004 - Insurance Valuation under Alternative Yield.pdf:pdf},
	journal = {American Journal of Agricultural Economics},
	keywords = {been employed in past,crop insurance,crop-yield distributions,day,distributions,e,g,gallagher,goodwin,have,many different conceptual frameworks,risk and uncertainty,studies of crop-yield,valuation},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {406--419},
	title = {{Insurance Valuation under Alternative Yield}},
	volume = {86},
	year = {2004}
}
@article{Iacoviello2004,
	author = {Iacoviello, Matteo},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iacoviello - 2004 - House Prices, Borrowing Constraints, and Monetary Policy in the Business Cycle.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Financial Frictions},
	number = {1989},
	pages = {739--764},
	title = {{House Prices, Borrowing Constraints, and Monetary Policy in the Business Cycle}},
	year = {2004}
}
@article{Hahn2004,
	author = {Hahn, Jinyong and Newey, Whitney K.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Hahn, Newey (2004) Analytical Bias Reduction in Nonlinear Panels.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {4},
	pages = {1295--1319},
	title = {{Jackknife and Analytical bias Reduction for Nonlinear Panel Models}},
	volume = {72},
	year = {2004}
}
@article{Angrist2004,
	author = {Angrist, Joshua and Hahn, Jinyong},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Angrist, Hahn - 2004 - When to Control for Covariates Panel Asymptotics for Estimates of Treatment Effects.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects},
	number = {February},
	pages = {58--72},
	title = {{When to Control for Covariates? Panel Asymptotics for Estimates of Treatment Effects}},
	volume = {86},
	year = {2004}
}
@article{Sherrick2004,
	author = {Sherrick, Bruce and Barry, Peter J. and Ellinger, Paul and {Gary D. Schnitkey}},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sherrick et al. - 2004 - Factors Influencing Farmers' Crop Insurance Decisions.pdf:pdf},
	isbn = {9781614719960},
	journal = {American Journal of Agricultural Economics},
	keywords = {crop insurance,fcic,multinomial logit,risk perceptions},
	mendeley-groups = {My Research/Theta/Review},
	number = {February},
	pages = {103--114},
	title = {{Factors Influencing Farmers' Crop Insurance Decisions}},
	volume = {86},
	year = {2004}
}
@article{Pesaran2004,
	abstract = {We build a compact global macroeconometric model capable of generating point and density forecasts for a core set of macroeconomic factors using recent advances in the analysis of cointegrating systems. We do so for a set of countries/regions and explicitly allow for interdependencies that exist between national and international factors. In an unrestricted VAR(p) model in k endogenous variables covering N countries, the number of unknown parameters will be unfeasibly large. We deal with this issue by first estimating individual country/region specific vector error-correcting models, where the domestic variables are related to corresponding foreign variables constructed to match the relevant international trade pattern. The country models are then combined to generate forecasts for all the variables in the world economy simultaneously. We estimate the model using quarterly data from 1979Q1 to 1999Q1, and investigate the time profile of the transmission of shocks of one variable to the rest of the world to show the degree of regional interdependencies.},
	author = {Pesaran, M. Hashem and Schuermann, Til and Weiner, Scott M.},
	doi = {10.1198/073500104000000019},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pesaran, Schuermann, Weiner - 2004 - Modeling Regional Interdependences Using a Global Error-Correcting Macroeconometric Model.pdf:pdf},
	issn = {07350015},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Credit loss distribution,Global interdependencies,Global macroeconometric modeling,Global vector error-correcting model,Risk management},
	mendeley-groups = {Panel Data/VAR},
	number = {2},
	pages = {129--162},
	title = {{Modeling Regional Interdependences Using a Global Error-Correcting Macroeconometric Model}},
	volume = {22},
	year = {2004}
}
@article{Potscher2005,
	author = {P{\"{o}}tscher, Benedikt M. and Leeb, Hannes},
	doi = {10.1017/S0266466605050036},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Leeb, P{\"{o}}tscher (2005) Facts and Fiction.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	pages = {21--59},
	title = {{Model Selection and Inference: Facts and Fiction}},
	volume = {21},
	year = {2005}
}
@article{Wooldridge2005,
	author = {Wooldridge, Jeffrey M.},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Wooldridge (2005) Fixed-Effects and Related Estimators for Correlated Random-Coefficient and Treatment-Effect Panel Data Models.pdf:pdf},
	journal = {The Review of Economics and Statistics},
	mendeley-groups = {Panel Data,Panel Data/Heterogeneous Panels},
	number = {May},
	pages = {385--390},
	title = {{Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models}},
	volume = {87},
	year = {2005}
}
@unpublished{Sun,
	author = {Sun, Yixiao},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Sun (2005) Estimation and Inference in Panel Structure Models.pdf:pdf},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	title = {{Estimation and Inference in Panel Structure Models}},
	year = {2005}
}
@article{Hansen2005,
	author = {Hansen, Bruce E},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Hansen (2005) Challenges.pdf:pdf},
	isbn = {0266466605},
	journal = {Econometric Theory},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	pages = {60--68},
	title = {{Challenges for Econometric Model Selection}},
	volume = {21},
	year = {2005}
}
@inproceedings{Rochet2005,
	address = {Rome},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	booktitle = {Advances in the Economics of Competition Law},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rochet, Tirole - 2005 - Competition Policy in Two-Sided Markets.pdf:pdf},
	mendeley-groups = {IO},
	title = {{Competition Policy in Two-Sided Markets}},
	year = {2005}
}
@book{Binder2005,
	abstract = {This paper considers estimation and inference in panel vector autoregressions where (i) the individual effects are either random or fixed, (ii) the time-series properties of the model variables are unknown a priori and may feature unit roots and cointegrating relations, and (iii) the time dimension of the panel is short and its crosssectional dimension is large. Generalized method of moments (GMM) and quasi maximum likelihood (QML) estimators are obtained and compared in terms of their asymptotic and finite-sample properties. It is shown that the asymptotic variances of the GMM estimators that are based on levels in addition to first differences of the model variables depend on the variance of the individual effects, whereas by construction the fixed effects QML estimator is not subject to this problem. Monte Carlo evidence is provided showing that the fixed effects QML estimator tends to outperform the various GMM estimators in finite sample under both normal and nonnormal errors. The paper also shows how the fixed effects QML estimator can be successfully used for unit root and cointegration tests in short panels. {\textcopyright} 2005 Cambridge University Press.},
	author = {Binder, Michael and Hsiao, Cheng and {Hashem Pesaran}, M.},
	booktitle = {Econometric Theory},
	doi = {10.1017/S0266466605050413},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Binder et al. (2005) - Short T Panel VAR with Unit Roots and Cointegration.pdf:pdf},
	isbn = {0266466605},
	issn = {02664666},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {4},
	pages = {795--837},
	title = {{Estimation and inference in short panel vector autoregressions with unit roots and cointegration}},
	volume = {21},
	year = {2005}
}
@article{Ruhm2005,
	abstract = {Using microdata for adults from 1987 to 2000 years of the Behavioral Risk Factor Surveillance System (BRFSS), I show that smoking and excess weight decline during temporary economic downturns while leisure-time physical activity rises. The drop in tobacco use occurs disproportionately among heavy smokers, the fall in body weight among the severely obese and the increase in exercise among those who were completely inactive. Declining work hours may provide one reason why behaviors become healthier, possibly by increasing the non-market time available for lifestyle investments. Conversely, there is little evidence of an important role for income reductions. The overall conclusion is that changes in behaviors supply one mechanism for the procyclical variation in mortality and morbidity observed in recent research. ?? 2004 Elsevier B.V. All rights reserved.},
	author = {Ruhm, Christopher J.},
	doi = {10.1016/j.jhealeco.2004.09.007},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruhm - 2005 - Healthy living in hard times.pdf:pdf},
	isbn = {0167-6296},
	issn = {01676296},
	journal = {Journal of Health Economics},
	keywords = {Health investments,Lifestyles,Macroeconomic conditions},
	mendeley-groups = {My Research/Zeta},
	number = {2},
	pages = {341--363},
	pmid = {15721049},
	title = {{Healthy living in hard times}},
	volume = {24},
	year = {2005}
}
@misc{Bezpalyy2005,
	abstract = {This research is devoted to development of the procedures of demand and its elasticities' estimation for chocolate tablets market in Ukraine on macro and micro - levels. To estimate the demand for chocolate tablet as an aggregate product of diffe rent brands, ECM (Error Correction Mechanism) and GMM procedure with instrumental variables were applied. Also, following Eagle and Ambler (2002) there w ere est imated the determinants market growth rate. We found that seasonality has the most significant i nfluence on chocolate consumption. Demand appeared to be price elastic : - 1.99 {\%} and income inelastic: 0.12 {\%} with quit low significance, however advertising clearly adds explanatory power to the model, but its elasticity is low too: 0.04{\%}. The price, income and adver tising growth by 1 unit will lead the growth of market to change by - 1.38{\%}, 0.054{\%} and 0.003{\%} respectively. But necessary to be cautious with the results which concern the growth of market size as they were obtained from the model with some dra wbacks in order to have commensurability with results of Eagle {\&} Ambler (2002)},
	address = {Kiev},
	author = {Bezpalyy, Yuriy},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bezpalyy - 2005 - Estimation of Demand Elasticities for Chocolate Tablets in.pdf:pdf},
	keywords = {Chocolate,Demand,Estimation},
	mendeley-groups = {My Research/Zeta},
	mendeley-tags = {Chocolate,Demand,Estimation},
	title = {{Estimation of Demand Elasticities for Chocolate Tablets in}},
	year = {2005}
}
@article{Kiefer2005,
	annote = {doi: 10.1016/j.jmhg.2005.04.010},
	author = {Kiefer, Ingrid and Rathmanner, Theres and Kunze, Michael},
	doi = {10.1016/j.jmhg.2005.04.010},
	issn = {1571-8913},
	journal = {The Journal of Men's Health and Gender},
	mendeley-groups = {My Research/Zeta},
	month = {jun},
	number = {2},
	pages = {194--201},
	publisher = {Elsevier},
	title = {{Eating and dieting differences in men and women}},
	volume = {2},
	year = {2005}
}
@book{Kurdila2005,
	author = {Kurdila, Andrew J. and Zabarankin, Michael},
	doi = {10.1007/3-7643-7357-1},
	isbn = {978-3-7643-2198-7},
	mendeley-groups = {Random Useful Math/Operator Theory and Functional Analysis},
	pages = {1--228},
	publisher = {Birkh{\"{a}}user Basel},
	title = {{Convex Functional Analysis}},
	year = {2005}
}
@article{Han2005,
	author = {Han, Chirok and Orea, Luis and Schmidt, Peter},
	doi = {10.1016/j.jeconom.2004.05.002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Han et al. (2004) Parametric Individual Effects.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {cost,fixed-effects,generalized method of moments,gmm,time-varying individual effects},
	mendeley-groups = {Panel Data,Panel Data/Factor Models and Interactive Effects},
	pages = {241--267},
	title = {{Estimation of a panel data model with parametric temporal variation in individual effects}},
	volume = {126},
	year = {2005}
}
@article{Dufour2005,
	abstract = {It is well known that standard asymptotic theory is not applicable or is very unreliable in models with identification problems or weak instruments. One possible way out consists of using a variant of the Anderson-Rubin ((1949), AR) procedure. The latter allows one to build exact tests and confidence sets only for the full vector of the coefficients of the endogenous explanatory variables in a structural equation, but not for individual coefficients. This problem may in principle be overcome by using projection methods (Dufour (1997), Dufour and Jasiak (2001)). At first sight, however, this technique requires the application of costly numerical algorithms. In this paper, we give a general necessary and sufficient condition that allows one to check whether an AR-type confidence set is bounded. Furthermore, we provide an analytic solution to the problem of building projection-based confidence sets from AR-type confidence sets. The latter involves the geometric properties of "quadrics" and can be viewed as an extension of usual confidence intervals and ellipsoids. Only least squares techniques are needed to build the confidence intervals.},
	author = {Dufour, Jean Marie and Taamoun, Mohamed},
	doi = {10.1111/j.1468-0262.2005.00618.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/Weak IV/Dufour, Taamouti (2005) Projection-Based Statistical Inference in Linear Structural Models with Possibly Weak Instruments.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Asymptotic theory,Confidence interval,Exact inference,Instrumental variable,Projection,Quadric,Simultaneous equations,Structural model,Testing,Weak instrument},
	mendeley-groups = {Confidence Sets/Weak IV},
	number = {4},
	pages = {1351--1365},
	title = {{Projection-Based Statistical Inference in Linear Structural Models with Possibly Weak Instruments}},
	volume = {73},
	year = {2005}
}
@article{Baltagi2006,
	author = {Baltagi, Badi H. and Bresson, Georges and Pirotte, Alain},
	doi = {10.1016/j.jeconom.2005.06.029},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baltagi, Bresson, Pirotte - 2006 - Joint LM test for homoskedasticity in a one-way error component model.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {error components,heteroskedasticity,lagrange multiplier tests,monte carlo simulations,panel data},
	mendeley-groups = {Panel Data,Panel Data/Heteroskedasticity},
	pages = {401--417},
	title = {{Joint LM test for homoskedasticity in a one-way error component model}},
	volume = {134},
	year = {2006}
}
@phdthesis{Mutl2006,
	author = {Mutl, Jan},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mutl - 2006 - Dynamic Panel Data Models with Spatially Correlated Disturbances.pdf:pdf},
	mendeley-groups = {Panel Data/Spatial Panels},
	school = {University of Maryland},
	title = {{Dynamic Panel Data Models with Spatially Correlated Disturbances}},
	year = {2006}
} 

@article{Lee2006,
	author = {Lee, Tae-hwy},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee - 2006 - Copula-based Multivariate GARCH Model with Uncorrelated Dependent Errors ∗.pdf:pdf},
	keywords = {c3,c5,copula,density forecast,g0,jel classi fi cation,lated dependent errors,mgarch,non-normal multivariate distribution,uncorre-},
	mendeley-groups = {My Research/Balts},
	number = {951},
	title = {{Copula-based Multivariate GARCH Model with Uncorrelated Dependent Errors ∗}},
	volume = {1},
	year = {2006}
}
@article{Johnson2006,
	abstract = {Using questions expressly added to the Consumer Expenditure Survey, we estimate the change in consumption expenditures caused by the 2001 federal income tax rebates and test the permanent income hypothesis. We exploit the unique, randomized timing of rebate receipt across households. Households spent 20 to 40 percent of their rebates on nondurable goods during the three-month period in which their rebates arrived, and roughly two-thirds of their rebates cumulatively during this period and the subsequent three-month period. The implied effects on aggregate consumption demand are substantial. Consistent with liquidity constraints, responses are larger for households with low liquid wealth or low income. (JEL D12, D91, E21, E62, H24, H31)},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Johnson, David S. and Parker, Jonathan A. and Souleles, Nicholas S.},
	doi = {10.1257/aer.96.5.1589},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Parker, Souleles - 2006 - Household expenditure and the income tax rebates of 2001.pdf:pdf},
	isbn = {00028282},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro II/Consumption},
	number = {5},
	pages = {1589--1610},
	pmid = {25246403},
	title = {{Household expenditure and the income tax rebates of 2001}},
	volume = {96},
	year = {2006}
}
@article{Jondeau2006,
	abstract = {Modeling the dependency between stock market returns is a difficult task when returns follow a complicated dynamics. When returns are non-normal, it is often simply impossible to specify the multivariate distribution relating two or more return series. In this context, we propose a new methodology based on copula functions, which consists in estimating first the univariate distributions and then the joining distribution. In such a context, the dependency parameter can easily be rendered conditional and time varying. We apply this methodology to the daily returns of four major stock markets. Our results suggest that conditional dependency depends on past realizations for European market pairs only. For these markets, dependency is found to be more widely affected when returns move in the same direction than when they move in opposite directions. Modeling the dynamics of the dependency parameter also suggests that dependency is higher and more persistent between European stock markets. ?? 2006.},
	author = {Jondeau, Eric and Rockinger, Michael},
	doi = {10.1016/j.jimonfin.2006.04.007},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jondeau, Rockinger - 2006 - The Copula-GARCH model of conditional dependencies An international stock market application.pdf:pdf},
	isbn = {0261-5606},
	issn = {02615606},
	journal = {Journal of International Money and Finance},
	keywords = {Copula function,Dependency,GARCH model,International correlation,Skewed Student-t distribution,Stock indices},
	mendeley-groups = {My Research/Balts},
	number = {5},
	pages = {827--853},
	pmid = {22278030},
	title = {{The Copula-GARCH model of conditional dependencies: An international stock market application}},
	volume = {25},
	year = {2006}
}
@article{Gerdtham2006,
	abstract = {This study uses aggregate data for 23 Organization for Economic Cooperation and Development (OECD) countries over the 1960-1997 period to examine the relationship between macroeconomic conditions and deaths. The main finding is that total mortality and deaths from several common causes rise when labor markets strengthen. For instance, controlling for year effects, location fixed-effects (FE), country-specific time trends and demographic characteristics, a 1{\%} point decrease in the national unemployment rate is associated with growth of 0.4{\%} in total mortality and the following increases in cause-specific mortality: 0.4{\%} for cardiovascular disease, 1.1{\%} for influenza/pneumonia, 1.8{\%} for liver disease, 2.1{\%} for motor vehicle deaths, and 0.8{\%} for other accidents. These effects are particularly pronounced for countries with weak social insurance systems, as proxied by public social expenditure as a share of GDP. The findings are consistent with evidence provided by other recent research and cast doubt on the hypothesis that economic downturns have negative effects on physical health. ?? 2006 Elsevier B.V. All rights reserved.},
	author = {Gerdtham, Ulf G. and Ruhm, Christopher J.},
	doi = {10.1016/j.ehb.2006.04.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gerdtham, Ruhm - 2006 - Deaths rise in good economic times Evidence from the OECD.pdf:pdf},
	isbn = {1570-677X (Print)$\backslash$n1570-677x},
	issn = {1570677X},
	journal = {Economics and Human Biology},
	keywords = {Business cycles,Health,Mortality},
	mendeley-groups = {My Research/Zeta},
	number = {3},
	pages = {298--316},
	pmid = {16713407},
	title = {{Deaths rise in good economic times: Evidence from the OECD}},
	volume = {4},
	year = {2006}
}
@article{Zhao2006,
	abstract = {Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are "irrepresentable" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.},
	archivePrefix = {arXiv},
	arxivId = {1305.7477},
	author = {Zhao, Peng and Yu, Bin},
	doi = {10.1109/TIT.2006.883611},
	eprint = {1305.7477},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Yu - 2006 - On strong consistency of model selection in classification.pdf:pdf},
	isbn = {1532-4435},
	issn = {00189448},
	journal = {Journal of Machine Learning Research},
	keywords = {Error probability,Hannan and Quinn's procedure,Kullback-Leibler divergence,Law of the iterated logarithm,Model selection,Strong consistency},
	number = {11},
	pages = {4767--4774},
	title = {{On strong consistency of model selection in classification}},
	volume = {52},
	year = {2006}
}
@article{Lacomba2006,
	author = {Lacomba, Juan A. and Lagos, Francisco},
	doi = {10.1007/s11294-006-9045-z},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacomba, Lagos - 2006 - Reinforcing the Link Between Contributions and Pensions The Effect of the Population Aging.pdf:pdf},
	issn = {1083-0898},
	journal = {International Advances in Economic Research},
	keywords = {aging,earnings-related pensions,flexible retirement,social security},
	mendeley-groups = {My Research/Pensions},
	number = {4},
	pages = {530--539},
	title = {{Reinforcing the Link Between Contributions and Pensions: The Effect of the Population Aging}},
	volume = {12},
	year = {2006}
}
@incollection{Timmermann2006,
	abstract = {Forecast combinations have frequently been found in empirical studies to produce better forecasts on average than methods based on the ex ante best individual forecasting model. Moreover, simple combinations that ignore correlations between forecast errors often dominate more refined combination schemes aimed at estimating the theoretically optimal combination weights. In this chapter we analyze theoretically the factors that determine the advantages from combining forecasts (for example, the degree of correlation between forecast errors and the relative size of the individual models' forecast error variances). Although the reasons for the success of simple combination schemes are poorly understood, we discuss several possibilities related to model misspecification, instability (non-stationarities) and estimation error in situations where the number of models is large relative to the available sample size. We discuss the role of combinations under asymmetric loss and consider combinations of point, interval and probability forecasts. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	author = {Timmermann, Allan},
	booktitle = {Handbook of Economic Forecasting},
	chapter = {4},
	doi = {10.1016/S1574-0706(05)01004-9},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Timmermann (2006) Forecast Combinations.pdf:pdf},
	isbn = {9780444513953},
	issn = {15740706},
	keywords = {diversification gains,forecast combinations,model misspecification,pooling and trimming,shrinkage methods},
	mendeley-groups = {Forecasting/Forecast Combination},
	pages = {135--196},
	title = {{Forecast Combinations}},
	volume = {1},
	year = {2006}
}
@article{Verbic2006,
	abstract = {This paper uses a dynamic overlapping-generations (OLG) general equilibrium model to analyze welfare effects in Slovenia, the macroeconomic effects of the Slovenian pension reform, and the effects of the pension fund deficit on the sustainability of Slovenian public finances. Although young and new generations will lose from the pension reform, even complete implementation of reforms might not sufficiently compensate for unfavorable demographic developments. The level of expected deficit for the pay-as-you-go state pension fund seems to be most worrying. Financing the pension system with value-added tax revenues, as an extreme case, could result in more sustainable public finances, because gross domestic product and welfare levels ought to increase; however this might be infeasible to implement politically, given that generations of voters would have their welfare decreased. In addition, the present pension system is opaque and tremendously complicated and primarily, should be made more comprehensible to the public.},
	author = {Verbi{\v{c}}, Miroslav and Majcen, Boris and {Van Nieuwkoop}, Renger},
	doi = {10.2753/EEE0012-8775440403},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbi{\v{c}}, Majcen, Van Nieuwkoop - 2006 - Sustainability of the Slovenian Pension System An Analysis with an Overlapping-Generations Genera.pdf:pdf},
	isbn = {0012-8775},
	issn = {0012-8775},
	journal = {Eastern European Economics},
	mendeley-groups = {My Research/Pensions},
	number = {4},
	pages = {60--81},
	title = {{Sustainability of the Slovenian Pension System: An Analysis with an Overlapping-Generations General Equilibrium Model}},
	volume = {44},
	year = {2006}
}
@inproceedings{Herbohn2006,
	address = {Galway},
	author = {Herbohn, John},
	booktitle = {Small-scale forestry and rural development, the intersection of ecosфystems, economics and society. Proceedings of IUFRO 3.08 Conference, hosted by Galway-Mayo Institute of Technology, Galway, Ireland (2006)},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herbohn - 2006 - Small-scale Forestry – Is it simply a smaller version of Industrial (Large-scale) Multiple Use Foresty.pdf:pdf},
	pages = {158--163},
	title = {{Small-scale Forestry – Is it simply a smaller version of Industrial (Large-scale) Multiple Use Foresty ?}},
	year = {2006}
}
@unpublished{Coloma2006,
	address = {Buenos Aires},
	author = {Coloma, German},
	booktitle = {CEMA Working Papers},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coloma - 2006 - Estimation of demand systems based on elasticities of substitution.pdf:pdf},
	institution = {CEMA University},
	keywords = {carbonated,demand systems,elasticity of substitution,simultaneous equations},
	mendeley-groups = {My Research/Alpha},
	pages = {1--22},
	title = {{Estimation of demand systems based on elasticities of substitution}},
	year = {2006}
}
@article{Hansen2007,
	abstract = {This paper considers the problem of selection of weights for averaging across least squares estimates obtained from a set of models. Existing model average methods are based on exponential Akaike information criterion (AIC) and Bayesian information criterion (BIC) weights. In distinction, this paper proposes selecting the weights by minimizing a Mallows criterion, the latter an estimate of the average squared error from the model average fit. We show that our new Mallows model average (MMA) estimator is asymptotically optimal in the sense of achieving the lowest possible squared error in a class of discrete model average estimators. In a simulation experiment we show that the MMA estimator compares favorably with those based on AIC and BIC weights. The proof of the main result is an application of the work of Li (1987). {\textcopyright} The Econometric Society 2007.},
	author = {Hansen, Bruce E.},
	doi = {10.1111/j.1468-0262.2007.00785.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Hansen (2007) Least Squares Model Averaging.pdf:pdf},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Mallows criterion,Model selection,Optimality,Series estimators},
	mendeley-groups = {Model Selection and Averaging},
	number = {4},
	pages = {1175--1189},
	title = {{Least squares model averaging}},
	volume = {75},
	year = {2007}
}
@article{Karol2007,
	author = {Karol, David and Miguel, Edward},
	doi = {10.1111/j.1468-2508.2007.00564.x},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karol, Miguel - 2007 - Iraq War casualties and the 2004 US presidential election.pdf:pdf},
	issn = {0022-3816},
	journal = {Journal of Politics},
	number = {3},
	pages = {633--648},
	title = {{Iraq War casualties and the 2004 US presidential election}},
	volume = {69},
	year = {2007}
}
@article{Chitnis2007,
	author = {Chitnis, Varsha and Wright, Danaya C},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chitnis, Wright - 2007 - The Legacy of Colonialism Law and Women' s Rights in India The Legacy of Colonialism Law and.pdf:pdf},
	journal = {UF Law Scholarship Repository},
	mendeley-groups = {My Research/Prima},
	title = {{The Legacy of Colonialism : Law and Women' s Rights in India The Legacy of Colonialism : Law and}},
	volume = {1315},
	year = {2007}
}
@article{Hardle2007,
	abstract = {Partially linear models (PLM) are regression models in which the response depends on some covariates linearly but on other covariates nonparametrically. PLMs generalize standard linear regression techniques and are special cases of additive models. This chapter covers the basic results and explains how PLMs are applied in the biometric practice. More specifically, we are mainly concerned with least squares estimators of the linear parameter while the nonparametric part is estimated by e.g. kernel regression, spline approximation, piecewise polynomial and local polynomial techniques. When the model is heteroscedastic, the variance functions are approximated by weighted least squares estimators. Numerous examples illustrate the implementation in practice. PLMs are defined by Y = X-{\`{A}} + g(T) + -{\~{A}}, (5.1) where X and T are d-dimensional and scalar regressors, -{\`{A}} is a vector of unknown parameters, g(-E) an unknown smooth function and -{\~{A}} an error term with mean zero conditional on X and T. The PLM is a special form of the additive regression models Hastie and Tibshrani (1990); Stone (1985), which allows easier interpretation of the effect of each variables and may be preferable to a completely nonparametric regression since the well-known reason -gcurse of dimensionality-h. On the other hand, PLMs are more flexible than the standard linear models since they combine both parametric and nonparametric components. Several methods have been proposed to estimate PLMs. Suppose there are n observations {\{}Xi, Ti, Yi{\}}n i=1. Engle, Granger, Rice and Weiss (1986), Heck88 5 Partially Linear Models man (1986) and Rice (1986) used spline smoothing and defined estimators of -{\`{A}} and g as the solution of argmin -{\`{A}},g 1 n n i=1 {\{}Yi. Xi-{\`{A}}. g(Ti){\}}2 + -{\'{E}} {\{}g(u){\}}2du. (5.2) Speckman (1988) estimated the nonparametric component by W-{\'{A}}, where W is a (n -{\~{}} q).matrix of full rank and -{\'{A}} is an additional parameter. A PLM may be rewritten in a matrix form Y = X-{\`{A}} +W-{\'{A}} + -{\~{A}}. (5.3) The estimator of -{\`{A}} based on (5.3) is-{\`{A}}S = {\{}X(I. PW)X{\}}.1{\{}X(I. PW)Y {\}}, (5.4) where PW = W(WW).1W is a projection matrix and I is a d.order identity matrix. Green, Jennison and Seheult (1985) proposed another class of estimates-{\`{A}}GJS = {\{}X(I.Wh)X){\}}.1{\{}X(I.Wh)Y){\}} by replacing W in (5.4) by another smoother operator Wh. Chen (1988) proposed a piecewise polynomial to approximate nonparametric function and then derived the least squares estimator which is the same form as (5.4). Recently H-Nardle, Liang and Gao (2000) have systematically summarized the different approaches to PLM estimation. No matter which regression method is used for the nonparametric part, the forms of the estimators of -{\`{A}} may always be written as {\{}X(I.W)X{\}}.1{\{}X(I.W)Y {\}}, where W is a projection operation. The estimators are asymptotically normal under appropriate assumptions. The next section will be concerned with several nonparametric fit methods for g(t) because of their popularity, beauty and importance in nonparametric statistics. In Section 5.4, the Framingham heart study data are investigated for illustrating the theory and the proposed statistical techniques. {\textcopyright} 2007 Springer-Verlag Berlin Heidelberg.},
	author = {H{\"{a}}rdle, Wolfgang and Liang, Hua},
	doi = {10.1007/978-3-540-32691-5_5},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/H{\"{a}}rdle, Liang (1999) Partially Linear Models.pdf:pdf},
	isbn = {9783540326908},
	journal = {Statistical Methods for Biostatistics and Related Fields},
	mendeley-groups = {Semiparametric Estimation},
	number = {39562},
	pages = {87--103},
	title = {{Partially linear models}},
	year = {2007}
}
@book{Li2007,
	author = {Li, Qi and Racine, Jeffrey Scott},
	isbn = {9780691121611},
	mendeley-groups = {Nonparametric Estimation,Nonparametric Estimation/Books and Notes},
	publisher = {Princeton University Press},
	title = {{Nonparametric Econometrics: Theory and Practice}},
	year = {2007}
}
@article{Wollenberg2007,
	abstract = {Although community managed forests constitute a significant proportion of the worlds' forests, there is little information about their condition or how they are managed. The International Forestry Resources and Institutions (IFRI) network is a research programme established in 1992 to collect interdisciplinary information about forest sustainability and governance. IFRI is unique in terms of the large number of small-scale sites monitored (more than 350 communities and 9000 forest plots) for more than a decade, under the guidance of strong central leadership, a well defined research framework, relative autonomy of network members, and a strong inward focus. These features have enabled IFRI to have particular impacts on new knowledge, policy and local communities, and capacity building. Lessons about how to further strengthen, extend and sustain these impacts include developing more robust agreement about measures of forest sustainability, building network members' capacities to conduct comparative analysis, ensuring the database meets the needs of multiple users and expanding the membership and outreach of the network.},
	author = {Wollenberg, E and Merino, L and Agrawal, a and Ostrom, E},
	doi = {10.1505/ifor.9.2.670},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wollenberg et al. - 2007 - Fourteen years of monitoring community-managed forests learning from IFRI's experience.pdf:pdf},
	isbn = {1465-5489},
	issn = {1465-5489},
	journal = {International Forestry Review},
	keywords = {community forests,des for{\^{e}}ts g{\'{e}}r{\'{e}}es communautairement,exp{\'{e}}rience,governance,le{\c{c}}ons de l,monitoring,networks,quatorze ans de surveillance,research},
	number = {2},
	pages = {670--684},
	title = {{Fourteen years of monitoring community-managed forests: learning from IFRI's experience}},
	volume = {9},
	year = {2007}
}
@article{Tsakumis2007,
	author = {Tsakumis, George T and Curatola, Anthony P and Porcano, Thomas M},
	doi = {10.1016/j.intaccaudtax.2007.06.004},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsakumis, Curatola, Porcano - 2007 - The relation between national cultural dimensions and tax evasion.pdf:pdf},
	journal = {Journal of International Accounting, Auditing and Taxation},
	keywords = {hofstede,national culture,tax compliance,tax evasion},
	pages = {131--147},
	title = {{The relation between national cultural dimensions and tax evasion}},
	volume = {16},
	year = {2007}
}
@incollection{Golosov2007,
	author = {Golosov, Mikhail and Tsyvinski, Aleh and Werning, Ivan},
	booktitle = {NBER Macroeconomics Annual 2006},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Golosov, Tsyvinski, Werning - 2007 - New Dynamic Public Finance A User's Guide.pdf:pdf},
	isbn = {9780262012393},
	mendeley-groups = {Fiscal Policy},
	number = {May},
	title = {{New Dynamic Public Finance: A User's Guide}},
	volume = {21},
	year = {2007}
}
@article{Bramoulle2007,
	abstract = {This paper considers incentives to provide goods that are non-excludable along social or geographic links. We find, first, that networks can lead to specialization in public good provision. In every social network there is an equilibrium where some individuals contribute and others free ride. In many networks, this extreme is the only outcome. Second, specialization can benefit society as a whole. This outcome arises when contributors are linked, collectively, to many agents. Finally, a new link increases access to public goods, but reduces individual incentives to contribute. Hence, overall welfare can be higher when there are holes in a network. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
	author = {Bramoull{\'{e}}, Yann and Kranton, Rachel},
	doi = {10.1016/j.jet.2006.06.006},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bramoull{\'{e}}, Kranton - 2007 - Public goods in networks.pdf:pdf},
	isbn = {0022-0531},
	issn = {00220531},
	journal = {Journal of Economic Theory},
	keywords = {Experimentation,Independent sets,Information sharing,Strategic substitutes},
	mendeley-groups = {My Research/Theta/Review},
	number = {1},
	pages = {478--494},
	title = {{Public goods in networks}},
	volume = {135},
	year = {2007}
}
@article{Miyagiwa2007,
	abstract = {In the literature studying aggregate economies the aggregate elasticity of substitution (AES) between capital and labor is often treated as a constant or 'deep' parameter. This view contrasts with the conjecture put forward by Arrow et al. [1961. Capital-labor substitution and economic efficiency. Review of Economics and statistics 43, 225-250] that AES evolves over time and changes with the process of economic development. This paper evaluates this conjecture in a simple dynamic multi-sector growth model, in which AES is endogenously determined. Our findings support the conjecture, and in particular demonstrate that AES tends to be positively related to the state of economic development, a result consistent with recent empirical findings. ?? 2006 Elsevier B.V. All rights reserved.},
	author = {Miyagiwa, Kaz and Papageorgiou, Chris},
	doi = {10.1016/j.jedc.2006.06.009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyagiwa, Papageorgiou - 2007 - Endogenous Aggregate Elasticity of Substitution.pdf:pdf},
	isbn = {0165-1889},
	issn = {01651889},
	journal = {Journal of Economic Dynamics and Control},
	keywords = {Aggregate elasticity of substitution,Economic development,Factor-endowment model,Neoclassical growth model},
	mendeley-groups = {My Research/Alpha},
	number = {9},
	pages = {2899--2919},
	title = {{Endogenous Aggregate Elasticity of Substitution}},
	volume = {31},
	year = {2007}
}
@incollection{Chen2007,
	abstract = {Often researchers find parametric models restrictive and sensitive to deviations from the parametric specifications; semi-nonparametric models are more flexible and robust, but lead to other complications such as introducing infinite-dimensional parameter spaces that may not be compact and the optimization problem may no longer be well-posed. The method of sieves provides one way to tackle such difficulties by optimizing an empirical criterion over a sequence of approximating parameter spaces (i.e., sieves); the sieves are less complex but are dense in the original space and the resulting optimization problem becomes well-posed. With different choices of criteria and sieves, the method of sieves is very flexible in estimating complicated semi-nonparametric models with (or without) endogeneity and latent heterogeneity. It can easily incorporate prior information and constraints, often derived from economic theory, such as monotonicity, convexity, additivity, multiplicity, exclusion and nonnegativity. It can simultaneously estimate the parametric and nonparametric parts in semi-nonparametric models, typically with optimal convergence rates for both parts. This chapter describes estimation of semi-nonparametric econometric models via the method of sieves. We present some general results on the large sample properties of the sieve estimates, including consistency of the sieve extremum estimates, convergence rates of the sieve M-estimates, pointwise normality of series estimates of regression functions, root-n asymptotic normality and efficiency of sieve estimates of smooth functionals of infinite-dimensional parameters. Examples are used to illustrate the general results. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Chen, Xiaohong},
	booktitle = {Handbook of Econometrics},
	doi = {10.1016/S1573-4412(07)06076-X},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Semiparametric Estimation/Chen (2007) Chapter 76 Large Sample Sieve Estimation of Semi-Nonparametric Models.pdf:pdf},
	isbn = {9780444532008},
	issn = {15734412},
	keywords = {endogeneity in semi-nonparametric models,semiparametric two-step estimation,series,sieve extremum estimation,sieve minimum distance},
	mendeley-groups = {Semiparametric Estimation},
	number = {SUPPL. PART B},
	pages = {5549--5632},
	title = {{Chapter 76 Large Sample Sieve Estimation of Semi-Nonparametric Models}},
	volume = {6},
	year = {2007}
}
@article{Bao2007,
	abstract = {We develop analytical results on the second-order bias and mean squared error of estimators in time-series models. These results provide a unified approach to developing the properties of a large class of estimators in linear and nonlinear time-series models and they are valid for both normal and nonnormal samples of observations, and where the regressors are stochastic. The estimators included are the generalized method of moments, maximum likelihood, least squares, and other extremum estimators. Our general results are applied to four time-series models. We investigate the effects of nonnormality on the second-order bias results for two of these models, while for all four models, the second-order bias and mean squared error results are given under normality. Numerical results for some of these models are also presented. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	author = {Bao, Yong and Ullah, Aman},
	doi = {10.1016/j.jeconom.2006.07.007},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Bao, Ullah (2007) The second-order bias and mean squared error of estimators in time-series models.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Higher-order moments,Stochastic expansion,Time series},
	mendeley-groups = {Small Sample Results/Nonlinear},
	number = {2},
	pages = {650--669},
	title = {{The second-order bias and mean squared error of estimators in time-series models}},
	volume = {140},
	year = {2007}
}
@article{VanBiesebroeck2007,
	author = {{Van Biesebroeck}, Johannes},
	doi = {10.1111/j.1467-6451.2007.00322.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Van Biesebroeck (2007) Robusntess of Productivity Estimates.pdf:pdf},
	journal = {The Journal of Industrial Economics},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {3},
	pages = {529--569},
	title = {{Robustness of Productivity Estimates}},
	volume = {LV},
	year = {2007}
}
@incollection{Ackerberg2007EconometricToolsAnalyzing,
  title = {Econometric {{Tools}} for {{Analyzing Market Outcomes}}},
  booktitle = {Handbook of {{Econometrics}}},
  author = {Ackerberg, Daniel and Lanier Benkard, C. and Berry, Steven and Pakes, Ariel},
  year = {2007},
  volume = {6},
  pages = {4171--4276},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4412(07)06063-1},
  urldate = {2024-11-13},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  isbn = {978-0-444-50631-3},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Econometrics)\Production Function Estimation\Ackerberg et al. - 2007 - Econometric Tools for Analyzing Market Outcomes.pdf}
}
 
@article{Hansen2008,
	author = {Hansen, Bruce E.},
	doi = {10.1016/j.jeconom.2008.08.022},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Hansen (2008) Least-squares forecast averaging..pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Forecasting,Model Selection and Averaging,Forecasting/Forecast Combination},
	number = {2},
	pages = {342--350},
	publisher = {Elsevier B.V.},
	title = {{Least-squares forecast averaging}},
	volume = {146},
	year = {2008}
}
@article{Lorenzoni2008,
	author = {Lorenzoni, Guido},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorenzoni - 2008 - Inefficient Credit Booms.pdf:pdf},
	journal = {The Review of Economic Studies},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {3},
	pages = {809--833},
	title = {{Inefficient Credit Booms}},
	volume = {75},
	year = {2008}
}
@article{Barnett2008,
	abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Barnett, William A. and Seck, Ousmane},
	doi = {10.1002/jae.1009},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnett, Seck - 2008 - Rotterdam model versus almost ideal demand system Will the best specification please stand up.pdf:pdf},
	isbn = {9780874216561},
	issn = {08837252},
	journal = {Journal of Applied Econometrics},
	keywords = {almost ideal demand system,c22,c3,c43,carlo study,consumer demand system,e41,flexible functional forms,g12,jel classifications,monte,rotterdam model},
	mendeley-groups = {My Research/Alpha},
	number = {6},
	pages = {795--824},
	pmid = {15991970},
	title = {{Rotterdam model versus almost ideal demand system: Will the best specification please stand up?}},
	volume = {23},
	year = {2008}
}
@article{Amendola2008,
	abstract = {A novel approach to the combination of volatility forecasts is discussed. The proposed procedure makes use of the generalized method of moments (GMM) for estimating the combination weights. The asymptotic properties of the GMM estimator are derived while its finite sample properties are assessed by means of a simulation study. The results of an application to a time series of daily returns on the S{\&}P500 are presented. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Amendola, Alessandra and Storti, Giuseppe},
	doi = {10.1016/j.csda.2007.10.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Amendola, Storti (2008) A GMM procedure for combining volatility forecasts.pdf:pdf},
	issn = {01679473},
	journal = {Computational Statistics and Data Analysis},
	keywords = {Forecast combination,GARCH,GMM,Volatility},
	mendeley-groups = {Forecasting/Forecast Combination},
	number = {6},
	pages = {3047--3060},
	title = {{A GMM procedure for combining volatility forecasts}},
	volume = {52},
	year = {2008}
}
@article{Chade2008,
	abstract = {We study infinitely repeated games with perfect monitoring, where players have $\beta$-$\delta$ preferences. We compute the continuation payoff set using recursive techniques and then characterize equilibrium payoffs. We then explore the cost of the present-time bias, producing comparative statics. Unless the minimax outcome is a Nash equilibrium of the stage game, the equilibrium payoff set is not monotonic in $\beta$ or $\delta$. Finally, we show how the equilibrium payoff set is contained in that of a repeated game with smaller discount factor. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
	author = {Chade, Hector and Prokopovych, Pavlo and Smith, Lones},
	doi = {10.1016/j.jet.2007.06.007},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chade, Prokopovych, Smith - 2008 - Repeated Games with Present-Biased Preferences.pdf:pdf},
	issn = {00220531},
	journal = {Journal of Economic Theory},
	keywords = {Perfect monitoring,Repeated games,Strotz-Pollak equilibrium,Subgame perfect equilibrium,$\beta$-$\delta$ preferences},
	number = {1},
	pages = {157--175},
	title = {{Repeated Games with Present-Biased Preferences}},
	volume = {139},
	year = {2008}
}
@article{HashemPesaran2008,
	abstract = {This paper proposes a standardized version of Swamy's test of slope homogeneity for panel data models where the cross section dimension (N) could be large relative to the time series dimension (T). The proposed test, denoted by over($\Delta$, ̃), exploits the cross section dispersion of individual slopes weighted by their relative precision. In the case of models with strictly exogenous regressors, but with non-normally distributed errors, the test is shown to have a standard normal distribution as (N, T)over(→, j) ∞ such that sqrt(N) / T2 → 0. When the errors are normally distributed, a mean-variance bias adjusted version of the test is shown to be normally distributed irrespective of the relative expansion rates of N and T. The test is also applied to stationary dynamic models, and shown to be valid asymptotically so long as N / T → $\kappa$, as (N, T) over(→, j) ∞, where 0 ≤ $\kappa$ {\textless} ∞. Using Monte Carlo experiments, it is shown that the test has the correct size and satisfactory power in panels with strictly exogenous regressors for various combinations of N and T. Similar results are also obtained for dynamic panels, but only if the autoregressive coefficient is not too close to unity and so long as T ≥ N. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Pesaran, M. Hashem and Yamagata, Takashi},
	doi = {10.1016/j.jeconom.2007.05.010},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Pesaran, Yamagata (2008). Testing slope homogeneity in large panels.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Dispersion tests,Large panels,Monte Carlo results,Tests of slope homogeneity},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {50--93},
	title = {{Testing slope homogeneity in large panels}},
	volume = {142},
	year = {2008}
}
@article{Murtazashvili2008,
	abstract = {We provide a set of conditions sufficient for consistency of a general class of fixed effects instrumental variables (FE-IV) estimators in the context of a correlated random coefficient panel data model, where one ignores the presence of individual-specific slopes. We discuss cases where the assumptions are met and violated. Monte Carlo simulations verify that the FE-IV estimator of the population averaged effect performs notably better than other standard estimators, provided a full set of period dummies is included. We also propose a simple test of selection bias in unbalanced panels when we suspect the slopes may vary by individual. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	author = {Murtazashvili, Irina and Wooldridge, Jeffrey M.},
	doi = {10.1016/j.jeconom.2007.09.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Murtazashvili, Wooldridge (2008) Fixed effects instrumental variables estimation in correlated random coefficient panel data models.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Average treatment effect,Correlated random coefficient model,Fixed effects,Instrumental variables,Population averaged effect},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {539--552},
	title = {{Fixed effects instrumental variables estimation in correlated random coefficient panel data models}},
	volume = {142},
	year = {2008}
}
@article{Marcellino2008,
	abstract = {The literature on model comparison often requires the assumption that the true conditional distribution corresponds to that of one of the competing models. This strong assumption has been extended by the notion of encompassing and in likelihood based model comparisons. This paper takes the latter approach and develops tests for the comparison of competing nonlinear dynamic models, focusing on the nested and overlaping cases. The null hypothesis is that the models are equally close to the data generating process (DGP), according to a certain measure of closeness. The alternative is that one model is closer to the DGP. The models can be correctly specified or not. Their parameters can be estimated by a variety of methods, including (pseudo) maximum likelihood and ordinary least squares. The tests are symmetric and directional. Their asymptotic distribution under the null is either normal or a weighted sum of chi-squared distributions, depending on the nesting characteristics of the competing models. The comparison of nested AR models, and of nested ARMA models with GARCH errors and exogenous forcing variables (ARMAX-GARCH) are discussed as examples.},
	author = {Marcellino, Massimiliano and Rossi, Barbara},
	doi = {10.1111/j.1468-0084.2008.00534.x},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcellino, Rossi - 2008 - Model Selection for Nested and Overlapping Nonlinear, Dynamic and Possibly Mis-specified Models.pdf:pdf},
	issn = {03059049},
	journal = {Oxford Bulletin of Economics and Statistics},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {SUPPL. 1},
	pages = {867--893},
	title = {{Model Selection for Nested and Overlapping Nonlinear, Dynamic and Possibly Mis-specified Models}},
	volume = {70},
	year = {2008}
}
@article{Eenennaam2008,
	author = {Eenennaam, Alison Van},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eenennaam - 2008 - Genetically Engineered Animals an Overview.pdf:pdf},
	mendeley-groups = {My Teaching/Agro},
	number = {August},
	pages = {1--6},
	title = {{Genetically Engineered Animals: an Overview}},
	year = {2008}
}
@article{Warton2008,
	abstract = {High dimensionality causes problems in various areas of statistics. A particular situation that rarely has been considered is the testing of hypotheses about multivariate regression models in which the dimension of the multivariate response is large. In this article a ridge regularization approach is proposed in which either the covariance or the correlation matrix is regularized to ensure nonsingularity irrespective of the dimensionality of the data. It is shown that the proposed approach can be derived through a penalized likelihood approach, which suggests cross-validation of the likelihood function as a natural approach for estimation of the ridge parameter. Useful properties of this likelihood estimator are derived, discussed, and demonstrated by simulation. For a class of test statistics commonly used in multivariate analysis, the proposed regularization approach is compared with some obvious alternative regularization approaches using generalized inverse and data reduction through principal compone...},
	author = {Warton, David I.},
	doi = {10.1198/016214508000000021},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Warton - 2008 - Penalized normal likelihood and ridge regularization of correlation and covariance matrices.pdf:pdf},
	isbn = {0162-1459},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Cross-validation,Generalized inverse,High-dimensional data,Multivariate analysis,Multivariate regression},
	mendeley-groups = {UPF/ADTSI},
	number = {481},
	pages = {340--349},
	title = {{Penalized normal likelihood and ridge regularization of correlation and covariance matrices}},
	volume = {103},
	year = {2008}
}
@incollection{Anselin2008,
	author = {Anselin, Luc and Gallo, Julie Le and Jayet, Hubert},
	booktitle = {The Econometrics of Panel Data},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anselin, Gallo, Jayet - 2008 - Spatial Panel Econometrics.pdf:pdf},
	mendeley-groups = {Panel Data/Spatial Panels},
	pages = {625--660},
	title = {{Spatial Panel Econometrics}},
	year = {2008}
}
@article{Bickel2008a,
	abstract = {This paper considers regularizing a covariance matrix of {\$}p{\$} variables estimated from {\$}n{\$} observations, by hard thresholding. We show that the thresholded estimate is consistent in the operator norm as long as the true covariance matrix is sparse in a suitable sense, the variables are Gaussian or sub-Gaussian, and {\$}(\backslashlog p)/n\backslashto0{\$}, and obtain explicit rates. The results are uniform over families of covariance matrices which satisfy a fairly natural notion of sparsity. We discuss an intuitive resampling scheme for threshold selection and prove a general cross-validation result that justifies this approach. We also compare thresholding to other covariance estimators in simulations and on an example from climate data.},
	archivePrefix = {arXiv},
	arxivId = {0901.3079},
	author = {Bickel, Peter J. and Levina, Elizaveta},
	doi = {10.1214/08-AOS600},
	eprint = {0901.3079},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bickel, Levina - 2008 - Covariance regularization by thresholding.pdf:pdf},
	isbn = {0090-5364},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Covariance estimation,High dimension low sample size.,Large p small n,Regularization,Sparsity,Thresholding},
	mendeley-groups = {UPF/ADTSI},
	number = {6},
	pages = {2577--2604},
	pmid = {15863380},
	title = {{Covariance regularization by thresholding}},
	volume = {36},
	year = {2008}
}
@book{Claeskens2008,
	abstract = {Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer? Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled, with discussions of frequentist and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R code.},
	address = {Cambridge},
	author = {Claeskens, Gerda and Hjort, Nils Lid},
	booktitle = {Cambridge Series in Statistical and Probabilistic Mathematics},
	doi = {10.1017/CBO9780511790485},
	isbn = {9780521852258},
	mendeley-groups = {Model Selection and Averaging},
	publisher = {Cambridge University Press},
	title = {{Model Selection and Model Averaging}},
	year = {2008}
}
@inproceedings{Vedenov2008,
	address = {Orlando},
	author = {Vedenov, Dmitry},
	booktitle = {Proceedings of the American Agricultural Association Annual Meeting},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vedenov - 2008 - Application of copulas to estimation of joint crop yield distributions.pdf:pdf},
	mendeley-groups = {My Research/Theta/Review},
	pages = {1--24},
	title = {{Application of copulas to estimation of joint crop yield distributions}},
	year = {2008}
}
@book{Kosorok2008,
	author = {Kosorok, Michael R.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kosorok - 2008 - Introduction to Empirical Processes and Semiparametric Inference.pdf:pdf},
	isbn = {9780387749778},
	mendeley-groups = {UPF/Research Seminar},
	publisher = {Springer Series in Statistics},
	title = {{Introduction to Empirical Processes and Semiparametric Inference}},
	year = {2008}
}
@article{Kyriazidou2008,
	author = {Kyriazidou, Ekaterini},
	doi = {10.1057/978-1-349-95121-5},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Kyriazidou (2008) Palgrave Nonlinear Panel Data Models.pdf:pdf},
	isbn = {9781349951215},
	keywords = {condition-,conditional maximum likelihood,fixed effects,heteroskedasticity,incidental parameters,individual effects,ing variables,logit},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	pages = {1--11},
	title = {{Nonlinear Panel Data Models}},
	year = {2008}
}
@article{Yasar2008,
	author = {Yasar, Mahmut and Raciborski, Rafal and Poi, Brian},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Yasar, Raciborski, Poi (2008) Production function estimation in Stata using OP.pdf:pdf},
	journal = {The Stata Journal},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {2},
	pages = {221--231},
	title = {{Production function estimation in Stata using the Olley and Pakes method}},
	volume = {8},
	year = {2008}
}
@article{Caballero2008,
	abstract = {Three of the most important recent facts in global macroeconomics --the sustained rise in the US current account deficit, the stubborn decline in long run real rates, and the rise in the share of US assets in global portfolio --appear as anomalies from the perspective of conventional wisdom and models. Instead, in this paper we provide a model that rationalizes these facts as an equilibrium outcome of two observed forces: a) potential growth differentials among different regions of the world and, b) heterogeneity in these regions' capacity to generate financial assets from real investments. In extensions of the basic model, we also generate exchange rate and FDI excess returns which are broadly consistent with the recent trends in these variables. Unlike the conventional wisdom, in the absence of a large change in (a) or (b), our model does not augur any catastrophic event. More generally, the framework is flexible enough to shed light on a range of scenarios in a global equilibrium environment.},
	author = {Caballero, Ricardo J and Farhi, Emmanuel and Gourinchas, Pierre-Olivier},
	doi = {10.3386/w11996},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caballero, Farhi, Gourinchas - 2008 - An Equilibrium Model of Global Imbalances and Low Interest Rates.pdf:pdf},
	journal = {American Economic Review},
	keywords = {asymmetries,capital flows,current account deficits,exchange rates,global portfolios and equilibrium,gross flows,growth and financial development,interest rates,metzler diagram},
	mendeley-groups = {UPF/Adv. Macro III},
	number = {1},
	pages = {358--93},
	title = {{An Equilibrium Model of "Global Imbalances" and Low Interest Rates}},
	volume = {98},
	year = {2008}
}
@article{Guiso2008,
	author = {Guiso, Luigi and Paiella, Monica},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guiso, Paiella - 2008 - Risk Aversion, Wealth, and Background Risk.pdf:pdf},
	journal = {Journal of the European Economic Association},
	number = {December},
	pages = {1109--1150},
	title = {{Risk Aversion, Wealth, and Background Risk}},
	volume = {6},
	year = {2008}
}
@incollection{Arellano2008,
	abstract = {The purpose of this paper is to review recently development methods of estimation of nonlinear fixed effects panel data models with reduced bias properties. We begin by describing fixed effects estimators and the incidental parameters problem. Next the explain how to construct analytical bias correction of estimators, followed by bias correction of estimators, followed by bias correction of the moment equation, and bias corrections for the concentrated likelihood. We then turn to discuss other approaches leading to bias correction based on orthogonalization and their extensions. The remaining sections consider quasi maximum likelihood estimation for dynamic models, the estimation of marginal effects, and automatic methods based on simulation.},
	author = {Arellano, Manuel and Hahn, Jinyong},
	booktitle = {Advances in Economics and Econometrics},
	chapter = {12},
	doi = {10.1017/ccol0521871549.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Arellano, Hahn (2006) Understanding Bias in Nonlinear Panel Models - Some Recent Developments.pdf:pdf},
	mendeley-groups = {Panel Data/Large Panels,Panel Data/Nonlinear Panels},
	number = {August},
	pages = {381--409},
	title = {{Understanding Bias in Nonlinear Panel Models: Some Recent Developments}},
	year = {2008}
}
@article{Fan2008,
	abstract = {High dimensionality comparable to sample size is common in many statistical problems. We examine covariance matrix estimation in the asymptotic framework that the dimensionality p tends to ∞ as the sample size n increases. Motivated by the Arbitrage Pricing Theory in finance, a multi-factor model is employed to reduce dimensionality and to estimate the covariance matrix. The factors are observable and the number of factors K is allowed to grow with p. We investigate the impact of p and K on the performance of the model-based covariance matrix estimator. Under mild assumptions, we have established convergence rates and asymptotic normality of the model-based estimator. Its performance is compared with that of the sample covariance matrix. We identify situations under which the factor approach increases performance substantially or marginally. The impacts of covariance matrix estimation on optimal portfolio allocation and portfolio risk assessment are studied. The asymptotic results are supported by a thorough simulation study. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {math/0701124},
	author = {Fan, Jianqing and Fan, Yingying and Lv, Jinchi},
	doi = {10.1016/j.jeconom.2008.09.017},
	eprint = {0701124},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Fan, Lv - 2008 - High dimensional covariance matrix estimation using a factor model.pdf:pdf},
	isbn = {03044076},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Asymptotic properties,Covariance matrix estimation,Diverging dimensionality,Factor model,Portfolio management},
	mendeley-groups = {UPF/ADTSI},
	number = {1},
	pages = {186--197},
	pmid = {11232919},
	primaryClass = {math},
	publisher = {Elsevier B.V.},
	title = {{High dimensional covariance matrix estimation using a factor model}},
	volume = {147},
	year = {2008}
}
@article{Lecue2008,
	author = {Lecu{\'{e}}, Guillaume and Mendelson, Shahar},
	doi = {10.1007/s00440-008-0180-8},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecu{\'{e}}, Mendelson - 2008 - Aggregation via empirical risk minimization.pdf:pdf},
	journal = {Probability Theory and Related Fields},
	mendeley-groups = {UPF/Research Seminar},
	number = {September},
	title = {{Aggregation via empirical risk minimization}},
	year = {2008}
}
@article{Trip2008,
	author = {Trip, Aili Mari and Kang, Alice},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trip, Kang - 2008 - The Global Impact of Quotas.pdf:pdf},
	journal = {Comparative Political Studies},
	keywords = {cross-national,legislatures,political representation,quotas,women},
	mendeley-groups = {My Research/Prima},
	number = {3},
	pages = {338--361},
	title = {{The Global Impact of Quotas}},
	volume = {41},
	year = {2008}
}
@article{Baik2008,
	abstract = {Abstract  We examine the equilibrium effort levels of individual players and groups in contests in which n groups compete to win a group-specific public-good prize, the individual players choose their effort levels simultaneously and independently, and the probability of winning for each group depends on the groups' effort levels. In the basic model, we show that, in each group, only the highest-valuation players expend positive effort and the rest expend zero effort; there is underinvestment in the contest for the group as a whole. Next, in the main model in which the players are budget-constrained, we show that low-valuation players free ride on high-valuation players' contributions, not vice versa, but the free-rider problem is “alleviated” as compared with the basic model.},
	author = {Baik, Kyung Hwan},
	doi = {10.1007/s00355-007-0226-3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baik - 2008 - Contests with group-specific public-good prizes.pdf:pdf},
	issn = {01761714},
	journal = {Social Choice and Welfare},
	number = {1},
	pages = {103--117},
	title = {{Contests with group-specific public-good prizes}},
	volume = {30},
	year = {2008}
}
@article{Bickel2008,
	abstract = {This paper considers estimating a covariance matrix of {\$}p{\$} variables from {\$}n{\$} observations by either banding or tapering the sample covariance matrix, or estimating a banded version of the inverse of the covariance. We show that these estimates are consistent in the operator norm as long as {\$}(\backslashlog p)/n\backslashto0{\$}, and obtain explicit rates. The results are uniform over some fairly natural well-conditioned families of covariance matrices. We also introduce an analogue of the Gaussian white noise model and show that if the population covariance is embeddable in that model and well-conditioned, then the banded approximations produce consistent estimates of the eigenvalues and associated eigenvectors of the covariance matrix. The results can be extended to smooth versions of banding and to non-Gaussian distributions with sufficiently short tails. A resampling approach is proposed for choosing the banding parameter in practice. This approach is illustrated numerically on both simulated and real data.},
	archivePrefix = {arXiv},
	arxivId = {0803.1909},
	author = {Bickel, Peter J. and Levina, Elizaveta},
	doi = {10.1214/009053607000000758},
	eprint = {0803.1909},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bickel, Levina - 2008 - Regularized estimation of large covariance matrices.pdf:pdf},
	isbn = {0090-5364},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Banding,Cholesky decomposition,Covariance matrix,Regularization},
	mendeley-groups = {UPF/ADTSI},
	number = {1},
	pages = {199--227},
	pmid = {26386327},
	title = {{Regularized estimation of large covariance matrices}},
	volume = {36},
	year = {2008}
}
@article{Bolton2009,
	abstract = {BACKGROUND: The impact of unemployment on behaviours such as smoking, drinking and body weight has been extensively researched. However, little is known about the possible protective effects of social assistance programs on these behavioural changes. This study examines the impact of unemployment periods on smoking, drinking and body weight changes among re-employed individuals and investigates whether the receipt of unemployment benefits influences these behaviours.$\backslash$n$\backslash$nMETHODS: This study used panel data provided by the Panel Study of Income Dynamics. Logistic regression models were used to analyze whether a period of unemployment in 2000 resulted in an increase in smoking and drinking or fluctuations in body weight among 2001 re-employed individuals in comparison with 1999 baseline levels. A total of 3,451 respondents who had been initially healthy and who had been continuously employed between 1998 and 1999 were included in the analysis.$\backslash$n$\backslash$nRESULTS: Compared to stably employed respondents, those who had experienced periods of unemployment in 2000 and did not receive unemployment benefits were more likely than continuously employed individuals to report an increase in alcohol consumption (OR 1.8, 95{\%} CI 1.0-3.1) and a decrease in body weight (OR 1.7, 95{\%} CI 1.1-2.8) when they were already re-employed in 2001.$\backslash$n$\backslash$nCONCLUSION: Our findings suggest that the receipt of unemployment benefits confers a protective effect on health behavioural changes following periods of unemployment. These findings underscore the need to monitor the impact of unemployment assistance programs on health, particularly in light of the rapidly changing structure of employment and unemployment benefits.},
	author = {Bolton, Kelly L and Rodriguez, Eunice},
	doi = {10.1186/1471-2458-9-77},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolton, Rodriguez - 2009 - Smoking, drinking and body weight after re-employment does unemployment experience and compensation make a di.pdf:pdf},
	issn = {1471-2458},
	journal = {BMC public health},
	keywords = {Adult,Alcohol Drinking,Alcohol Drinking: epidemiology,Alcohol Drinking: psychology,Body Weight,Female,Health Behavior,Humans,Logistic Models,Longitudinal Studies,Male,Michigan,Michigan: epidemiology,Public Assistance,Public Assistance: economics,Smoking,Smoking: epidemiology,Smoking: psychology,Social Support,Socioeconomic Factors,Stress, Psychological,Stress, Psychological: complications,Stress, Psychological: economics,Stress, Psychological: psychology,Unemployment,Unemployment: psychology,Unemployment: statistics {\&} numerical data},
	mendeley-groups = {My Research/Zeta},
	number = {1},
	pages = {77},
	pmid = {19267893},
	title = {{Smoking, drinking and body weight after re-employment: does unemployment experience and compensation make a difference?}},
	volume = {9},
	year = {2009}
}
@article{Claessens2009,
	author = {Claessens, Stijn and Kose, Ayhan M. and Terrones, Marco},
	doi = {10.1111/j.1468-0327.2009.00231.x},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Claessens, Kose, Terrones - 2009 - What Happens During Recessions, Crunches and Busts.pdf:pdf},
	journal = {Economic Policy},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {1},
	pages = {655--700},
	title = {{What Happens During Recessions, Crunches and Busts?}},
	volume = {October},
	year = {2009}
}
@unpublished{Won2009,
	author = {Won, Joong-ho and Lim, Johan and Kim, Seung-jean},
	booktitle = {Stanford Department of Statistics Technical Reports},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Won, Lim, Kim - 2009 - Maximum Likelihood Covariance Estimation With a Condition Number Constraint.pdf:pdf},
	mendeley-groups = {UPF/ADTSI},
	pages = {1--45},
	title = {{Maximum Likelihood Covariance Estimation With a Condition Number Constraint}},
	year = {2009}
}
@article{Kocherlakota2009,
	author = {Kocherlakota, Narayana R},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kocherlakota - 2009 - The New Dynamic Public Finance.pdf:pdf},
	mendeley-groups = {Fiscal Policy},
	title = {{The New Dynamic Public Finance}},
	year = {2009}
}
@article{Gonand2009,
	abstract = {This paper assesses the impact on growth and the inter-generational redistributive effects of some possible pension reforms in France using a dynamic general equilibrium model with overlapping generations. Results suggest that a reform increasing the effective average retirement age by 1.25 years per decade and diminishing the average replacement rate by around 6 percentage points up to 2020 could stabilise social contributions for the youngest, but would gradually foster the poverty rate among pensioners. On the other hand, a reform incorporating a rise in the age of retirement by 1.25 years per decade, unchanged replacement rates and a discretionary increase of all pensions of 2.5 per cent in 2008 would limit the poverty rate among pensioners, but it would weigh down on the growth rate and the welfare of future generations.},
	author = {Gonand, Fr{\'{e}}d{\'{e}}ric and Legros, Florence},
	doi = {10.1057/gpp.2009.29},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonand, Legros - 2009 - Pension reforms in france Impact on growth and inter-generational redistributive effects.pdf:pdf},
	isbn = {1018-5895},
	issn = {10185895},
	journal = {Geneva Papers on Risk and Insurance: Issues and Practice},
	keywords = {Age of retirement,Intergenerational fairness,Pensions},
	mendeley-groups = {My Research/Pensions},
	number = {4},
	pages = {639--659},
	title = {{Pension reforms in france: Impact on growth and inter-generational redistributive effects}},
	volume = {34},
	year = {2009}
}
@book{Horowitz2009,
	author = {Horowitz, Joel L.},
	doi = {10.1007/978-0-387-92870-8},
	isbn = {978-0-387-92869-2},
	mendeley-groups = {Semiparametric Estimation},
	pages = {276},
	publisher = {Springer Series in Statistics},
	title = {{Semiparametric and Nonparametric Methods in Econometrics}},
	year = {2009}
}
@book{Tsybakov2009,
	author = {Tsybakov, Alexandre},
	doi = {10.1007/b13794},
	isbn = {978-0-387-79051-0},
	mendeley-groups = {Nonparametric Estimation,Nonparametric Estimation/Books and Notes},
	publisher = {Springer Series in Statistics},
	title = {{Introduction to Nonparametric Estimation}},
	year = {2009}
}
@article{Ozaki2009,
	abstract = {Over the years, crop insurance programs became the focus of agricultural policy in theUSA,Spain,Mexico, and more recently in Brazil. Given the increasing interest in insurance, accurate calculation of the premium rate is of great importance.We address the crop-yield distribution issue and its implications in pricing an insurance contract considering the dynamic structure of the data and incorporating the spatial correlation in the Hierarchical Bayesian framework. Results showthat empirical (insurers) rates are higher in lowrisk areas and lower in high risk areas. Such methodological improvement is primarily important in situations of limited data.},
	author = {Ozaki, Vitor Augusto and Silva, Ralph S.},
	doi = {10.1080/02664760802474256},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ozaki, Silva - 2009 - Bayesian ratemaking procedure of crop insurance contracts with skewed distribution.pdf:pdf},
	isbn = {0266476080247},
	issn = {0266-4763},
	journal = {Journal of Applied Statistics},
	mendeley-groups = {My Research/Theta/Review},
	number = {4},
	pages = {443--452},
	title = {{Bayesian ratemaking procedure of crop insurance contracts with skewed distribution}},
	volume = {36},
	year = {2009}
}
@article{Ponomarenko2009,
	author = {Ponomarenko, Alexey and Porshakov, Alexey and Vasilieva, Elena},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponomarenko, Porshakov, Vasilieva - 2009 - Overnight interest rates and liquidity of the Russian money market the impact of financial tu.pdf:pdf},
	journal = {BOS Madrid Conference},
	title = {{Overnight interest rates and liquidity of the Russian money market: the impact of financial turmoil}},
	year = {2009}
}
@article{Wooldridge2009,
	author = {Wooldridge, Jeffrey M},
	doi = {10.1016/j.econlet.2009.04.026},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Wooldridge (2009) On estimating firm-level production functions using proxy variables to control for unobservables.pdf:pdf},
	issn = {0165-1765},
	journal = {Economics Letters},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {3},
	pages = {112--114},
	publisher = {Elsevier B.V.},
	title = {{On Estimating Firm-Level Production Functions Using Proxy Variables to Control for Unobservables}},
	volume = {104},
	year = {2009}
}
@article{Fernandez-Val2009,
	author = {Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Fern{\'{a}}ndez-Val (2009) Fixed effects estimation of structural parameters and marginal effects in panel probit models.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {bias,discrete choice models,fixed effects,incidental parameters prob-,labor force participation,lem,panel data,probit},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {71--85},
	title = {{Fixed Effects Estimation of Structural Parameters and Marginal Effects in Panel Probit Models}},
	volume = {150},
	year = {2009}
}
@article{Saito2009,
	abstract = {■ The aim of this study is three-fold: (1) to examine the similarities and differences between instructor and peer assessments of EFL group presentations; (2) to understand the utility of peer assessment for discriminating each group member's contribution to group presentations in college EFL classrooms; and (3) to investigate the relationship between the quality of a group product and group cooperation, each of which were measured by peer assessments. Eighty-three Japanese freshmen worked in groups to create TV commercials. They then completed two types of peer assessment. Results indicated an overall similarity between peer and instructor assessments, along with some notable differences in item difficulties. Most group members succeeded in differentiating the degree of each member's contribution to the group project. Strong support, however, was not found for the assumption that groups with high cooperators produced quality group presentations. Although these results are encouraging for using peer assessment for EFL group presentations, some caution is advised.},
	author = {Saito, Hidetoshi and Fujita, Tomoko},
	doi = {10.1177/0033688209105868},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saito, Fujita - 2009 - Peer-Assessing Peers' Contribution to EFL Group Presentations.pdf:pdf},
	issn = {0033-6882},
	journal = {RELC Journal},
	keywords = {group contributions,peer assessment,presentations,tv commercials},
	number = {2},
	pages = {149--171},
	title = {{Peer-Assessing Peers' Contribution to EFL Group Presentations}},
	volume = {40},
	year = {2009}
}
@article{Material2009,
	author = {Material, Econometrica Supplementary},
	doi = {10.3982/ECTA6135},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Bai (2009) Interactive Fixed Effects Supplement.pdf:pdf},
	mendeley-groups = {Panel Data/Factor Models and Interactive Effects},
	number = {4},
	title = {{Supplement to “panel data models with interactive fixed effects”: technical details and proofs (}},
	volume = {77},
	year = {2009}
}
@article{Bai2009,
	author = {Bai, Jushan},
	doi = {10.3982/ECTA6135},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Bai (2009) Interactive Fixed Effects.pdf:pdf},
	journal = {Econometrica},
	keywords = {Additive effects, interactive effects, factor erro,factor erro,interactive effects},
	mendeley-groups = {Panel Data,Panel Data/Factor Models and Interactive Effects},
	number = {4},
	pages = {1229--1279},
	title = {{Panel Data Models With Interactive Fixed Effects}},
	volume = {77},
	year = {2009}
}
@article{Sayman2009,
	abstract = {Preference between two future outcomes may change over timea phenomenon labeled as time inconsistency. The term "time inconsistency" is usually used to refer to cases in which a larger-later outcome is preferred over a smaller-sooner one when both are delayed by some time, but then with the passage of time a preference switches to the smaller-sooner outcome. The current paper presents four empirical studies showing that time inconsistency in the other direction is also possible: A person may prefer the smaller-sooner outcome when both options are in the future, but decide to wait for the larger-later one when the smaller option becomes immediately available. We find that such "reverse time inconsistency" is more likely to be observed when the delays to and between the two outcomes are short (up to a week). We propose that reverse time inconsistency may be associated with a reversed-S shape discount function, and provide evidence that such a discount function captures part of the variation in intertemporal preferences.},
	archivePrefix = {arXiv},
	arxivId = {physics/0204029},
	author = {Sayman, Serdar and {\"{O}}nc{\"{u}}ler, Ayse},
	doi = {10.1287/mnsc.1080.0942},
	eprint = {0204029},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sayman, {\"{O}}nc{\"{u}}ler - 2009 - An Investigation of Time Inconsistency.pdf:pdf},
	isbn = {0025-1909},
	issn = {0025-1909},
	journal = {Management Science},
	keywords = {2006,2008,accepted by george wu,decision analysis,history,in advance december 17,intertemporal preferences,months for 4 revisions,published online in articles,received april 28,reverse time inconsistency,the authors,this paper was with,time inconsistency,time preference},
	number = {3},
	pages = {470--482},
	pmid = {36895146},
	primaryClass = {physics},
	title = {{An Investigation of Time Inconsistency}},
	volume = {55},
	year = {2009}
}
@article{Hewitt2009,
	author = {Hewitt, Daphne and Delgadillo, Monica Castro},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hewitt, Delgadillo - 2009 - Key factors for successful community-corporate partnerships – Results of a comparative analysis among Latin.pdf:pdf},
	keywords = {commercial partnership,community development,company-community relationship,competitiveness,forestry},
	number = {October},
	pages = {18--23},
	title = {{Key factors for successful community-corporate partnerships – Results of a comparative analysis among Latin American cases}},
	year = {2009}
}
@article{McGee2009,
	author = {McGee, Robert W and Gelman, Wendy},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McGee, Gelman - 2009 - Opinions of the Ethics of Tax Evasion.pdf:pdf},
	journal = {Akron Tax Journal},
	number = {2005},
	pages = {69--92},
	title = {{Opinions of the Ethics of Tax Evasion}},
	volume = {289},
	year = {2009}
}
@article{Judson2009,
	author = {Judson, Ruth and Klee, Elizabeth},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Judson, Klee - 2009 - Whither the Liquidity Effect The impact of Federal Reserve open market operations in recent years.pdf:pdf},
	journal = {Federal Reserve Board WP},
	title = {{Whither the Liquidity Effect: The impact of Federal Reserve open market operations in recent years}},
	volume = {25},
	year = {2009}
}
@article{Rysman2009,
	author = {Rysman, Marc},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rysman - 2009 - The Economies of Two-Sided Markets.pdf:pdf},
	journal = {The Journal of Economic Perspectives},
	mendeley-groups = {IO},
	number = {3},
	pages = {125--143},
	title = {{The Economies of Two-Sided Markets}},
	volume = {23},
	year = {2009}
}
@article{Issler2009,
	abstract = {In this paper, we propose a novel approach to econometric forecasting of stationary and ergodic time series within a panel-data framework. Our key element is to employ the (feasible) bias-corrected average forecast. Using panel-data sequential asymptotics we show that it is potentially superior to other techniques in several contexts. In particular, it is asymptotically equivalent to the conditional expectation, i.e., has an optimal limiting mean-squared error. We also develop a zero-mean test for the average bias and discuss the forecast-combination puzzle in small and large samples. Monte-Carlo simulations are conducted to evaluate the performance of the feasible bias-corrected average forecast in finite samples. An empirical exercise, based upon data from a well known survey is also presented. Overall, these results show promise for the feasible bias-corrected average forecast. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
	author = {Issler, Jo{\~{a}}o Victor and Lima, Luiz Renato},
	doi = {10.1016/j.jeconom.2009.01.002},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Issler, Lima (2009) A panel data approach to economic forecasting The bias-corrected average forecast.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Bias-corrected average forecast,Common features,Forecast combination,Forecast-combination puzzle,Panel-data},
	mendeley-groups = {Forecasting/Forecasting with Panels,Forecasting/Forecast Combination},
	number = {2},
	pages = {153--164},
	publisher = {Elsevier B.V.},
	title = {{A panel data approach to economic forecasting: The bias-corrected average forecast}},
	volume = {152},
	year = {2009}
}
@article{Sarafidis2009,
	abstract = {This paper explores the impact of error cross-sectional dependence (modelled as a factor structure) on a number of widely used IV and generalized method of moments (GMM) estimators in the context of a linear dynamic panel data model. It is shown that, under such circumstances, the standard moment conditions used by these estimators are invalid - a result that holds for any lag length of the instruments used. Transforming the data in terms of deviations from time-specific averages helps to reduce the asymptotic bias of the estimators, unless the factor loadings have mean zero. The finite sample behaviour of IV and GMM estimators is investigated by means of Monte Carlo experiments. The results suggest that the bias of these estimators can be severe to the extent that the standard fixed effects estimator is not generally inferior anymore in terms of root median square error. Time-specific demeaning alleviates the problem, although the effectiveness of this transformation decreases when the variance of the factor loadings is large. {\textcopyright} The Author(s). Journal compilation {\textcopyright} Royal Economic Society 2008.},
	author = {Sarafidis, Vasilis and Robertson, Donald},
	doi = {10.1111/j.1368-423X.2008.00260.x},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Sarafidis, Robertson (2009) On the impact of error cross-sectional dependence in short dynamic panel estimation.pdf:pdf},
	issn = {13684221},
	journal = {Econometrics Journal},
	keywords = {Asymptotic bias,Cross-sectional dependence,Dynamic panel data,Generalized method of moments,Instrumental variables,Time-specific demeaning},
	mendeley-groups = {Panel Data/Factor Models and Interactive Effects,Panel Data/Dynamic Panels},
	number = {1},
	pages = {62--81},
	title = {{On the impact of error cross-sectional dependence in short dynamic panel estimation}},
	volume = {12},
	year = {2009}
}
@book{Hoff2009,
	abstract = {This book provides a compact self-contained introduction to the theory and application of Bayesian statistical methods. The book is accessible to readers with only a basic familiarity with probability, yet allows more advanced readers to quickly grasp the principles underlying Bayesian theory and methods. R code is provided throughout the text. Much of the example code can be run ``as is'' in R, and essentially all of it can be run after downloading the relevant datasets from the companion website for this book.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Hoff, Peter D.},
	booktitle = {Media},
	doi = {10.1007/978-0-387-92407-6},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoff - 2009 - A First Course in Bayesian Statistical Methods.pdf:pdf},
	isbn = {0387922997},
	issn = {0006341X},
	keywords = {BitTorrent,P2P file sharing,Performance modeling},
	number = {9-12},
	pages = {268},
	pmid = {10911016},
	title = {{A First Course in Bayesian Statistical Methods}},
	volume = {64},
	year = {2009}
}

@article{Roth2023WhatsTrendingDifferenceinDifferences,
  title = {What's {{Trending}} in {{Difference-in-Differences}}? {{A Synthesis}} of the {{Recent Econometrics Literature}}},
  shorttitle = {What's Trending in Difference-in-Differences?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H.C. and Bilinski, Alyssa and Poe, John},
  year = {2023},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {235},
  number = {2},
  pages = {2218--2244},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2023.03.008},
  urldate = {2025-05-10},
  abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of ``canonical'' assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Causal Inference and Policy Design\Regression\Two-Way FE\Roth et al. - 2023 - What’s Trending in Difference-in-Differences A Synthesis of the Recent Econometrics Literature.pdf}
}


@article{Rothman2009,
	abstract = {We propose a new class of generalized thresholding operators that combine thresholding with shrinkage, and Study generalized thresholding of the sample covariance matrix in high dimensions. Generalized thresholding of the covariance matrix has good theoretical properties and carries almost no computational burden. We obtain in explicit convergence rate in the operator norm that shows the tradeoff between the sparsity of the true model, dimension, and the sample size, and shows that generalized thresholding is consistent over a large class of models as long as the dimension p and the sample size it satisfy log p/n -{\textgreater} 0. In addition, we show that generalized thresholding has the "sparsistency" property, meaning it estimates true zeros a, zeros with probability tending to 1, and, under an additional mild condition, is sign consistent for nonzero elements. We show that generalized thresholding covers, as special cases, hard and soft thresholding, smoothly clipped absolute deviation, and adaptive lasso, and compare different types of generalized thresholding in a simulation study and in an example of gene clustering from a microarray experiment with tumor tissues.},
	author = {Rothman, Adam J. and Levina, Elizaveta and Zhu, Ji},
	doi = {10.1198/jasa.2009.0101},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothman, Levina, Zhu - 2009 - Generalized thresholding of large covariance matrices.pdf:pdf},
	isbn = {0162-1459$\backslash$r1537-274X},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Covariance,High-dimensional data,Regularization,Sparsity,Thresholding},
	mendeley-groups = {UPF/ADTSI},
	number = {485},
	pages = {177--186},
	title = {{Generalized thresholding of large covariance matrices}},
	volume = {104},
	year = {2009}
}
@article{Chaves2010,
	author = {Chaves, Andrea and Bahill, A Terry},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaves, Bahill - 2010 - Locating Sites for Photovolataic Solar Panels.pdf:pdf},
	journal = {ArcUser},
	keywords = {DEM derived from lidar,digital elevation model,siting solar panels},
	mendeley-groups = {My Teaching/Energy,My Research/Eta},
	pages = {24--27},
	title = {{Locating Sites for Photovolataic Solar Panels}},
	year = {2010}
}
@incollection{Wooldridge2010,
	author = {Wooldridge, Jeffrey M.},
	booktitle = {Identification and Inference for Econometric Models},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wooldridge - 2010 - Unobserved Heterogeneity and Estimation of Average Partial Effects.pdf:pdf},
	isbn = {9780511614491},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects},
	pages = {27--55},
	title = {{Unobserved Heterogeneity and Estimation of Average Partial Effects}},
	year = {2010}
}
@article{Gomes2010,
	abstract = {ABSTRACT This paper revisits the theoretical relation between financial leverage and stock returns in a dynamic world where both corporate investment and financing decisions are endogenous. We find that the link between leverage and stock returns is more complex than static textbook examples suggest, and depends on the investment opportunities available to the firm. In the presence of financial market imperfections, leverage and investment are generally correlated so that highly levered firms are also mature firms with relatively more (safe) book assets and fewer (risky) growth opportunities. A quantitative version of our model matches several stylized facts about leverage and returns.},
	annote = {doi: 10.1111/j.1540-6261.2009.01541.x},
	author = {Gomes, Joao F and Schmid, Lukas},
	doi = {10.1111/j.1540-6261.2009.01541.x},
	issn = {0022-1082},
	journal = {The Journal of Finance},
	month = {mar},
	number = {2},
	pages = {467--494},
	publisher = {John Wiley {\&} Sons, Ltd},
	title = {{Levered Returns}},
	volume = {65},
	year = {2010}
}
@article{Ludwig2010,
	abstract = {We model the reaction of a PAYG pension system to demographic shocks. We compare the ex ante first best and second best solution of a Ramsey planner with full commitment to the outcome under simple third best rules. The model is calibrated to the German economy. We find that the German system comes relatively close to the second- best solution, and that the recent baby-boom/baby-bust cycle leads to welfare losses of about 5 percent of lifetime consumption for some cohorts. We argue that it is crucial for all our results to correctly model the labor market distortions arising from the pension system.},
	author = {Ludwig, Alexander and Reiter, Michael},
	doi = {10.1257/pol.2.4.83},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ludwig, Reiter - 2010 - Sharing demographic risk-who is afraid of the baby bust.pdf:pdf},
	issn = {1945774X},
	journal = {American Economic Journal: Economic Policy},
	mendeley-groups = {My Research/Pensions},
	number = {4},
	pages = {83--118},
	title = {{Sharing demographic risk-who is afraid of the baby bust?}},
	volume = {2},
	year = {2010}
}
@article{Mikusheva2010,
	abstract = {This paper considers instrumental variable regression with a single endogenous variable and the potential presence of weak instruments. I construct confidence sets for the coefficient on the single endogenous regressor by inverting tests robust to weak instruments. I suggest a numerically simple algorithm for finding the Conditional Likelihood Ratio (CLR) confidence sets. Full descriptions of possible forms of the CLR, Anderson-Rubin (AR) and Lagrange Multiplier (LM) confidence sets are given. I show that the CLR confidence sets have nearly the shortest expected arc length among similar symmetric invariant confidence sets in a circular model. I also prove that the CLR confidence set is asymptotically valid in a model with non-normal errors. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
	author = {Mikusheva, Anna},
	doi = {10.1016/j.jeconom.2009.12.003},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikusheva - 2010 - Robust confidence sets in the presence of weak instruments.pdf:pdf},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Confidence set,Uniform asymptotics,Weak instruments},
	mendeley-groups = {My Research/LR-Based Confidence Sets,Confidence Sets,Confidence Sets/Weak IV},
	number = {2},
	pages = {236--247},
	publisher = {Elsevier B.V.},
	title = {{Robust Confidence Sets in the Presence of Weak Instruments}},
	volume = {157},
	year = {2010}
}

@article{Evdokimov2010IdentificationEstimationNonparametric,
  title = {Identification and {{Estimation}} of a {{Nonparametric Panel Data Model}} with {{Unobserved Heterogeneity}}},
  author = {Evdokimov, Kirill},
  year = {2010},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Nonparametric Analysis\Evdokimov - 2010 - Identification and Estimation of a Nonparametric Panel Data Model with Unobserved Heterogeneity.pdf}
}


@article{Choi2010,
	author = {Choi, Jay Pil},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi - 2010 - TYING IN TWO-SIDED MARKETS WITH MULTI-HOMING.pdf:pdf},
	journal = {The Journal of Industrial Economics},
	mendeley-groups = {IO},
	number = {3},
	pages = {607--626},
	title = {{TYING IN TWO-SIDED MARKETS WITH MULTI-HOMING}},
	volume = {58},
	year = {2010}
}
@article{LeSage2010,
	abstract = {There is near universal agreement that estimates and inferences from spatial regression models are sensitive to particular specifications used for the spatial weight structure in these models. We find little theoretical basis for this commonly held belief, if estimates and inferences are based on the true partial derivatives for a well-specified spatial regression model. We conclude that this myth may have arisen from past applied work that incorrectly interpreted the model coefficients $\backslash$emph{\{}as if{\}} they were partial derivatives, or from use of mis-specified models.},
	author = {LeSage, James P. and Pace, R. Kelley},
	doi = {10.2139/ssrn.1725503},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeSage, Pace - 2010 - The Biggest Myth in Spatial Econometrics.pdf:pdf},
	isbn = {0-7803-5935-6},
	issn = {2225-1146},
	journal = {Econometrics},
	keywords = {C11,C21,C23,direct and indirect effects estimates,sensitivity to spatial weights},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {1},
	pages = {217--249},
	title = {{The Biggest Myth in Spatial Econometrics}},
	year = {2010}
}
@article{Carpenter2010,
	author = {Carpenter, Seth B and Demiralp, Selva},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carpenter, Demiralp - 2010 - Money, Reserves, and the Transmission of Monetary Policy Does the Money Multiplier Exist.pdf:pdf},
	journal = {Federal Reserve Board WP},
	keywords = {Money mulitplier,reserves},
	title = {{Money, Reserves, and the Transmission of Monetary Policy: Does the Money Multiplier Exist?}},
	volume = {41},
	year = {2010}
}
@article{Wang2010,
	abstract = {Survival Analysis method has been commonly used in biology, medical science, and human life insurance studies but it is rarely applied in agricultural insurance research. The main objective of this study is to explore the appropriateness of the Survival Analysis model for the crop insurance program design. Our analysis was mainly focused on the catastrophic risk premium rate estimates under the condition of 70{\%} yield coverage for rice, corn and sorghum in Panjin of Liaoning province, China. The results indicate that the estimated premium rates for each crop are consistent with the currently prevailed crop insurance premium rate in Panjin. ?? 2010 Published by Elsevier B.V.},
	author = {Wang, Erda and Yu, Yang and Little, Bertis B. and Li, Zuozhi},
	doi = {10.1016/j.aaspro.2010.09.009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2010 - Crop insurance premium design based on survival analysis model.pdf:pdf},
	issn = {22107843},
	journal = {Agriculture and Agricultural Science Procedia},
	keywords = {Crop insurance,Government supported crop insurance,Premium rate,Risk management,Survival analysis},
	mendeley-groups = {My Research/Theta/Review},
	pages = {67--75},
	title = {{Crop insurance premium design based on survival analysis model}},
	volume = {1},
	year = {2010}
}
@article{Eisenmann2010,
	author = {Eisenmann, Thomas and Eisenmann, Thomas and Parker, Geoffrey and Alstyne, Marshall Van and Eisenmann, Thomas},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eisenmann et al. - 2010 - Platform Envelopment.pdf:pdf},
	journal = {Harvard Business School WP},
	keywords = {bundling,foreclosure,market entry,network effects,platforms},
	mendeley-groups = {IO},
	title = {{Platform Envelopment}},
	year = {2010}
}
@unpublished{Eberhardt2010,
	author = {Eberhardt, Markus and Helmers, Christian},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Eberhardt, Helmers (2010) Untested Assumptions and Data Slicing.pdf:pdf},
	institution = {University of Oxford},
	mendeley-groups = {Applied/Production Function Estimation},
	series = {Discussion Paper Series},
	title = {{Untested Assumptions and Data Slicing: A Critical Review of Firm-Level Production Function Estimators,}},
	year = {2010}
}
@article{Beukeboom2010,
	author = {Beukeboom, Hans J J and {van Der Laan}, C and van Kreveld, a and Akwah, George},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beukeboom et al. - 2010 - Can Community Forestry contribute to livelihood improvement and biodiversity.pdf:pdf},
	number = {July},
	pages = {81},
	title = {{Can Community Forestry contribute to livelihood improvement and biodiversity ?}},
	year = {2010}
}
@article{Veni2010,
	author = {Veni, K Krishna and Balachandar, S Raja},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veni, Balachandar - 2010 - A new heuristic approach for large size zero–one multi knapsack problem using intercept matrix.pdf:pdf},
	issn = {20103905},
	journal = {International Journal of Computational and Mathematical Sciences},
	keywords = {0-1 multi constrained knapsack,computational complexity,heuristic,np-hard problems,problem},
	mendeley-groups = {My Teaching/Energy},
	number = {5},
	pages = {259--263},
	title = {{A new heuristic approach for large size zero–one multi knapsack problem using intercept matrix}},
	volume = {4},
	year = {2010}
}

@book{Baltagi2021EconometricAnalysisPanel,
  title = {Econometric {{Analysis}} of {{Panel Data}}},
  author = {Baltagi, Badi H.},
  year = {2021},
  series = {Springer {{Texts}} in {{Business}} and {{Economics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-53953-5},
  urldate = {2025-05-12},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-030-53952-8 978-3-030-53953-5},
  langid = {english}
}


@article{Baltagi2010,
	author = {Baltagi, Badi H. and Cheol, Byoung and Heun, Seuck},
	doi = {10.1016/j.jeconom.2009.04.009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baltagi, Cheol, Heun - 2010 - Testing for heteroskedasticity and serial correlation in a random effects panel data model.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data,Panel Data/Heteroskedasticity},
	number = {2},
	pages = {122--124},
	publisher = {Elsevier B.V.},
	title = {{Testing for heteroskedasticity and serial correlation in a random effects panel data model}},
	volume = {154},
	year = {2010}
}
@article{Friedman2010,
	author = {Friedman, Benjamin M and Kuttner, Kenneth N},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Kuttner - 2010 - Implementaion of Monetary Policy How Do Central Banks Set Interest Rates.pdf:pdf},
	journal = {NBER Working Paper},
	number = {16165},
	title = {{Implementaion of Monetary Policy: How Do Central Banks Set Interest Rates}},
	year = {2010}
}
@article{Cottier2010,
	author = {Cottier, Thomas and Mualumfashi, Garba},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cottier, Mualumfashi - 2010 - Energy in WTO law and policy.pdf:pdf},
	number = {6},
	pages = {1--25},
	title = {{Energy in WTO law and policy}},
	volume = {42},
	year = {2010}
}
@article{Arellano2011,
	abstract = {Nonlinear panel data models arise naturally in economic applications, yet their analysis is challenging. Here we provide a progress report on some recent advances in the area. We start by reviewing the properties of random-effects likelihood approaches. We emphasize a link with Bayesian computation and Markov chain Monte Carlo, which provides a convenient approach to estimation and inference. The relaxation of parametric assumptions on the distribution of individual effects raises serious identification problems. In discrete choice models, common parameters and average marginal effects are generally set identified. The availability of continuous outcomes, however, provides opportunities for point identification. We end by reviewing recent progress on non-fixed-T approaches. In panel applications in which the time dimension is not negligible relative to the size of the cross section, it makes sense to view the estimation problem as a time-series finite-sample bias. Several perspectives to bias reduction are now available. We review their properties, with a special emphasis on random-effects methods.},
	author = {Arellano, Manuel and Bonhomme, St{\'{e}}phane},
	doi = {10.1146/annurev-economics-111809-125139},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Arellano, Bonhomme (2011) Nonlinear  Panel  Data  Analysis.pdf:pdf},
	issn = {1941-1383},
	journal = {Annual Review of Economics},
	keywords = {incidental parameters,panel data},
	mendeley-groups = {Panel Data/Nonlinear Panels},
	number = {1},
	pages = {395--424},
	title = {{Nonlinear Panel Data Analysis}},
	volume = {3},
	year = {2011}
}
@article{Pappas2011,
	abstract = {A common optimization problem is the minimization of a symmetric positive definite quadratic form 〈x, T x〉 under linear constraints. The solution to this problem may be given using the Moore-Penrose inverse matrix. In this work at first we extend this result to infinite dimensional complex Hilbert spaces, where a generalization is given for positive operators not necessarily invertible, considering as constraint a singular operator. A new approach is proposed when T is positive semidefinite, where the minimization is considered for all vectors belonging to N (T)⊥. {\textcopyright} 2011, Duke University Press. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {1003.5676},
	author = {Pappas, Dimitrios},
	doi = {10.15352/afa/1399900257},
	eprint = {1003.5676},
	file = {:D$\backslash$:/Academic Things/Math/Articles/Functional Analysis and Operator Theory/Pappas (2010) Minimization of Constrained Quadratic forms in Hilbert Spaces.pdf:pdf},
	issn = {20088752},
	journal = {Annals of Functional Analysis},
	keywords = {Constrained optimization,Moore-penrose inverse,Positive operator,Quadratic form},
	mendeley-groups = {Random Useful Math/Operator Theory and Functional Analysis},
	number = {1},
	pages = {1--12},
	title = {{Minimization of Constrained quadratic Forms in Hilbert Spaces}},
	volume = {2},
	year = {2011}
}
@article{Sezgin2011,
	author = {Sezgin, Sennur and Yildiz, Seyfi and Ayyildiz, Yasar},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sezgin, Yildiz, Ayyildiz - 2011 - The Ethics of Tax Evasion A Comparative Study of Turkey and Kyrgyzystan.pdf:pdf},
	journal = {Journal of Applied Business and Economics},
	number = {3},
	title = {{The Ethics of Tax Evasion : A Comparative Study of Turkey and Kyrgyzystan}},
	volume = {12},
	year = {2011}
}
@article{Song2011,
	abstract = {We construct a growth model consistent with China's economic transition: high output growth, sustained returns on capital, reallo- cation within the manufacturing sector, and a large trade surplus. Entrepreneurial firms use more productive technologies, but due to financial imperfections they must finance investments through internal savings. State-owned firms have low productivity but survive because of better access to credit markets. High-productivity firms outgrow low-productivity firms if entrepreneurs have sufficiently high savings. The downsizing of financially integrated firms forces domestic sav- ings to be invested abroad, generating a foreign surplus. A calibrated version of the theory accounts quantitatively for China's economic transition.},
	author = {Song, Zheng and Storesletten, Kjetil and Zilibotti, Fabrizio},
	doi = {10.1257/aer.101.1.196},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Storesletten, Zilibotti - 2011 - Growing like China.pdf:pdf},
	isbn = {0002-8282},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro III},
	number = {1},
	pages = {196--233},
	title = {{Growing like China}},
	volume = {101},
	year = {2011}
}
@article{Banos2011,
	abstract = {Energy is a vital input for social and economic development. As a result of the generalization of agricultural, industrial and domestic activities the demand for energy has increased remarkably, especially in emergent countries. This has meant rapid grower in the level of greenhouse gas emissions and the increase in fuel prices, which are the main driving forces behind efforts to utilize renewable energy sources more effectively, i.e. energy which comes from natural resources and is also naturally replenished. Despite the obvious advantages of renewable energy, it presents important drawbacks, such as the discontinuity of generation, as most renewable energy resources depend on the climate, which is why their use requires complex design, planning and control optimization methods. Fortunately, the continuous advances in computer hardware and software are allowing researchers to deal with these optimization problems using computational resources, as can be seen in the large number of optimization methods that have been applied to the renewable and sustainable energy field. This paper presents a review of the current state of the art in computational optimization methods applied to renewable and sustainable energy, offering a clear vision of the latest research advances in this field. ?? 2010 Elsevier Ltd. All rights reserved.},
	author = {Ba{\~{n}}os, R. and Manzano-Agugliaro, F. and Montoya, F. G. and Gil, C. and Alcayde, A. and G??mez, J.},
	doi = {10.1016/j.rser.2010.12.008},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ba{\~{n}}os et al. - 2011 - Optimization methods applied to renewable and sustainable energy A review.pdf:pdf},
	isbn = {1364-0321},
	issn = {13640321},
	journal = {Renewable and Sustainable Energy Reviews},
	keywords = {Control,Design,Multi-criteria decision analysis,Optimization,Planning,Renewable energy systems},
	mendeley-groups = {My Research/Eta},
	number = {4},
	pages = {1753--1766},
	pmid = {109176610},
	title = {{Optimization methods applied to renewable and sustainable energy: A review}},
	volume = {15},
	year = {2011}
}
@article{Deb2011,
	abstract = {This paper examines the impact of job loss due to business closings on body mass index (BMI) and alcohol consumption. We suggest that the ambiguous findings in the extant literature may be due in part to unobserved heterogeneity in response and in part due to an overly broad measure of job loss that is partially endogenous (e.g., layoffs). We improve upon this literature using: exogenously determined business closings, a sophisticated estimation approach (finite mixture models) to deal with complex heterogeneity, and national, longitudinal data from the Health and Retirement Study. For both alcohol consumption and BMI, we find evidence that individuals who are more likely to respond to job loss by increasing unhealthy behaviors are already in the problematic range for these behaviors before losing their jobs. These results suggest the health effects of job loss could be concentrated among " at risk" individuals and could lead to negative outcomes for the individuals, their families, and society at large. ?? 2011 Elsevier B.V.},
	author = {Deb, Partha and Gallo, William T. and Ayyagari, Padmaja and Fletcher, Jason M. and Sindelar, Jody L.},
	doi = {10.1016/j.jhealeco.2010.12.009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deb et al. - 2011 - The effect of job loss on overweight and drinking.pdf:pdf},
	isbn = {0167-6296},
	issn = {01676296},
	journal = {Journal of Health Economics},
	keywords = {BMI,Business closings,Drinking,Finite mixture models,Job loss},
	mendeley-groups = {My Research/Zeta},
	number = {2},
	pages = {317--327},
	pmid = {21288586},
	publisher = {Elsevier B.V.},
	title = {{The effect of job loss on overweight and drinking}},
	volume = {30},
	year = {2011}
}
@article{Lafrance2011,
	author = {Lafrance, Jeffrey and Pope, Rulon and Tack, Jesse},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lafrance, Pope, Tack - 2011 - Risk Response in Agriculture.pdf:pdf},
	journal = {NBER Working Paper},
	mendeley-groups = {My Research/Theta/Review},
	number = {16716},
	pages = {1--49},
	title = {{Risk Response in Agriculture}},
	year = {2011}
}
@article{Megahed2011,
	abstract = {One aspect of the call for democracy in the recent Arab region uprisings is the issue of women's rights and gender equality. Three cultural and ideological forces have continued to shape the gender discourse in Arab Muslim-majority societies. They are: “Islamic” teaching and local traditions concerning women's roles in a given society; Western, European colonial perception of women's rights; and finally national gender-related policy reforms. This paper examines the past and present status of women and gender-educational inequality in the Arab world with particular reference to Egypt and Tunisia, prior to and post colonialism. Special attention is given to colonial legacy and its influence on gender and education; to current gender practices in the social sphere with a focus on women's modesty (hijab); to international policies and national responses with regard to women's rights and finally to female participation in pre-university and higher education. These issues incorporate a discussion of cultural and religious constraints. The paper demonstrates similarities and differences between Egypt's and Tunisia's reform policies towards gender parity. It highlights the confrontation of conservative versus liberal ideologies that occurred in each country with the implementation of its gender-related reform policy.},
	author = {Megahed, Nagwa and Lack, Stephen},
	doi = {10.1007/s11159-011-9215-y},
	issn = {1573-0638},
	journal = {International Review of Education},
	mendeley-groups = {My Research,My Research/Prima},
	number = {3},
	pages = {397--418},
	title = {{Colonial legacy, women's rights and gender-educational inequality in the Arab World with particular reference to Egypt and Tunisia}},
	volume = {57},
	year = {2011}
}
@article{Hansen2011,
	author = {Hansen, Peter R. and Lunde, Asger and Nason, James M.},
	doi = {10.3982/ECTA5771},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hansen, Lunde, Nason - 2011 - The Model Confidence Set.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {2},
	pages = {453--497},
	title = {{The Model Confidence Set}},
	volume = {79},
	year = {2011}
}
@article{Abadie2011,
	author = {Abadie, Alberto and Imbens, Guido W},
	doi = {10.1198/jbes.2009.07333},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2011 - Bias-Corrected Matching Estimators for Average Treatment Effects Bias-Corrected Matching Estimators for Average.pdf:pdf},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Selection on observables, Treatment effects,Treatment effects,selection on observables,treatment effects},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects},
	number = {1},
	title = {{Bias-Corrected Matching Estimators for Average Treatment Effects Bias-Corrected Matching Estimators for Average Treatment Effects}},
	volume = {29},
	year = {2011}
}
@article{Bonatti2011,
	abstract = {This paper examines moral hazard in teams over time. Agents are collectively engaged in a project whose duration and outcome are uncertain, and their individual efforts are unobserved. Free-riding leads not only to a reduction in effort, but also to procrastination. Collaboration among agents dwindles over time, but does not cease as long as the project has not succeeded. In addition, the delay until the project succeeds, if it ever does, increases with the number of agents. We show why deadlines, but not necessarily better monitor- ing, help to mitigate moral hazard. (JEL D81, D82, D83)},
	author = {Bonatti, Alessandro and H{\"{o}}rner, Johannes},
	doi = {10.1257/aer.101.2.632},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonatti, H{\"{o}}rner - 2011 - Collaborating.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	number = {2},
	pages = {632--663},
	pmid = {21918203},
	title = {{Collaborating}},
	volume = {101},
	year = {2011}
}
@article{Audibert2011,
	author = {Audibert, Jean-Yves and Catoni, Olivier},
	doi = {10.1214/11-AOS918},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Audibert, Catoni - 2011 - Robust linear least squares regression.pdf:pdf},
	journal = {Annals of Statistics},
	keywords = {and phrases,estimators,generalization error,gibbs posterior distributions,linear regression,pac-bayesian theo-,randomized,rems,resistant estimators,risk bounds,robust statistics,shrinkage,statistical learning theory},
	mendeley-groups = {UPF/Research Seminar},
	number = {5},
	pages = {2766--2794},
	title = {{Robust linear least squares regression}},
	volume = {39},
	year = {2011}
}
@article{Brynjolfsson2011,
	abstract = {Many markets have historically been dominated by a small number of best-selling products. The Pareto principle, also known as the 80/20 rule, describes this common pattern of sales concentration. However, information technology in general and Internet markets in particular have the potential to substantially increase the collective share of niche products, thereby creating a longer tail in the distribution of sales. This paper investigates the Internet's “long tail” phenomenon. By analyzing data collected from a multichannel retailer, it provides empirical evidence that the Internet channel exhibits a significantly less concentrated sales distribution when compared with traditional channels. Previous explanations for this result have focused on differences in product availability between channels. However, we demonstrate that the result survives even when the Internet and traditional channels share exactly the same product availability and prices. Instead, we find that consumers' usage of Internet search and discovery tools, such as recommendation engines, are associated with an increase the share of niche products. We conclude that the Internet's long tail is not solely due to the increase in product selection but may also partly reflect lower search costs on the Internet. If the relationships we uncover persist, the underlying trends in technology portend an ongoing shift in the distribution of product sales.},
	author = {Brynjolfsson, Erik and Hu, Yu (Jeffrey) and Simester, Duncan},
	doi = {10.1287/mnsc.1110.1371},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brynjolfsson, Hu, Simester - 2011 - Goodbye Pareto Principle, Hello Long Tail The Effect of Search Costs on the Concentration of Product.pdf:pdf},
	isbn = {0025-1909},
	issn = {0025-1909},
	journal = {Management Science},
	keywords = {2007,2011,accepted february 23,by ramayya krishnan,concentration,electronic commerce,history,information,internet,long tail,product sales,product variety,received november 27,search cost},
	number = {8},
	pages = {1373--1386},
	pmid = {16483117},
	title = {{Goodbye Pareto Principle, Hello Long Tail: The Effect of Search Costs on the Concentration of Product Sales}},
	volume = {57},
	year = {2011}
}
@article{Martin2011,
	abstract = {This paper explores a view of the crisis as a shock to investor sentiment that led to the collapse of a bubble or pyramid scheme in financial markets. The paper embeds this view in a standard model of the financial accelerator and explores its empirical and policy implications. In particular, it shows how the model can account for: (i) a gradual and protracted expansionary phase followed by a sudden and sharp recession; (ii) the connection (or lack of connection!) between financial and real economic activity; and (iii) a fast and strong transmission of shocks across countries. The paper also uses the model to explore the role of fiscal policy. {\textcopyright} 2011 International Monetary Fund.},
	author = {Martin, Alberto and Ventura, Jaume},
	doi = {10.1057/imfer.2011.1},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin, Ventura - 2011 - Theoretical notes on bubbles and the current crisis.pdf:pdf},
	issn = {20414161},
	journal = {IMF Economic Review},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {1},
	pages = {6--40},
	publisher = {Nature Publishing Group},
	title = {{Theoretical notes on bubbles and the current crisis}},
	url = {http://dx.doi.org/10.1057/imfer.2011.1},
	volume = {59},
	year = {2011}
}
@article{Takeuchi2011,
	abstract = {This paper reports the elicited time preference of human subjects in a laboratory setting. The model allows for non-linear utility functions, non-separability between delay and reward, and time inconsistency including future bias in addition to present bias. In particular, the experiment (1) runs a non-parametric test of time consistency and (2) estimates the form of time discount function independently of instantaneous utility functions, and then (3) the result suggests that many subjects exhibiting future bias, indicating an inverse S-curve time discount function. {\textcopyright} 2010 Elsevier Inc.},
	author = {Takeuchi, Kan},
	doi = {10.1007/978-4-431-55402-8_4},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takeuchi - 2011 - Non-parametric test of time consistency Present bias and future bias.pdf:pdf},
	isbn = {9784431554028},
	issn = {08998256},
	journal = {Games and Economic Behavior},
	keywords = {Experiment,Future bias,Time preference},
	number = {2},
	pages = {77--116},
	publisher = {Elsevier Inc.},
	title = {{Non-parametric test of time consistency: Present bias and future bias}},
	url = {http://dx.doi.org/10.1016/j.geb.2010.05.005},
	volume = {71},
	year = {2011}
}
@article{Lee2012,
	abstract = {Abstract This paper investigates the quasi-maximum likelihood estimation of spatial dynamic panel data models where spatial weights matrices can be time varying. We find that QML estimate is consistent and asymptotically normal. We investigate marginal impacts of explanatory variables in this system via space–time multipliers. Monte Carlo results are reported to investigate the finite sample properties of QML estimates and marginal effects. When spatial weights matrices are substantially varying over time, a model misspecification of a time invariant spatial weights matrix may cause substantial bias in estimation. Slowly time varying spatial weights matrices would be of less concern. R{\'{E}}SUM{\'{E}} la pr{\'{e}}sente communication se penche sur l'estimation du quasi maximum de vrai semblance de mod{\`{e}}les de donn{\'{e}}es du groupe des dynamiques spatiales, o{\`{u}} les matrices de poids spatiales peuvent varier en fonction du temps. Nous relevons que l'estimation de QML est homog{\`{e}}ne et normale sur un plan asymptotique. Nous nous penc...},
	author = {Lee, Lung-fei and Yu, Jihai},
	doi = {10.1080/17421772.2011.647057},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yu - 2012 - QML Estimation of Spatial Dynamic Panel Data Models with Time Varying Spatial Weights Matrices.pdf:pdf},
	issn = {1742-1772},
	journal = {Spatial Economic Analysis},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {1},
	pages = {31--74},
	title = {{QML Estimation of Spatial Dynamic Panel Data Models with Time Varying Spatial Weights Matrices}},
	volume = {7},
	year = {2012}
}
@article{Ledoit2012,
	abstract = {This paper introduces a nonlinear shrinkage estimator of the covariance matrix that does not require recovering the population eigenvalues first. We estimate the sample spectral density and its Hilbert transform directly by smoothing the sample eigenvalues with a variable-bandwidth kernel. Relative to numerically inverting the so-called QuEST function, the main advantages of direct kernel estimation are: (1) it is much easier to comprehend because it is analogous to kernel density estimation; (2) it is only twenty lines of code in Matlab — as opposed to thousands — which makes it more verifiable and customizable; (3) it is 200 times faster without significant loss of accuracy; and (4) it can handle matrices of a dimension larger by a factor of ten. Even for dimension 10, 000, the code runs in less than two minutes on a desktop computer; this makes the power of nonlinear shrinkage as accessible to applied statisticians as the one of linear shrinkage.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1207.5322v1},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.1214/12-AOS989},
	eprint = {arXiv:1207.5322v1},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2012 - Nonlinear shrinkage estimation of large-dimensional covariance matrices.pdf:pdf},
	isbn = {762122220},
	issn = {00905364},
	journal = {Annals of Statistics},
	keywords = {Large-dimensional asymptotics,Nonlinear shrinkage,Rotation equivariance},
	mendeley-groups = {UPF/ADTSI},
	number = {2},
	pages = {1024--1060},
	pmid = {23301833},
	title = {{Nonlinear shrinkage estimation of large-dimensional covariance matrices}},
	volume = {40},
	year = {2012}
}
@unpublished{Mendoza2004,
	author = {Mendoza, Enrique and Terrones, Marco},
	booktitle = {Documentros de Trabajo},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mendoza, Terrones - 2012 - An Anatomy of Credit Booms and Their Demise.pdf:pdf},
	institution = {Banco Central de Chile},
	mendeley-groups = {UPF/Adv. Macro III/Martin},
	number = {670},
	pages = {1--48},
	title = {{An Anatomy of Credit Booms and Their Demise}},
	year = {2012}
}
@article{Hayakawa2012,
	abstract = {This paper considers GMM based estimation and testing procedures for two versions of the AR(1) model with Fixed Effects, henceforth abbreviated as ARFE(1): the conditional ARFE(1) model, and the inclusive ARFE(1) model, which contains the stationary ARFE(1) models and the ARFE(1) model with a unit root. First, the paper presents a two-step Optimal Linear GMM (OLGMM) estimator for the inclusive model, which is asymptotically equivalent to the optimal nonlinear GMM estimator of Ahn and Schmidt (1997). Then the paper examines the properties of the GMM estimators for both versions of the model when the data are persistent. Among other things, we find that the OLGMM estimator is superefficient in the unit root case. Furthermore, under stationarity the covariances of the instruments of the Arellano-Bond estimator and the first differences of the dependent variable are not weak. We also derive new approximations to the finite sample distributions of the Arellano-Bond estimator (for both versions of the model), the Arellano-Bover estimator, and the System estimator. We employ local-to-zero asymptotics (cf Staiger and Stock (1997)) for the Arellano-Bond estimator for the conditional model, because its instruments are weak in this context, and we employ local-to-unity asymptotics, which is developed in this paper, for the estimators for the stationary model. The new approximations agree well with the Monte Carlo evidence in terms of bias and variance. Finally, various GMM based unit root tests against stationary and conditional alternatives are proposed.},
	author = {Hayakawa, Kazuhiko},
	doi = {10.2139/ssrn.253866},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Hayakawa (2012) Dynamic Panel with Interactive Effects.pdf:pdf},
	journal = {Journal of the Japan Statistical Society},
	keywords = {C11,C14,C23,Generalized Method of Moments,dynamic panel data models,fixed effects,local-to-unity asymptotics,local-to-zero asymptotics,redundancy,superefficiency,unit root tests,weak moment conditions},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {2},
	pages = {109--123},
	title = {{GMM Estimation of Short Dynamic Panel Data Models with Interactive Fixed Effects}},
	volume = {42},
	year = {2012}
} 

@article{DiTella2004PoliceReduceCrime,
  title = {Do {{Police Reduce Crime}}? {{Estimates Using}} the {{Allocation}} of {{Police Forces After}} a {{Terrorist Attack}}},
  shorttitle = {Do {{Police Reduce Crime}}?},
  author = {Di Tella, Rafael and Schargrodsky, Ernesto},
  year = {2004},
  month = feb,
  journal = {American Economic Review},
  volume = {94},
  number = {1},
  pages = {115--133},
  issn = {0002-8282},
  doi = {10.1257/000282804322970733},
  urldate = {2025-06-02},
  abstract = {An important challenge in the crime literature is to isolate causal effects of police on crime. Following a terrorist attack on the main Jewish center in Buenos Aires, Argentina, in July 1994, all Jewish institutions received police protection. Thus, this hideous event induced a geographical allocation of police forces that can be presumed exogenous in a crime regression. Using data on the location of car thefts before and after the attack, we find a large deterrent effect of observable police on crime. The effect is local, with no appreciable impact outside the narrow area in which the police are deployed.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Urban\Di Tella and Schargrodsky - 2004 - Do Police Reduce Crime Estimates Using the Allocation of Police Forces After a Terrorist Attack.pdf}
}


@article{Graham2012IdentificationEstimationAverage,
  title = {Identification and {{Estimation}} of {{Average Partial Effects}} in "{{Irregular}}" {{Correlated Random Coefficient Panel Data Models}}},
  author = {Graham, Bryan S. and Powell, James L.},
  year = {2012},
  journal = {Econometrica},
  volume = {80},
  number = {5},
  pages = {2105--2152},
  issn = {0012-9682},
  doi = {10.3982/ECTA8220},
  urldate = {2024-10-03},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Linear Heterogeneous Panels\Graham and Powell - 2012 - Identification and Estimation of Average Partial Effects in Irregular Correlated Random Coefficien.pdf}
}

@article{Lin2012,
	author = {Lin, Chang-Ching and Ng, Serena},
	doi = {10.1515/2156-6674.1000},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Lin, Ng (2011) Estimation of Panel Data Models with Parameter Heterogeneity when Group Membership is Unknown.pdf:pdf},
	journal = {Journal of Econometric Methods},
	keywords = {and the,author notes,clubs,cluster analysis,convergence,council of taiwan,national science,nsc-96-2415-h-001-025,regional housing,research support from the,threshold models},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {42--55},
	title = {{Estimation of Panel Data Models with Parameter Heterogeneity when Group Membership is Unknown}},
	volume = {1},
	year = {2012}
}

@article{Arellano2012IdentifyingDistributionalCharacteristics,
  title = {Identifying {{Distributional Characteristics}} in {{Random Coefficients Panel Data Models}}},
  author = {Arellano, Manuel and Bonhomme, St{\'e}phane},
  year = {2012},
  journal = {Review of Economic Studies},
  volume = {79},
  number = {3},
  pages = {987--1020},
  issn = {00346527},
  doi = {10.1093/restud/rdr045},
  abstract = {We study the identification of panel models with linear individual-specific coefficients when T is fixed. We show identification of the variance of the effects under conditional uncorrelatedness. Identification requires restricted dependence of errors, reflecting a trade-off between heterogeneity and error dynamics. We show identification of the probability distribution of individual effects when errors follow an Autoregressive Moving Average process under conditional independence. We discuss Generalized Method of Moments estimation of moments of effects and errors and construct non-parametric estimators of their densities. As an application, we estimate the effect that a mother smoking during pregnancy has on her child's birth weight. {\copyright} The Author 2012. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
  keywords = {Multiple effects,Nonparametric identification,Panel data,Random coefficients},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\Linear Heterogeneous Panels\Arellano and Bonhomme - 2012 - Identifying Distributional Characteristics in Random Coefficients Panel Data Models.pdf}
}


@article{Xue2012,
	abstract = {The thresholding covariance estimator has nice asymptotic properties for estimating sparse large covariance matrices, but it often has negative eigenvalues when used in real data analysis. To ﬁx this drawback of thresholding estimation, we develop a positive-deﬁnite 1- penalized covariance estimator for estimating sparse large covariance matrices. We derive an efﬁcient alternating direction method to solve the challenging optimization problem and establish its convergence properties. Under weak regularity conditions, nonasymptotic statistical theory is also established for the proposed estimator. The competitive ﬁnite-sample performance of our proposal is demonstrated by both simulation and real applications.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1208.5702v1},
	author = {Xue, Lingzhou and Ma, Shiqian and Zou, Hui},
	doi = {10.1080/01621459.2012.725386},
	eprint = {arXiv:1208.5702v1},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xue, Ma, Zou - 2012 - Positive-Definite l1-Penalized Estimation of Large Covariance Matrices.pdf:pdf},
	isbn = {1047-3211 1460-2199},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	keywords = {Alternating direction methods,Matrix norm,Positive-definite estimation,Soft-thresholding,Sparsity},
	mendeley-groups = {UPF/ADTSI},
	number = {500},
	pages = {1480--1491},
	title = {{Positive-Definite l1-Penalized Estimation of Large Covariance Matrices}},
	volume = {107},
	year = {2012}
}
@article{Hill2012,
	author = {Hill, Sarah E and Rodeheffer, Christopher D and Griskevicius, Vladas and Durante, Kristina and Hill, Sarah E and Rodeheffer, Christopher D and Durante, Kristina and White, Andrew Edward},
	doi = {10.1037/a0028657},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hill et al. - 2012 - Boosting Beauty in an Economic Decline Mating, Spending, and the Lipstick Effect.pdf:pdf},
	journal = {Journal of Personality and Social Psychology},
	keywords = {2009,attractiveness,business wire news,by many econ-,consumer behavior,economic recessions,evolutionary psychology,marked by,mating,omists to be the,sion,the great depres-,the year 2007 began,this period has been,what has been considered,worst economic recession since},
	number = {2},
	pages = {275--91},
	title = {{Boosting Beauty in an Economic Decline: Mating, Spending, and the Lipstick Effect}},
	volume = {103},
	year = {2012}
}
@article{Enjolras2012,
	abstract = {The aim of this paper is to understand which factors affect crop insurance decision in France and in Italy. These neighbor countries are characterized by a changing insurance system from a public fund to private policies which are highly subsidized. Despite the stakes related to crop insurance-CAP reform, size of the market, implication of the governments -, few studies have been drawn on this topic. The literature in finance and in agricultural economics allows to build a two-stage empirical model which computes the elasticities of demand for crop insurance, and to define its key determinants. It appears that France and Italy present similar insurance systems in terms of products and of ability to indemnify. However, the farmers' sensitivity to insurance is most contrasted across the Alps. This leads to a discussion about the creation of an insurance market at the European scale.},
	author = {Enjolras, Geoffroy and Capitanio, Fabian and Adinolfi, Felice},
	doi = {10.2139/ssrn.1836798},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Enjolras, Capitanio, Adinolfi - 2012 - The demand for crop insurance Combined approaches for France and Italy.pdf:pdf},
	issn = {11092580},
	journal = {Agricultural Economics Review},
	keywords = {Crop insurance,Insurance demand},
	mendeley-groups = {My Research/Theta/Review},
	number = {1},
	pages = {5--22},
	title = {{The demand for crop insurance: Combined approaches for France and Italy}},
	volume = {13},
	year = {2012}
}
@misc{Hill2012,
	abstract = {Although consumer spending typically declines in economic recessions, some observers have noted that recessions appear to increase women's spending on beauty products—the so-called lipstick effect. Using both historical spending data and rigorous experiments, the authors examine how and why economic recessions influence women's consumer behavior. Findings revealed that recessionary cues—whether naturally occurring or experimentally primed—decreased desire for most products (e.g., electronics, household items). However, these cues consistently increased women's desire for products that increase attractiveness to mates—the first experimental demonstration of the lipstick effect. Additional studies show that this effect is driven by women's desire to attract mates with resources and depends on the perceived mate attraction function served by these products. In addition to showing how and why economic recessions influence women's desire for beauty products, this research provides novel insights into women's mating psychology, consumer behavior, and the relationship between the two. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
	address = {Hill, Sarah E.: Department of Psychology, Texas Christian University, Fort Worth, TX, US, 76129, s.e.hill@tcu.edu},
	author = {Hill, Sarah E and Rodeheffer, Christopher D and Griskevicius, Vladas and Durante, Kristina and White, Andrew Edward},
	booktitle = {Journal of Personality and Social Psychology},
	doi = {10.1037/a0028657},
	isbn = {1939-1315(Electronic);0022-3514(Print)},
	keywords = {*Consumer Behavior,*Economics,*Human Females,*Physical Attractiveness,Economy,Evolutionary Psychology,Human Mate Selection},
	mendeley-groups = {My Research/Zeta},
	number = {2},
	pages = {275--291},
	publisher = {American Psychological Association},
	title = {{Boosting beauty in an economic decline: Mating, spending, and the lipstick effect.}},
	volume = {103},
	year = {2012}
}
@article{Porter-Bolland2012,
	abstract = {This paper assesses the role of protected and community managed forests for the long term maintenance of forest cover in the tropics. Through a meta-analysis of published case-studies, we compare land use/cover change data for these two broad types of forest management and assess their performance in maintaining forest cover. Case studies included 40 protected areas and 33 community managed forests from the peer reviewed literature. A statistical comparison of annual deforestation rates and a Qualitative Comparative Analysis were conducted. We found that as a whole, community managed forests presented lower and less variable annual deforestation rates than protected forests. We consider that a more resilient and robust forest conservation strategy should encompass a regional vision with different land use types in which social and economic needs of local inhabitants, as well as tenure rights and local capacities, are recognized. Further research for understanding institutional arrangements that derive from local governance in favor of tropical forest conservation is recommended. ?? 2012 Elsevier B.V..},
	author = {Porter-Bolland, Luciana and Ellis, Edward A. and Guariguata, Manuel R. and Ruiz-Mall{\'{e}}n, Isabel and Negrete-Yankelevich, Simoneta and Reyes-Garc{\'{i}}a, Victoria},
	doi = {10.1016/j.foreco.2011.05.034},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter-Bolland et al. - 2012 - Community managed forests and forest protected areas An assessment of their conservation effectiveness ac.pdf:pdf},
	isbn = {0378-1127},
	issn = {03781127},
	journal = {Forest Ecology and Management},
	keywords = {Community managed forests,Land use/cover change,Meta-analysis,Protected areas,Qualitative Comparative Analysis,Tropical deforestation},
	pages = {6--17},
	pmid = {21516884},
	publisher = {Elsevier B.V.},
	title = {{Community managed forests and forest protected areas: An assessment of their conservation effectiveness across the tropics}},
	url = {http://dx.doi.org/10.1016/j.foreco.2011.05.034},
	volume = {268},
	year = {2012}
}
@unpublished{Daykin2012,
	address = {Brussels},
	author = {Daykin, Chris},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Daykin - 2012 - Sustainability of pension systems in Europe – the demographic challenge Challenges to sustainability.pdf:pdf},
	institution = {Groupe Consultatif Actuariel Europ{\'{e}}en Position},
	mendeley-groups = {My Research/Pensions},
	number = {July 2012},
	pages = {1--18},
	title = {{Sustainability of pension systems in Europe – the demographic challenge Challenges to sustainability}},
	year = {2012}
}
@incollection{Korte2012,
	address = {Berlin, Heidelberg},
	author = {Korte, Bernhard and Vygen, Jens},
	doi = {10.1007/978-3-642-24488-9_17},
	editor = {Korte, Bernhard and Vygen, Jens},
	isbn = {978-3-642-24488-9},
	mendeley-groups = {My Teaching/Energy},
	pages = {459--470},
	publisher = {Springer Berlin Heidelberg},
	title = {{The Knapsack Problem BT  - Combinatorial Optimization: Theory and Algorithms}},
	url = {http://dx.doi.org/10.1007/978-3-642-24488-9{\_}17},
	year = {2012}
}
@article{Goodrich2012,
	abstract = {The price of photovoltaic (PV) systems in the United States (i.e., the cost to the system owner) has dropped precipitously in recent years, led by substantial reductions in global PV module prices. However, system cost reductions are not necessarily realized or realized in a timely manner by many customers. Many reasons exist for the apparent disconnects between installation costs, component prices, and system prices; most notable is the impact of fair market value considerations on system prices. To guide policy and research and development strategy decisions, it is necessary to develop a granular perspective on the factors that underlie PV system prices and to eliminate subjective pricing parameters. This report's analysis of the overnight capital costs (cash purchase) paid for PV systems attempts to establish an objective methodology that most closely approximates the book value of PV system assets.},
	author = {Goodrich, Alan and James, Ted and Woodhouse, Michael},
	doi = {10.2172/1036048},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodrich, James, Woodhouse - 2012 - Residential, Commercial, and Utility-Scale Photovoltaic (PV) System Prices in the United States Curr.pdf:pdf},
	isbn = {9781620815434},
	journal = {NREL Technical Report},
	keywords = {February 2012,NREL/TP-6A20-53347,PV,commercial rooftop,costs,ground mount,installation,installed,land costs,photovoltaics,price,residential rooftop,silicon PV,solar,solar farm,solar system,tracking},
	mendeley-groups = {My Teaching/Energy},
	number = {February},
	pages = {64},
	title = {{Residential, Commercial, and Utility-Scale Photovoltaic (PV) System Prices in the United States: Current Drivers and Cost-Reduction Opportunities}},
	year = {2012}
}
@unpublished{Mendicino2012,
	author = {Mendicino, Caterina},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mendicino - 2012 - Collateral Requirements Macroeconomic Fluctuations and Macroprudential Policy.pdf:pdf},
	isbn = {9789896781316},
	mendeley-groups = {Financial Frictions},
	series = {Banco de Portugal Working Papers},
	title = {{Collateral Requirements: Macroeconomic Fluctuations and Macroprudential Policy}},
	year = {2012}
}
@book{Roland2012,
	abstract = {applicability for this approach.},
	author = {Roland, Gerald},
	doi = {10.1057/9780230361836},
	editor = {Roland, Gerald},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roland - 2012 - Economies in Transition The Long Run View.pdf:pdf},
	isbn = {978-1-349-34469-7},
	number = {9},
	publisher = {Palgrave Macmillan UK},
	title = {{Economies in Transition: The Long Run View}},
	year = {2012}
}
@article{Weiss2012,
	abstract = {(www.fao.org/forestry) for official information. The purpose of these papers is to provide early information on ongoing activities and programmes, to facilitate dialogue and to stimulate discussion. The Forest Economics, Policy and Products Division works in the broad areas of strengthening national institutional capacities, including research, education and extension; forest policies and governance; support to national forest programmes; forests, poverty alleviation and food security; participatory forestry and sustainable livelihoods.},
	author = {Weiss, Gerhard and Guduri{\'{c}}, Ivana and Wolfslehner, Bernhard},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weiss, Guduri{\'{c}}, Wolfslehner - 2012 - Review of forest owners' organizations in selected Eastern European countries.pdf:pdf},
	title = {{Review of forest owners' organizations in selected Eastern European countries}},
	url = {www.fao.org/forestry},
	year = {2012}
}
@article{Okhrin2013,
	abstract = {This article explores the possibility of spatial diversification of weather risk for 17 agricultural production regions in China.We investigate the relation between the size of the buffer load and the size of the trading area of a hypothetical temperature-based insurance. The analysis adopts the hierarchical Archimedean copula approach that allows for flexible modeling of the dependence structure of insured losses.We find that the spatial diversification effect depends on the type of the weather index and the strike level of the insurance. Our findings are relevant for the current discussion on the viability of private crop insurance in China.},
	author = {Okhrin, Ostap and Odening, Martin and Xu, Wei},
	doi = {10.1111/j.1539-6975.2012.01476.x},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Okhrin, Odening, Xu - 2013 - Systemic Weather Risk and Crop Insurance The Case of China.pdf:pdf},
	issn = {00224367},
	journal = {Journal of Risk and Insurance},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {351--372},
	title = {{Systemic Weather Risk and Crop Insurance: The Case of China}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6975.2012.01476.x},
	volume = {80},
	year = {2013}
}
@article{Methods2013,
	author = {Methods, Econometric and Shams, Sedigheh and Haghighi, Fatemeh K},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Methods, Shams, Haghighi - 2013 - A Copula-GARCH Model of Conditional Dependencies Methodology.pdf:pdf},
	keywords = {arma-garch model,copula,dependency structure,garch,model,portfolio,value-at-risk},
	mendeley-groups = {My Research/Balts},
	number = {2},
	pages = {39--50},
	title = {{A Copula-GARCH Model of Conditional Dependencies : Methodology}},
	volume = {2},
	year = {2013}
}
@unpublished{Canova2013,
	abstract = {This paper provides an overview of the panel VAR models used in macroeconomics and {\ldots}nance. It discusses what are their distinctive features, what they are used for, and how they can be derived from economic theory. It also describes how they are estimated and how shock identi{\ldots}cation is performed, and compares panel VARs to other approaches used in the literature to deal with dynamic models involving heterogeneous units. Finally, it shows how structural time variation can be dealt with and illustrates the challanges that they present to researchers interested in studying cross-unit dynamics interdependences in heterogeneous setups.},
	author = {Canova, Fabio and Ciccarelli, Matteo},
	booktitle = {ECB Working Paper Series},
	doi = {10.1108/S0731-9053(2013)0000031006},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Canova, Ciccarelli - 2013 - Panel vector autoregressive models A survey.pdf:pdf},
	isbn = {9781781907528},
	issn = {07319053},
	keywords = {Estimation,Identification,Inference,Panel VAR},
	mendeley-groups = {Panel Data/VAR},
	number = {15},
	pages = {205--246},
	title = {{Panel vector autoregressive models: A survey}},
	volume = {32},
	year = {2013}
}
@article{Inderst2013,
	author = {Inderst, Georg and Advisory, Inderst},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inderst, Advisory - 2013 - Private Infrastructure Finance and Investment in Europe.pdf:pdf},
	title = {{Private Infrastructure Finance and Investment in Europe}},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2359648},
	year = {2013}
}
@article{Doraszelski2013,
	abstract = {We develop a model of endogenous productivity change to examine the impact of the investment in knowledge on the productivity of firms. Our dynamic investment model extends the tradition of the knowledge capital model of Griliches (1979) that has remained a cornerstone of the productivity literature. Rather than constructing a stock of knowledge capital from a firm's observed R{\&}D expenditures, we consider productivity to be unobservable to the econometrician. Our approach accounts for uncertainty, non-linearity, and heterogeneity across firms in the link between R{\&}D and productivity. We also derive a novel estimator for production functions in this setting. Using an unbalanced panel of more than 1800 Spanish manufacturing firms in nine industries during the 1990s, we provide evidence of non-linearities as well as economically significant uncertainties in the R{\&}D process. R{\&}D expenditures play a key role in determining the differences in productivity across firms and the evolution of firm-level productivity over time. {\textcopyright} The Author 2013. Published by Oxford University Press on behalf of The Review of Economic Studies Limited.},
	author = {Doraszelski, Ulrich and Jaumandreu, Jordi},
	doi = {10.1093/restud/rdt011},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Doraszelski, Jaumandreu (2013) Estimating Endogenous Productivity.pdf:pdf},
	issn = {00346527},
	journal = {Review of Economic Studies},
	keywords = {Knowledge capital model,Production function estimation,Productivity,RandD},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {4},
	pages = {1338--1383},
	title = {{R and D and productivity: Estimating endogenous productivity}},
	volume = {80},
	year = {2013}
}
@article{Glauber2013,
	annote = {10.1093/ajae/aas091},
	author = {Glauber, Joseph W},
	doi = {10.1093/ajae/aas091},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta},
	month = {jan},
	number = {2 },
	pages = {482--488},
	title = {{The Growth Of The Federal Crop Insurance Program, 1990–2011}},
	url = {http://ajae.oxfordjournals.org/content/95/2/482.short},
	volume = {95 },
	year = {2013}
}
@article{Khan2013,
	abstract = {We study the cyclical implications of credit market imperfections in a quantitative dynamic, stochastic general equilibrium model wherein firms face persistent shocks to aggregate and individual productivity. In our model economy, optimal capital reallocation is distorted by two frictions: collateralized borrowing and partial investment irreversibil- ity. We find that a negative shock to borrowing conditions can, on its own, generate a large and persistent recession through disruptions to the distribution of capital. This recession, and the subsequent recov- ery, is distinguished both quantitatively and qualitatively from that driven by an exogenous shock to total factor productivity.},
	author = {Khan, Aubhik and Thomas, Julia K.},
	doi = {10.1086/674142},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Thomas - 2013 - Credit Shocks and Aggregate Fluctuations in an Economy with Production Heterogeneity.pdf:pdf},
	isbn = {00223808},
	issn = {0022-3808},
	journal = {Journal of Political Economy},
	mendeley-groups = {Financial Frictions},
	number = {6},
	pages = {1055--1107},
	title = {{Credit Shocks and Aggregate Fluctuations in an Economy with Production Heterogeneity}},
	url = {http://www.journals.uchicago.edu/doi/10.1086/674142},
	volume = {121},
	year = {2013}
}
@article{Glauber2013,
	author = {Glauber, Joseph W.},
	doi = {10.1093/ajae/aas091},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glauber - 2013 - The growth of the federal crop insurance program, 1990-2011.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {482--488},
	title = {{The growth of the federal crop insurance program, 1990-2011}},
	volume = {95},
	year = {2013}
}
@article{Bayar2013,
	author = {Bayar, Yilmaz},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayar - 2013 - Financial Sustainability of Pension Systems in the European Union.pdf:pdf},
	journal = {European Research Studies},
	keywords = {ageing,g20,h55,j11,j32,jel classification,old-age dependency ratio,pension systems,retirement policies},
	mendeley-groups = {My Research/Pensions},
	number = {3},
	pages = {46--70},
	title = {{Financial Sustainability of Pension Systems in the European Union}},
	volume = {XVI},
	year = {2013}
}
@book{Kuppuswamy2013,
	abstract = {Entrepreneurs are turning to crowdfunding as a way to finance their creative ideas. Crowdfunding involves relatively small contributions of many consumer-investors over a fixed time period (generally a few weeks). The purpose of this paper is to add to our empirical understanding of backer dynamics over the project funding cycle. Two years of publicly available data on projects listed on Kickstarter is used to establish that the typical pattern of project support is U-shaped—in general, backers are more likely to contribute to a project in the first and last week as compared to the middle period of the funding cycle. We further establish that this U-shape pattern of support is pervasive across projects, including both successfully and unsuccessfully funded projects, those with large and small goals, and projects in different categories. We then empirically explore the dynamics associated with several factors, including collective attention effects from platform sorting options, the role of family and friends in supporting projects, the effects of social influence, and the role of project updates over the project funding cycle.},
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Kuppuswamy, Venkat and Bayus, Barry L},
	booktitle = {SSRN Electronic Journal},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuppuswamy, Bayus - 2013 - The Dynamics of Porject Backers in Kickstarter.pdf:pdf},
	isbn = {2116548608},
	issn = {1098-6596},
	keywords = {entrepreneurship,innovation,strategy,venture financing},
	pages = {42},
	pmid = {25246403},
	title = {{The Dynamics of Porject Backers in Kickstarter}},
	volume = {March 2013},
	year = {2013}
}
@article{Pedroni2013,
	abstract = {The paper proposes a structural approach to VAR analysis in panels, which takes into account responses to both idiosyncratic and common structural shocks, while permitting full cross member heterogeneity of the response dynamics. In the context of this structural approach, estimation of the loading matrices for the decomposition into idiosyncratic versus common shocks is straightforward and transparent. The method appears to do remarkably well at uncovering the properties of the sample distribution of the underlying structural dynamics, even when the panels are relatively short, as illustrated in Monte Carlo simulations. Finally, these simulations also illustrate that the SVAR panel method can be used to improve inference, not only for properties of the sample distribution, but also for dynamics of individual members of the panel that lack adequate data for a conventional time series SVAR analysis. This is accomplished by using fitted cross sectional regressions of the sample of estimated panel responses to correlated static measures, and using these to interpolate the member-specific dynamics.},
	author = {Pedroni, Peter},
	doi = {10.3390/econometrics1020180},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pedroni - 2013 - Structural Panel VARs.pdf:pdf},
	journal = {Econometrics},
	keywords = {panel time series,panel vars,structural var},
	mendeley-groups = {Panel Data/VAR},
	number = {2},
	pages = {180--206},
	title = {{Structural Panel VARs}},
	volume = {1},
	year = {2013}
}
@book{Boucheron2013,
	author = {Boucheron, Stephane and Lugosi, Gabor and Massart, Pasc},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boucheron, Lugosi, Massart - 2013 - Concentration Inequalities A Nonasymptotic Theory of Independence.pdf:pdf},
	mendeley-groups = {UPF/Research Seminar},
	publisher = {Oxford University Press},
	title = {{Concentration Inequalities: A Nonasymptotic Theory of Independence}},
	year = {2013}
}
@article{Goodwin2013,
	author = {Goodwin, Barry K. and Smith, Vincent H.},
	doi = {10.1093/ajae/aas092},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodwin, Smith - 2013 - What harm is done by subsidizing crop insurance.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {489--497},
	title = {{What harm is done by subsidizing crop insurance?}},
	volume = {95},
	year = {2013}
}
@article{Deng2013,
	abstract = {For statistical inferences that involve covariance matrices, it is desirable to obtain an accurate covariance matrix estimate with a well-structured eigen-system. We propose to estimate the covariance matrix through its matrix logarithm based on an approximate log-likelihood function. We develop a generalization of the Leonard and Hsu log-likelihood approximation that no longer requires a nonsingular sample covariance matrix. The matrix log-transformation provides the ability to impose a convex penalty on the transformed likelihood such that the largest and smallest eigenvalues of the covariance matrix estimate can be regularized simultaneously. The proposed method transforms the problem of estimating the covariance matrix into the problem of estimating a symmetric matrix, which can be solved efficiently by an iterative quadratic programming algorithm. The merits of the proposed method are illustrated by a simulation study and two real applications in classification and portfolio optimization. Supplementary materials for this article are available online. {\textcopyright} 2013 American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of North America.},
	author = {Deng, Xinwei and Tsui, Kam Wah},
	doi = {10.1080/10618600.2012.715556},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Tsui - 2013 - Penalized covariance matrix estimation using a matrix-logarithm transformation.pdf:pdf},
	issn = {10618600},
	journal = {Journal of Computational and Graphical Statistics},
	keywords = {Eigenvalues,Penalized likelihood function,Well-conditioned},
	mendeley-groups = {UPF/ADTSI},
	number = {2},
	pages = {494--512},
	title = {{Penalized covariance matrix estimation using a matrix-logarithm transformation}},
	volume = {22},
	year = {2013}
}
 
@article{Dainelli2013,
	author = {Dainelli, Francesco and Giunta, Francesco},
	pages = {21--47},
	title = {{Determinants of SME credit worthiness under Basel rules : the value of credit history information}},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2257305},
	volume = {66},
	year = {2013}
}
@article{Haushofer2013,
	abstract = {This paper studies the response of poor rural households in rural Kenya to large temporary income changes. Using a randomized controlled trial, households were randomly assigned to receive unconditional cash transfers of at least USD 404 from the NGO GiveDirectly. We designed the experiment to address several long-standing questions in the economics literature: what is the shape of households' Engel curves? Do household members effectively pool income? Are there constraints to savings? Do transfers generate externalities? In addition, we study in detail the effects of transfers on psychological well-being and levels of the stress hormone cortisol. We randomized at both the village and household levels; further, within the treatment group, we randomized recipient gender (wife vs. husband), transfer timing (lump-sum transfer vs. monthly installments over 9 months), and transfer magnitude (USD 404 vs. USD 1,520). We find a strong consumption response to transfers, with an increase in monthly consumption from USD 157 to USD 194 four months after the transfer ended. Implied expenditure elasticities for food, medical and education expenditures range between 0.84 and 1.47, while the point estimates are negative for alcohol and tobacco. Intriguingly, recipient gender does not affect the household response to the program. Households may face savings constraints: monthly transfers are more likely than lump-sum transfers to improve food security, while lump-sum transfers are more likely to be spent on durables. We find no evidence for externalities on non-recipients except for a significant positive spillover on female empowerment. Transfer recipients experience large increases in psychological well-being, and several types of transfers lead to reductions in levels of the stress hormone cortisol. Together, these results suggest that unconditional cash transfers have significant impacts on consumption and psychological well-being.},
	author = {Haushofer, Johannes and Shapiro, Jeremy},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haushofer, Shapiro - 2013 - Household Response to Income Changes Evidence from an Unconditional Cash Transfer Program in Kenya.pdf:pdf},
	journal = {Mimeo},
	mendeley-groups = {My Teaching/Agro},
	title = {{Household Response to Income Changes: Evidence from an Unconditional Cash Transfer Program in Kenya}},
	year = {2013}
}
@article{Fernandez-Val2013,
	author = {Fern{\'{a}}ndez-Val, Iv{\'{a}}n and Lee, Joonhwah},
	doi = {10.3982/QE75},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Fern{\'{a}}ndez-Val, Lee (2013) Panel data models with nonadditive unobserved heterogeneity.pdf:pdf},
	journal = {Quantitative Economics},
	keywords = {C23,Correlated random-coefficient model,GMM,J31,J51,bias,cigarette demand,fixed effects,incidental parameter problem,instrumental variables,panel data},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {453--481},
	title = {{Panel Data Models with Nonadditive Unobserved Heterogeneity: Estimation and Inference}},
	volume = {4},
	year = {2013}
}
@book{FAO2013,
	author = {FAO},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/FAO - 2013 - Mainstreaming Disaster Risk Reduction into Agriculture A case study from Bicol Region, Philippines.pdf:pdf},
	isbn = {9789251077443},
	keywords = {Arnulfo M.,Mascari{\~{n}}as,et al.},
	mendeley-groups = {My Research/Theta},
	pages = {113},
	title = {{Mainstreaming Disaster Risk Reduction into Agriculture A case study from Bicol Region, Philippines}},
	year = {2013}
}
@article{Coble2013,
	annote = {10.1093/ajae/aas093},
	author = {Coble, Keith H and Barnett, Barry J},
	doi = {10.1093/ajae/aas093},
	journal = {American Journal of Agricultural Economics},
	month = {jan},
	number = {2 },
	pages = {498--504},
	title = {{Why Do We Subsidize Crop Insurance?}},
	url = {http://ajae.oxfordjournals.org/content/95/2/498.short},
	volume = {95 },
	year = {2013}
}
@article{Varian2013,
	abstract = {Rejecting the market-failure hypothesis, Dr Foldvary argues that an entrepreneur can provide collective goods by consensual community agreements. Instead of focusing particular services, as previous studies have done, this book concerns itself with entire private communities. A series of case studies demonstrates how real world communities, such as Walt Disney World, the Reston Association in Virginia and the private neighbourhoods of St Louis, are in fact financing their own public goods and services in accordance with this theory. For such communities to rise and prosper, the author contends, government must eliminate restrictions such as zoning as well as the taxation of private services. After considering the implications of his work for urban economies – at a time when many of America's cities are plagued by decay, violence and poverty – Dr Foldvary argues that prosperity can be restored to cities if private communities are allowed to develop. As an original response to an urgent, contemporary problem this well-written book will be welcomed by social scientists, policy makers and business leaders seeking solutions to problems of urban decay.},
	author = {Varian, Hal R.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varian - 2013 - Public Goods and Private Gifts.pdf:pdf},
	isbn = {9781852789510},
	keywords = {Economics and Finance,Politics and Public Policy},
	number = {January 2013},
	pages = {1--9},
	pmid = {304004532},
	title = {{Public Goods and Private Gifts}},
	year = {2013}
}
@article{Melius2013,
	abstract = {A number of methods have been developed using remote sensing data to estimate rooftop area suitable for the installation of photovoltaics (PV) at various geospatial resolutions. This report reviews the literature and patents on methods for estimating rooftop-area appropriate for PV, including constant-value methods, manual selection methods, and GIS-based methods. This report also presents NREL's proposed method for estimating suitable rooftop area for PV using Light Detection and Ranging (LiDAR) data in conjunction with a GIS model to predict areas with appropriate slope, orientation, and sunlight. NREL's method is validated against solar installation data from New Jersey, Colorado, and California to compare modeled results to actual on-the-ground measurements.},
	author = {Melius, J and Margolis, R and Ong, S},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Melius, Margolis, Ong - 2013 - Estimating Rooftop Suitability for PV A Review of Methods , Patents , and Validation Techniques.pdf:pdf},
	journal = {NREL Technical Report},
	keywords = {December 2013,GIS,LiDAR,NREL/TP-6A20-60593,PV,aspect,geographic information systems,hillshade,orientation,photovoltaics,rooftop,slope,solar,sunlight},
	mendeley-groups = {My Teaching/Energy},
	number = {December},
	pages = {35},
	title = {{Estimating Rooftop Suitability for PV: A Review of Methods , Patents , and Validation Techniques}},
	url = {www.nrel.gov/publications},
	year = {2013}
}
@article{Niraula2013,
	abstract = {Abstract During the 1990's community-based forest management gained momentum in Nepal. This study systematically evaluates the impacts that this had on land cover change and other associated aspects during the period 1990–2010 using repeat photography and satellite imagery in combination with interviews with community members. The results of the study clearly reflect the success of community-based forest management in the Dolakha district of the mid-hills of Nepal: during the study period, the rate of conversion of sparse forest into dense forest under community-based management was found to be between 1.13{\%} and 3.39{\%} per year. Similarly, the rate of conversion of non-forest area into forest was found to be between 1.11{\%} and 1.96{\%} per year. Community-based forest management has resulted in more efficient use of forest resources, contributed to a decline in the use of slash-and-burn agricultural practices, reduced the incidence of forest fires, spurred tree plantation, and encouraged the conservation and protection of trees on both public and private land. The resulting reclamation of forest in landside areas and river banks and the overall improvement in forest cover in the area has reduced flash floods and associated landslides.},
	author = {Niraula, Rabin Raj and Gilani, Hammad and Pokharel, Bharat Kumar and Qamer, Faisal Mueen},
	doi = {http://dx.doi.org/10.1016/j.jenvman.2013.04.006},
	issn = {0301-4797},
	journal = {Journal of Environmental Management},
	keywords = {Community forestry,Repeat photography,Satellite imagery and forest management},
	month = {sep},
	pages = {20--29},
	title = {{Measuring impacts of community forestry program through repeat photography and satellite remote sensing in the Dolakha district of Nepal}},
	url = {http://www.sciencedirect.com/science/article/pii/S0301479713002429},
	volume = {126},
	year = {2013}
}
@book{RECOFTC2013,
	abstract = {This analysis of community forestry in the Asia–Pacific region indicates that people will conserve biodiversity, reduce deforestation and manage forests sustainably when they derive regular benefits from them and when they are empowered to participate in decision-making processes regarding those forests. red to participate in decision-making processes regarding those forests. Authors: Thomas Sikor, David Gritten, Julian Atkinson, Bao Huy, Ganga Ram Dahal, Khwanchai Duangsathaporn, Francis Hurahura, Khamla Phanvilay, Ahmad Maryudi, Juan Pulhin, Mark Anthony Ramirez, Saw Win, Sumei Toh, Justine Vaz, Tol Sokchea, Srey Marona, and Zhao Yaqiao. -},
	author = {RECOFTC},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/RECOFTC - 2013 - Community forestry in Asia and the Pacific Pathway to inclusive development.pdf:pdf},
	keywords = {Borneo,Indonesia,Malaysia,Pacific,Southeast Asia,agroforestry,community development,community forestry,deforestation,land rights,land tenure,participation,participatory management,sustainable forest management},
	title = {{Community forestry in Asia and the Pacific: Pathway to inclusive development}},
	url = {http://www.recoftc.org/site/resources/Community-forestry-in-Asia-and-the-Pacific-Pathway-to-inclusive-development.php},
	year = {2013}
}
@book{Carrera2013,
	author = {Carrera, Sergio and Guild, Elspeth and Hernanz, Nicholas and Alcidi, Cinzia and Busse, Matthias and Errera, Roger and Ivanova, Ivanka and Guild, Elspeth and Hernanz, Nicholas and Alcidi, Cinzia and Busse, Matthias and Errera, Roger and Ivanova, Ivanka and Jowell, Jeffrey and Marsch, Nikolaus},
	isbn = {9789461383631},
	pages = {101},
	publisher = {CEPS},
	title = {{The Triangular Relationship between Fundamental Rights , Democracy and the Rule of Law in the EU Towards an EU Copenhagen Mechanism The Triangular Relationship between Fundamental Rights , Democracy and the Rule of Law in the EU Towards an EU Copenhagen M}},
	year = {2013}
}
@article{OECD2013,
	abstract = {Funded pension arrangements are recovering gradually from the financial crisis. While the investment losses suffered in 2008 are still far from being fully recouped, two key variables monitored by policymakers, investment returns and funding ratios in defined benefit plans, have shown a marked improvement in the first half of 2009. Official figures for the third quarter of 2009 are still not available for most OECD countries, but the recent market rally points to a further improvement in pension fund performance. Despite these good news, some of the structural challenges faced by private pension systems are yet to be addressed. In particular, the ongoing shift towards defined contribution arrangements calls for an overhaul of regulatory approaches, with default investment options that deliver risk mitigation as members approach retirement. There is also a need to strengthen disclosure requirements and to implement effective financial education programmes. The OECD is continuing its work in these areas and will be publishing reports and policy recommendations over the course of the next year. This sixth issue of Pensions Markets in Focus also presents data on public pensions reserve funds, which have also been severely affected by the crisis in many OECD countries. The information on pension fund investment returns has also been extended to non-OECD countries that participate in the OECD data collection via a cooperation agreement with the International Organisation of Pension Supervisors.},
	author = {OECD},
	doi = {Issue 9},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OECD - 2013 - Pension Markets In Focus.pdf:pdf},
	journal = {Pension Markets in Focus},
	mendeley-groups = {My Research/Pensions},
	number = {9},
	pages = {1--28},
	title = {{Pension Markets In Focus}},
	year = {2013}
}
@article{Jakubiec2013,
	abstract = {Abstract In this paper we present, demonstrate and validate a method for predicting city-wide electricity gains from photovoltaic panels based on detailed 3D urban massing models combined with Daysim-based hourly irradiation simulations, typical meteorological year climactic data and hourly calculated rooftop temperatures. The resulting data can be combined with online mapping technologies and search engines as well as a financial module that provides building owners interested in installing a photovoltaic system on their rooftop with meaningful data regarding spatial placement, system size, installation costs and financial payback. As a proof of concept, a photovoltaic potential map for the City of Cambridge, Massachusetts, USA, consisting of over 17,000 rooftops has been implemented as of September 2012.  The new method constitutes the first linking of increasingly available GIS and LiDAR urban datasets with the validated building performance simulation engine Daysim, thus-far used primarily at the scale of individual buildings or small urban neighborhoods. A comparison of the new method with its predecessors reveals significant benefits as it produces hourly point irradiation data, supports better geometric accuracy, considers reflections from near by urban context and uses predicted rooftop temperatures to calculate hourly PV efficiency. A validation study of measured and simulated electricity yields from two rooftop PV installations in Cambridge shows that the new method is able to predict annual electricity gains within 3.6–5.3{\%} of measured production when calibrating for actual weather data and detailed PV panel geometry. This predicted annual error using the new method is shown to be less than the variance which can be expected from climactic variation between years. Furthermore, because the new method generates hourly data, it can be applied to peak load mitigation studies at the urban level. This study also compares predicted monthly energy yields using the new method to those of preceding methods for the two validated test installations and on an annual basis for 10 buildings selected randomly from the Cambridge dataset.},
	author = {Jakubiec, J Alstan and Reinhart, Christoph F},
	doi = {http://dx.doi.org/10.1016/j.solener.2013.03.022},
	issn = {0038-092X},
	journal = {Solar Energy},
	keywords = {Geographical information systems,Photovoltaic electricity generation,Urban models,Urban photovoltaic potential},
	mendeley-groups = {My Teaching/Energy},
	month = {jul},
	pages = {127--143},
	title = {{A method for predicting city-wide electricity gains from photovoltaic panels based on LiDAR and GIS data combined with hourly Daysim simulations}},
	url = {http://www.sciencedirect.com/science/article/pii/S0038092X13001291},
	volume = {93},
	year = {2013}
}
@article{Grant2013,
	author = {Grant, Andrew and Razdan, Rohit and Shang, Thongchie},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grant, Razdan, Shang - 2013 - Transforming cities through GIS technology and geospatial analytics.pdf:pdf},
	journal = {McKinsey},
	mendeley-groups = {My Teaching/Energy},
	title = {{Transforming cities through GIS technology and geospatial analytics}},
	year = {2013}
}
@article{Finkelstein2013,
	annote = {NULL},
	author = {Finkelstein, Amy and Poterba, James},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Finkelstein, Poterba - 2013 - Adverse Selection in Insurance Markets Policyholder Evidence from the U.K. Annuity Market.pdf:pdf},
	journal = {Journal of Political Economy},
	number = {1},
	pages = {183--208},
	title = {{Adverse Selection in Insurance Markets: Policyholder Evidence from the U.K. Annuity Market}},
	volume = {112},
	year = {2013}
}
@article{Cole2013,
	author = {Cole, Rebel A},
	pages = {1--41},
	title = {{SME Credit Availability Around the World : Evidence from the World Bank ' s Enterprise Survey SME Credit Availability Around the World : Evidence from the World Bank ' s Enterprise Survey}},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2043624},
	year = {2013}
}
@article{Wei2013,
	abstract = {In contrast to “search goods” whose true quality can be determined before inspection, we examine information goods that are “experience goods” — goods whose true quality can only be determined through use. We analyze a “version-to-upgrade” strategy where a monopolist generates vertically differentiated versions as bridges that lead consumers to experience the goods so that they can assess their true quality, and then provide upgrades to consumers that initially purchase lower quality versions. Adopting a two-stage model, we find that if consumers have homogeneous expectations about quality before experience, then the version-to-upgrade strategy involves upgrading all the consumers that in the first stage purchased the low quality version. In this way, consumers that upgrade effectively pay a tax for learning. When consumers have heterogeneous expectations about quality before experience, if consumers are pessimistic, then the version-to-upgrade strategy still drives all consumers to upgrade. However, if consumers are optimistic, then, the version-to-upgrade strategy may induce only some of the consumers that initially purchased the low quality version to upgrade. As profits from upgrades increase, the monopolist sets the quality of the low quality version to the lowest quality that can feasibly reveal the true quality, justifying the use of trial or demonstration versions.},
	author = {Wei, Xueqi (David) and Nault, Barrie R},
	doi = {http://dx.doi.org/10.1016/j.dss.2012.11.006},
	issn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {Experience goods,Information goods,Pricing strategies,Versioning strategies},
	mendeley-groups = {My Research/Alpha},
	month = {dec},
	pages = {494--501},
	title = {{Experience information goods: “Version-to-upgrade”}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167923612003387},
	volume = {56},
	year = {2013}
}
@article{Bindseil2013,
	abstract = {This paper analyses the potential roles of bank asset fire sales and recourse to central bank credit to ensure banks' funding liquidity and solvency. Both asset liquidity and central bank haircuts are modeled as power functions within the unit interval. Funding stability is captured as strategic bank run game in pure strategies between depositors. Asset liquidity, the central bank collateral framework and regulation determine jointly the ability of the banking system to deliver maturity transformation and financial stability. The model also explains why banks tend to use the least liquid eligible assets as central bank collateral and why a sudden non-anticipated reduction of asset liquidity, or a tightening of the collateral framework, can destabilize short term liabilities of banks. Finally, the model allows discussing how the collateral framework can be understood, beyond its essential aim to protect the central bank, as financial stability and non-conventional monetary policy instrument.},
	author = {Bindseil, Ulrich},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bindseil - 2013 - Central bank collateral, asset fire sales, regulation and liquidity.pdf:pdf},
	isbn = {1610},
	issn = {1610},
	journal = {ECB Working Paper Series},
	keywords = {asset liquidity, liquidity regulation, bank run, c},
	number = {1610},
	pages = {42},
	title = {{Central bank collateral, asset fire sales, regulation and liquidity}},
	volume = {1610},
	year = {2013}
}
@article{Doris2013,
	abstract = {This technical report uses an established geospatial methodology to estimate the technical potential for renewable energy on tribal lands for the purpose of allowing Tribes to prioritize the development of renewable energy resources either for community scale on-tribal land use or for revenue generating electricity sales.},
	author = {Doris, E and Lopez, A and Beckley, D},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doris, Lopez, Beckley - 2013 - Geospatial Analysis of Renewable Energy Technical Potential on Tribal Lands.pdf:pdf;:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doris, Lopez, Beckley - 2013 - Geospatial Analysis of Renewable Energy Technical Potential on Tribal Lands(2).pdf:pdf},
	journal = {NREL Technical Report},
	keywords = {Market Transformation},
	pages = {59},
	title = {{Geospatial Analysis of Renewable Energy Technical Potential on Tribal Lands}},
	url = {http://www.nrel.gov/docs/fy13osti/56641.pdf},
	year = {2013}
}
@article{2013,
	author = {Павлов, Н.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Павлов - 2013 - Солнечная энергия – энергия будущего.pdf:pdf},
	journal = {Электроника},
	number = {123},
	pages = {130--137},
	title = {{Солнечная энергия – энергия будущего}},
	volume = {1},
	year = {2013}
}
@article{Frijters2013,
	abstract = {There is considerable policy interest in the impact of macroeconomic conditions on health-related behaviours and outcomes. This paper sheds new light on this issue by exploring the relationship between macroeconomic conditions and an indicator of problem drinking derived from state-level data on alcoholism-related Google searches conducted in the US over the period 2004-2011. We find the current recessionary period coincided with an almost 20{\%} increase in alcoholism-related searches. Controlling for state and time-effects, a 5{\%} rise in unemployment is followed in the next 12 months by an approximate 15{\%} increase in searches. The use of Internet searches to inform on health-related behaviours and outcomes is in its infancy; but we suggest that the data provides important real-time information for policy-makers and can help to overcome the under-reporting in surveys of sensitive information. ?? 2013 Elsevier Ltd.},
	author = {Frijters, Paul and Johnston, David W. and Lordan, Grace and Shields, Michael A.},
	doi = {10.1016/j.socscimed.2013.01.028},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frijters et al. - 2013 - Exploring the relationship between macroeconomic conditions and problem drinking as captured by Google searches.pdf:pdf},
	isbn = {1873-5347 (Electronic)$\backslash$n0277-9536 (Linking)},
	issn = {02779536},
	journal = {Social Science and Medicine},
	keywords = {Alcoholism,Global financial crisis,Google insights,Recession,United States},
	mendeley-groups = {My Research/Zeta},
	pages = {61--68},
	pmid = {23517705},
	publisher = {Elsevier Ltd},
	title = {{Exploring the relationship between macroeconomic conditions and problem drinking as captured by Google searches in the US}},
	url = {http://dx.doi.org/10.1016/j.socscimed.2013.01.028},
	volume = {84},
	year = {2013}
}
@article{Buyse2013,
	abstract = {We study the effects of pension reform on hours worked by three active generations, education of the young, the retirement decision of older workers, and aggregate growth in a four-period OLG model. The model explains important facts well for many OECD countries. Our simulation results prefer an intelligent pay-as-you-go system above a fully funded private system. Positive effects on employment and growth are the strongest when the pay-as-you-go system includes a tight link between individual labor income and the pension, and when it attaches a high weight to labor income earned as an older worker to compute the pension assessment base. {\textcopyright} 2012 Springer-Verlag.},
	author = {Buyse, Tim and Heylen, Freddy and van de Kerckhove, Renaat},
	doi = {10.1007/s00148-012-0416-x},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buyse, Heylen, van de Kerckhove - 2013 - Pension reform, employment by age, and long-run growth.pdf:pdf},
	isbn = {09331433},
	issn = {09331433},
	journal = {Journal of Population Economics},
	keywords = {Overlapping generations,Pension reform,Retirement},
	mendeley-groups = {My Research/Pensions},
	number = {2},
	pages = {769--809},
	pmid = {1351707},
	title = {{Pension reform, employment by age, and long-run growth}},
	volume = {26},
	year = {2013}
}
@book{Baltagi2013,
	abstract = {This chapter reviews the panel data forecasting literature. Starting with simple forecasts based on fixed and random effects panel data models. Next, these forecasts are extended to allow for various ARMA type structure on the disturbances, as well as spatial autoregressive and moving average type disturbances. These forecasting methods are then studied in the context of seemingly unrelated regressions. We highlight several forecasting empirical applications using panel data, as well as several Monte Carlo studies that compare various forecasting methods using panel data. The chapter concludes with suggestions for further work in this area. {\textcopyright} 2013 Elsevier B.V.},
	author = {Baltagi, Badi H.},
	booktitle = {Handbook of Economic Forecasting},
	doi = {10.1016/B978-0-444-62731-5.00018-X},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Forecasting/Baltagi  (2013) Panel Data Forecasting.pdf:pdf},
	isbn = {9780444627315},
	issn = {15740706},
	keywords = {Fixed effects,Heterogeneous panels,Random effects,Seemingly unrelated regressions,Serial correlation,Spatial dependence},
	mendeley-groups = {Forecasting/Forecasting with Panels},
	pages = {995--1024},
	publisher = {Elsevier B.V.},
	title = {{Panel data forecasting}},
	volume = {2},
	year = {2013}
}
@article{Schmidt2013,
	abstract = {Despite similar levels of per capita income, education and technology, the development of labour income shares in OECD countries has displayed different patterns since 1960. The paper examines the role of demography in this regard. We first use a standard overlapping generations model to derive the mechanisms by which demographic change can affect the labour share. It turns out that demographic change can affect the labour share either by altering the domestic capital intensity, by causing factor-biased technological change or in a small open economy framework by creating a gap between domestic savings and investments. The latter affects the country's investments abroad and in return its net foreign asset income which directly leads to changes in the labour share. Empirical estimations based on these insights, provide evidence that an increases in the expected retirement durations and old-age dependency ratios as well as declines in labour force growth rates have indeed been major forces behind the decline in labour shares that took place in many countries. These effects tend to be larger in open economies and pension reforms towards a funded pension system seem to have accelerated the effects. CR  - Copyright {\&}{\#}169; 2013 Springer},
	author = {Schmidt, Torsten and Vosen, Simeon},
	doi = {10.1007/s00148-012-0415-y},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Vosen - 2013 - Demographic change and the labour share of income.pdf:pdf},
	isbn = {09331433},
	issn = {09331433},
	journal = {Journal of Population Economics},
	keywords = {Demographic change,Labour share,Overlapping generations models,Panel cointegration},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {357--378},
	pmid = {1333727},
	title = {{Demographic change and the labour share of income}},
	volume = {26},
	year = {2013}
}
@article{Coble2013,
	author = {Coble, Keith H. and Barnett, Barry J.},
	doi = {10.1093/ajae/aas093},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coble, Barnett - 2013 - Why do we subsidize crop insurance.pdf:pdf},
	issn = {00029092},
	journal = {American Journal of Agricultural Economics},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {498--504},
	title = {{Why do we subsidize crop insurance?}},
	volume = {95},
	year = {2013}
}
@article{Markets2014,
	author = {Markets, International Equity},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Markets - 2014 - Centre for Applied Macroeconomic Analysis International Equity Markets.pdf:pdf},
	mendeley-groups = {My Research/Balts},
	title = {{Centre for Applied Macroeconomic Analysis International Equity Markets}},
	year = {2014}
}
@article{Broner2014,
	author = {Broner, F. and Erce, A. and Martin, A and Ventura, J.},
	journal = {Journal of Monetary Economics},
	mendeley-groups = {UPF/Adv. Macro III},
	number = {April},
	pages = {114--142},
	title = {{Sovereign Debt Markets in Trubulent Times: Creditor Discrimination and Crowding Out Effects}},
	volume = {61},
	year = {2014}
}
@phdthesis{Staudigel2014,
	author = {Staudigel, Matthias},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Staudigel - 2014 - Obesity , Food Demand , and Models of Rational Consumer Behaviour – Econometric Analyses and Challenges to Theory.pdf:pdf},
	mendeley-groups = {My Research/Zeta},
	school = {Justus Liebig University Gie ss en},
	title = {{Obesity , Food Demand , and Models of Rational Consumer Behaviour – Econometric Analyses and Challenges to Theory}},
	year = {2014}
}
@article{Kucuksari2014,
	abstract = {Finding the optimal size and locations for Photovoltaic (PV) units has been a major challenge for distribution system planners and researchers. In this study, a framework is proposed to integrate Geographical Information Systems (GIS), mathematical optimization, and simulation modules to obtain the annual optimal placement and size of PV units for the next two decades in a campus area environment. First, a GIS module is developed to find the suitable rooftops and their panel capacity considering the amount of solar radiation, slope, elevation, and aspect. The optimization module is then used to maximize the long-term net profit of PV installations considering various costs of investment, inverter replacement, operation, and maintenance as well as savings from consuming less conventional energy. A voltage profile of the electricity distribution network is then investigated in the simulation module. In the case of voltage limit violation by intermittent PV generations or load fluctuations, two mitigation strategies, reallocation of the PV units or installation of a local storage unit, are suggested. The proposed framework has been implemented in a real campus area, and the results show that it can effectively be used for long-term installation planning of PV panels considering both the cost and power quality. ?? 2013 Elsevier Ltd.},
	author = {Kucuksari, Sadik and Khaleghi, Amirreza M. and Hamidi, Maryam and Zhang, Ye and Szidarovszky, Ferenc and Bayraksan, Guzin and Son, Young Jun},
	doi = {10.1016/j.apenergy.2013.09.002},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kucuksari et al. - 2014 - An Integrated GIS, optimization and simulation framework for optimal PV size and location in campus area envir.pdf:pdf},
	isbn = {0306-2619},
	issn = {03062619},
	journal = {Applied Energy},
	keywords = {Distributed generation,GIS,Optimal location and size,Photovoltaic,Power quality},
	mendeley-groups = {My Research/Eta},
	pages = {1601--1613},
	publisher = {Elsevier Ltd},
	title = {{An Integrated GIS, optimization and simulation framework for optimal PV size and location in campus area environments}},
	volume = {113},
	year = {2014}
}
@unpublished{Nikolov2014,
	author = {Nikolov, Kalin},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nikolov - 2014 - Collateral Amplification under Complete Markets.pdf:pdf},
	isbn = {9789289911245},
	keywords = {amplification.,collateral constraints},
	mendeley-groups = {Financial Frictions},
	series = {ECB Working Papers},
	title = {{Collateral Amplification under Complete Markets}},
	year = {2014}
}
@article{Gorton2014,
	abstract = {Short-term collateralized debt, private money, is efficient if agents are willing to lend without producing costly information about the collateral backing the debt. When the economy relies on such informationally insensitive debt, firms with low quality collateral can borrow, generating a credit boom and an increase in output. Financial fragility is endogenous; it builds up over time as information about counterparties decays. A crisis occurs when a ( possibly small) shock causes agents to suddenly have incentives to produce information, leading to a decline in output. A social planner would produce more information than private agents but would not always want to eliminate fragility. (JEL D83, E23, E32, E44, G01)},
	author = {Gorton, By Gary and Ordo{\~{n}}ez, Guillermo},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gorton, Ordo{\~{n}}ez - 2014 - Collateral Crises.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {Financial Frictions},
	pages = {343--378},
	title = {{Collateral Crises}},
	volume = {104},
	year = {2014}
}
@article{Broner2014,
	abstract = {In 2007, countries in the euro zone periphery were enjoying stable growth, low deficits, and low spreads. Then the financial crisis erupted and pushed them into deep recessions, raising their deficits and debt levels. By 2010, they were facing severe debt problems. Spreads increased and, surprisingly, so did the share of the debt held by domestic creditors. Credit was reallocated from the private sector to the public sector, reducing investment and deepening the recessions even further. To account for these facts, we propose a simple model of sovereign risk in which debt can be traded in secondary markets. The model has two key ingredients: creditor discrimination and crowding-out effects. Creditor discrimination arises because, in turbulent times, sovereign debt offers a higher expected return to domestic creditors than to foreign ones. This provides incentives for domestic purchases of debt. Crowding-out effects arise because private borrowing is limited by financial frictions. This implies that domestic debt purchases displace productive investment. The model shows that these purchases reduce growth and welfare, and may lead to self-fulfilling crises. It also shows how crowding-out effects can be transmitted to other countries in the euro zone, and how they may be addressed by policies at the European level. {\textcopyright} 2013 Elsevier B.V.},
	author = {Broner, Fernando and Erce, Aitor and Martin, Alberto and Ventura, Jaume},
	doi = {10.1016/j.jmoneco.2013.11.009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Broner et al. - 2014 - Sovereign debt markets in turbulent times Creditor discrimination and crowding-out effects.pdf:pdf},
	issn = {03043932},
	journal = {Journal of Monetary Economics},
	keywords = {Crowding out,Discrimination,Economic growth,Rollover crises,Sovereign debt},
	mendeley-groups = {UPF/Adv. Macro III/Martin,UPF/Adv. Macro III},
	number = {1},
	pages = {114--142},
	publisher = {Elsevier},
	title = {{Sovereign debt markets in turbulent times: Creditor discrimination and crowding-out effects}},
	volume = {61},
	year = {2014}
}
@article{Verbic2014,
	abstract = {A rapidly aging population in high-income countries has exerted additional pressure on the sustainability of public pension expenditure. We present a theoretical model of public pension expenditure under endogenous human capital, where the latter facilitates a substantial decrease in equilibrium fertility rate alongside the improvement in life expectancy. We demonstrate how higher life expectancy and human capital endowment facilitate a rise of net replacement rate. We then provide and examine an empirical model of old-age expenditure in a panel of 33 countries for the period 1998-2008. Our results indicate that increases in effective retirement age and total fertility rate would reduce age-related expenditure substantially. While higher net replacement rate would alleviate the risk of old-age poverty, further increases would add considerable pressure on the fiscal sustainability of public pensions.},
	author = {Verbi{\v{c}}, Miroslav and Spruk, Rok},
	doi = {10.2298/PAN1403289V},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verbi{\v{c}}, Spruk - 2014 - Aging population and public pensions Theory and macroeconometric evidence.pdf:pdf},
	isbn = {9789616543897},
	issn = {1452595X},
	journal = {Panoeconomicus},
	keywords = {Ageing,Life expectancy,Public pensions,Replacement rate,Social security},
	mendeley-groups = {My Research/Pensions},
	number = {3},
	pages = {289--316},
	title = {{Aging population and public pensions: Theory and macroeconometric evidence}},
	volume = {61},
	year = {2014}
}
@article{Cipriani2014,
	abstract = {This paper shows the effects on a pay-as-you-go pension system of the demographic change in the standard overlapping generations model. Firstly, we consider a setting with exogenous fertility and then a model with endogenous fertility. In both cases, population aging due to increased longevity implies a reduction in pensions payouts.},
	author = {Cipriani, Giam Pietro},
	doi = {10.1007/s00148-013-0465-9},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cipriani - 2014 - Population aging and PAYG pensions in the OLG model.pdf:pdf},
	issn = {09331433},
	journal = {Journal of Population Economics},
	keywords = {Fertility,Longevity,PAYG pensions},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {251--256},
	title = {{Population aging and PAYG pensions in the OLG model}},
	volume = {27},
	year = {2014}
}
@article{Ediev2014,
	abstract = {When pension systems are contrasted it is common to use simplified demographic models, such as overlapping generation models with time-invariant mortality. Breaking with this tradition, we show that for a population with increasing longevity, the pay-as-you-go (PAYG) system may be more advantageous than a funded system (FS). Increasing longevity favours the PAYG system because for the workers living longer at retirement than current retirees, it is less costly to fund others' current pensions than their own. At present, the effect amounts to around 15 per cent in terms of the dependency ratio, or six more years at work in the FS, or 1 per cent per annum in terms of the real interest rate. In most developed countries the effect substantially exceeds that of the usually studied biological interest rate.},
	author = {Ediev, Dalkhat M.},
	doi = {10.1080/00324728.2013.780632},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ediev - 2014 - Why increasing longevity may favour a PAYG pension system over a funded system.pdf:pdf},
	issn = {00324728},
	journal = {Population Studies},
	keywords = {changing mortality,funded pension system,internal rate of return,old-age dependency ratio,pay-as-you-go pension system,pensions},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {95--110},
	pmid = {23587003},
	title = {{Why increasing longevity may favour a PAYG pension system over a funded system}},
	volume = {68},
	year = {2014}
}
@article{Knell2014,
	author = {Knell, Markus},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Knell - 2014 - Increasing Life Expectancy and Pay-As-You-Go Pension Systems.pdf:pdf},
	keywords = {40420 7218,40420 7299,43-1,a-1011 vienna,at,demographic change,economic studies division,email,fax,financial stability,h55,j1,j18,j26,jel-classification,knell,markus,oenb,otto-wagner-platz 3,pension system,phone,pob-61,the views expressed in,this},
	mendeley-groups = {My Research/Pensions},
	title = {{Increasing Life Expectancy and Pay-As-You-Go Pension Systems}},
	year = {2014}
}
@article{Szulecka2014,
	abstract = {This paper traces macro-level trends and changing approaches to plantation forestry, with particular emphasis on tropical and subtropical regions. Introducing the theoretical concept of a paradigm and drawing on the notions of discourse and epistemic community, it analyses the development of knowledge structures present in the history of plantation forestry. The historical context with an economic and developmental focus is provided to better understand the political economy of forest plantations in the South. A typology of plantation paradigms according to the selected criteria is put forth to illustrate both the discursive and technical changes plantation projects underwent in time. The paper concludes with a critical discussion on the parallels between the historical developments, change in economic thought and development aid and their influence on tree-planting activities as well as the strengths, weaknesses and challenges for both the plantation and general forestry epistemic community in the years to come.},
	author = {Szulecka, J and Pretzsch, J and Seccob, L},
	doi = {10.1505/146554814811724829},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szulecka, Pretzsch, Seccob - 2014 - Paradigms in tropical forest plantations A critical reflection on historical shifts in plantation ap.pdf:pdf},
	issn = {14655489},
	journal = {International Forestry Review},
	keywords = {development,forest plantations,history of plantations,paradigm change,planted forests},
	number = {2},
	pages = {128--143},
	title = {{Paradigms in tropical forest plantations: A critical reflection on historical shifts in plantation approaches}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84906330146{\&}partnerID=40{\&}md5=40aec5a23027018f4323a670b947a0ff},
	volume = {16},
	year = {2014}
}
@unpublished{Leonard2014,
	address = {Montreal},
	author = {L{\'{e}}onard, Daniel and Long, Ngo Van},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\'{e}}onard, Long - 2014 - Endogenous Changes in Tastes.pdf:pdf},
	institution = {CIRANO},
	mendeley-groups = {My Research/Alpha},
	number = {February},
	series = {Scientific Series},
	title = {{Endogenous Changes in Tastes}},
	year = {2014}
}
@book{Bekhouche2014,
	author = {Bekhouche, Yasmina and Hausmann, Ricardo and Tyson, Laura D'Andrea and Zahidi, Saadia and {World Economic Forum} and {Harvard University} and {University of California}, Berkeley},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bekhouche et al. - 2014 - The Global Gender Gap Report 2014.pdf:pdf},
	isbn = {978-92-95044-38-8 92-95044-38-X},
	mendeley-groups = {My Research/Prima},
	publisher = {World Economic Forum},
	title = {{The Global Gender Gap Report 2014}},
	year = {2014}
}
@inproceedings{Wang2014,
	address = {Berlin, Heidelberg},
	author = {Wang, Ke and Huo, Ran and Zhang, Qiao and Zhou, Xianhua},
	doi = {10.1007/978-3-642-54389-0_16},
	editor = {Xu, Shiwei},
	isbn = {978-3-642-54389-0},
	mendeley-groups = {My Research/Theta},
	pages = {179--191},
	publisher = {Springer Berlin Heidelberg},
	title = {{Study on Optimal Risk Sharing Issue for Chinese Crop Insurance: A Case Study from Government Perspective}},
	url = {http://dx.doi.org/10.1007/978-3-642-54389-0{\_}16},
	year = {2014}
}
@article{OxfordEconomics2014,
	author = {{Oxford Economics} and PWC},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oxford Economics, PWC - 2014 - Outlook to 2025.pdf:pdf},
	pages = {1--24},
	title = {{Outlook to 2025}},
	year = {2014}
}
@article{Giesecke2014,
	author = {Giesecke, Kay and Longstaff, Francis A and Schaefer, Stephen and Strebulaev, Ilya A},
	doi = {10.1016/j.jfineco.2013.10.014},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giesecke et al. - 2014 - Macroeconomic effects of corporate default crisis A long-term perspective.pdf:pdf},
	issn = {0304-405X},
	journal = {Journal of Financial Economics},
	keywords = {Banking crises,Corporate default rates,Financial crises},
	mendeley-groups = {Financial Frictions},
	number = {2},
	pages = {297--310},
	publisher = {Elsevier},
	title = {{Macroeconomic effects of corporate default crisis: A long-term perspective}},
	url = {http://dx.doi.org/10.1016/j.jfineco.2013.10.014},
	volume = {111},
	year = {2014}
}
@article{Gatti2014,
	author = {Gatti, Stefano},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatti - 2014 - Government and Market Based Instruments and Incentives to Stimulate Long-term Investment Finance in Infrastructure.pdf:pdf},
	number = {37},
	title = {{Government and Market Based Instruments and Incentives to Stimulate Long-term Investment Finance in Infrastructure}},
	year = {2014}
}
@article{Zhang2014,
	abstract = {This article studies model averaging for linear mixed-effects models. We establish an unbiased estimator of the squared risk for the model averaging, and use the estimator as a criterion for choosing weights. The resulting model average estimator is proved to be asymptotically optimal under some regularity conditions. Simulation experiments show it is superior or comparable to estimators based on the final models selected by the commonly-used methods and some existing averaging procedures. The proposed procedure is applied to data from an AIDS clinic trial. {\textcopyright} 2014 Biometrika Trust.},
	author = {Zhang, Xinyu and Zou, Guohua and Liang, Hua},
	doi = {10.1093/biomet/ast052},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Zhang Zou, Liang  (2014) Model averaging and weight choice in linear mixed-effects models.pdf:pdf},
	issn = {00063444},
	journal = {Biometrika},
	keywords = {Asymptotic optimality,Conditional Akaike information criterion,Model averaging,Squared loss},
	mendeley-groups = {Model Selection and Averaging/Panel},
	number = {1},
	pages = {205--218},
	title = {{Model averaging and weight choice in linear mixed-effects models}},
	volume = {101},
	year = {2014}
}
@article{Dhaene2014,
	author = {Dhaene, Geert and Jochmans, Koen},
	doi = {10.1093/restud/rdv007},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Nonlinear Panels/Dhaene, Jochmans (2015) Split Panel Jackknife.pdf:pdf},
	journal = {The Review of Economic Studies},
	mendeley-groups = {Panel Data/Nonlinear Panels,Resampling/Jacknife},
	number = {3},
	pages = {991--1030},
	title = {{Split-Panel Jackknife Estimation of Fixed-Effect Models}},
	volume = {82},
	year = {2015}
}
 
@article{Ng2014,
	abstract = {BACKGROUND: In the past decade, the United States has seen declining energy intakes and plateauing obesity levels. OBJECTIVE: We examined whether these observed trends suggest a longer-term shift in dietary and health behavior that is independent of adverse economic conditions. DESIGN: We used nationally representative cross-sectional surveys on intake and longitudinal household food purchase data along with random-effects models to address this question. Data included individuals in NHANES 2003-2004 to 2009-2010 (children: n = 13,422; adults: n = 10,791) and households from the 2000-2011 Nielsen Homescan Panel (households with children: n = 57,298; households with adults only: n = 108,932). RESULTS: In both data sets, we showed that children decreased their calories the most. Even after we controlled for important socioeconomic factors, caloric purchases fell significantly from 2003 to 2011 (P {\textless} 0.001), particularly for households with children. The Great Recession was associated with small increases in caloric purchases, in which a 1-percentage point increase in unemployment in the local market was associated with a 1.6-4.1-kcal {\textperiodcentered} capita⁻¹ {\textperiodcentered} d⁻¹ (P {\textless} 0.001) increase in total calories purchased. Results also indicated shifts in caloric purchases were driven more by declines in caloric purchases from beverages than food. CONCLUSIONS: US consumers have exhibited changes in intake and purchasing behavior since 2003 that were independent from changing economic conditions linked with the Great Recession or food prices. Public health efforts in the past decade may have contributed to this trend.},
	author = {Ng, Shu Wen and Slining, Meghan M and Popkin, Barry M},
	doi = {10.3945/ajcn.113.072892},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Slining, Popkin - 2014 - Turning point for US diets Recessionary effects or behavioral shifts in foods purchased and consumed.pdf:pdf},
	issn = {1938-3207},
	journal = {The American Journal of Clinical Nutrition},
	keywords = {Adult,Age Factors,Beverages,Beverages: economics,Child,Child Nutritional Physiological Phenomena,Cross-Sectional Studies,Diet,Diet Surveys,Diet: adverse effects,Diet: economics,Diet: trends,Economic,Economic Recession,Energy Intake,Family Characteristics,Female,Food Supply,Food Supply: economics,Health Behavior,Health Transition,Humans,Longitudinal Studies,Male,Models,Obesity,Obesity: epidemiology,Obesity: etiology,Obesity: prevention {\&} control,Unemployment,United States,United States: epidemiology},
	mendeley-groups = {My Research/Zeta},
	month = {mar},
	number = {3},
	pages = {609--16},
	pmid = {24429538},
	title = {{Turning point for US diets? Recessionary effects or behavioral shifts in foods purchased and consumed.}},
	url = {http://ajcn.nutrition.org/cgi/content/long/99/3/609},
	volume = {99},
	year = {2014}
}
@article{Kaplan2014,
	abstract = {Fiscal stimulus payments (i.e., direct lump-sum payments from the government to households) were used in the recessions of 2001 and 2008 in an attempt to simultaneously alleviate households' economic hardship and stimulate aggregate demand. Despite the similarities between the two stimulus policies, there were important differences in both their design and the prevailing economic conditions. We use the model of Kaplan and Violante (2013) to compare the consumption response to these policies. Consistent with empirical evidence from microdata, we find that the consumption response was around one-third lower in 2008, primarily due to the larger size of the payments. [ABSTRACT FROM AUTHOR]},
	author = {Kaplan, Greg and Violante, Giovanni L.},
	doi = {10.1257/aer.104.5.116},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaplan, Violante - 2014 - A tale of two stimulus payments 2001 versus 2008.pdf:pdf},
	issn = {00028282},
	journal = {American Economic Review},
	mendeley-groups = {UPF/Adv. Macro II/Consumption},
	number = {5},
	pages = {116--121},
	title = {{A tale of two stimulus payments: 2001 versus 2008}},
	volume = {104},
	year = {2014}
}
@article{Hagen2014,
	abstract = {citation: Hagen, Roy. 2014. Lessons Learned from Community Forestry and Their Relevance for REDD+. USAID- supported Forest Carbon, Markets and Communities (FCMC) Program. Washington, DC, USA.},
	author = {Hagen, Roy},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hagen - 2014 - Lessons Learned From Community Forestry and Their Relevance Implications for for.pdf:pdf},
	journal = {USAID- supported Forest Carbon, Markets and Communities (FCMC) Program.},
	number = {MARCH},
	title = {{Lessons Learned From Community Forestry and Their Relevance Implications for for}},
	year = {2014}
}
@article{Liang2014,
	abstract = {The distribution of P2X receptors on neurons in rat superior cervical ganglia and lability of P2X receptors on exposure to agonists were determined. Antibody labeling of each P2X subtype P2X(1)-P2X(7) showed neurons isolated into culture possessed primarily P2X(2) subunits with others occurring in order P2X(7) {\textgreater} P2X(6) {\textgreater} P2X(3) {\textgreater} P2X(1) {\textgreater} P2X(5) {\textgreater} P2X(4). Application of ATP and alpha,beta-meATP to neurons showed they possessed a predominantly nondesensitizing P2X receptor type insensitive to alpha,beta-meATP, consistent with immunohistochemical observations. P2X(1)-green fluorescent protein (GFP) was used to study the time course of P2X(1) receptor clustering in plasma membranes of neurons and internalization of receptors following prolonged exposure to ATP. At 12-24 h after adenoviral infection, P2X(1)-GFP formed clusters about 1 microm diameter in the neuron membrane. Application of ATP and alpha,beta-meATP showed these neurons possessed a predominantly desensitizing P2X receptor type sensitive to alpha,beta-meATP. Infection converted the major functional P2X receptor type in the membrane to P2X(1). Exposure of infected neurons to alpha,beta-meATP for less than 60 s led to the disappearance of P2X(1)-GFP fluorescence from the cell surface that was blocked by monensin, indicating the chimera is normally endocytosed into these organelles on exposure to agonist.},
	author = {Liang, Faming and Yuan, Cheng and Lin, Guang},
	doi = {10.1080/01621459.2013.872993},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Yuan, Lin - 2014 - Simulated Stochastic Approximation Annealing for Global Optimization With a Square-Root Cooling Schedule.pdf:pdf},
	journal = {Journal of the American Statistical Association},
	keywords = {local trap,markov chain monte carlo,simulated annealing,stochastic,stochastic approximation monte carlo},
	number = {37},
	pages = {29107--29112},
	title = {{Simulated Stochastic Approximation Annealing for Global Optimization With a Square-Root Cooling Schedule}},
	volume = {275},
	year = {2014}
}
@article{Krugman2014,
	abstract = {Ever since Greece experienced its debt crisis, fiscal discussion has been “Hellenized”—that is, there are constant warnings that other countries, including the United States, are on the verge of a similar crisis. But can countries that borrow in their own currency experience Greek-type crises? I argue, based both on evidence and on simple modeling, that the answer is no.},
	author = {Krugman, Paul},
	doi = {10.1057/imfer.2014.9},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krugman - 2014 - Currency Regimes, Capital Flows, and Crises.pdf:pdf},
	issn = {2041-4161},
	journal = {IMF Economic Review},
	mendeley-groups = {Macrofinance},
	number = {4},
	pages = {470--493},
	title = {{Currency Regimes, Capital Flows, and Crises}},
	url = {http://dx.doi.org/10.1057/imfer.2014.9},
	volume = {62},
	year = {2014}
}
@article{Resch2014,
	abstract = {In the face of the broad political call for an “energy turnaround”, we are currently witnessing three essential trends with regard to energy infrastructure planning, energy generation and storage: from planned production towards fluctuating production on the basis of renewable energy sources, from centralized generation towards decentralized generation and from expensive energy carriers towards cost-free energy carriers. These changes necessitate considerable modifications of the energy infrastructure. Even though most of these modifications are inherently motivated by geospatial questions and challenges, the integration of energy system models and Geographic Information Systems (GIS) is still in its infancy. This paper analyzes the shortcomings of previous approaches in using GIS in renewable energy-related projects, extracts distinct challenges from these previous efforts and, finally, defines a set of core future research avenues for GIS-based energy infrastructure planning with a focus on the use of renewable energy. These future research avenues comprise the availability base data and their “geospatial awareness”, the development of a generic and unified data model, the usage of volunteered geographic information (VGI) and crowdsourced data in analysis processes, the integration of 3D building models and 3D data analysis, the incorporation of network topologies into GIS, the harmonization of the heterogeneous views on aggregation issues in the fields of energy and GIS, fine-grained energy demand estimation from freely-available data sources, decentralized storage facility planning, the investigation of GIS-based public participation mechanisms, the transition from purely structural to operational planning, data privacy aspects and, finally, the development of a new dynamic power market design.},
	author = {Resch, Bernd and Sagl, G{\"{u}}nther and T{\"{o}}rnros, Tobias and Bachmaier, Andreas and Eggers, Jan-Bleicke and Herkel, Sebastian and Narmsara, Sattaya and G{\"{u}}ndra, Hartmut},
	doi = {10.3390/ijgi3020662},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Resch et al. - 2014 - GIS-Based Planning and Modeling for Renewable Energy Challenges and Future Research Avenues.pdf:pdf},
	issn = {2220-9964},
	journal = {ISPRS International Journal of Geo-Information},
	keywords = {GIS and renewable energy,GIS-based energy infrastructure planning,fluctuating renewables,future research challenges,integration of GIS and energy system models,operation optimization,structural planning of local energy systems},
	mendeley-groups = {My Research/Eta},
	number = {2},
	pages = {662--692},
	title = {{GIS-Based Planning and Modeling for Renewable Energy: Challenges and Future Research Avenues}},
	url = {http://www.mdpi.com/2220-9964/3/2/662/htm},
	volume = {3},
	year = {2014}
}
@article{Lee2014,
	abstract = {In this paper we derive the asymptotic properties of GMM estimators for the spatial dynamic panel data model with fixed effects when n is large, and T can be large, but small relative to n. The GMM estimation methods are designed with the fixed individual and time effects eliminated from the model, and are computationally tractable even under circumstances where the ML approach would be either infeasible or computationally complicated. The ML approach would be infeasible if the spatial weights matrix is not row-normalized while the time effects are eliminated, and would be computationally intractable if there are multiple spatial weights matrices in the model; also, consistency of the MLE would require T to be large and not small relative to n if the fixed effects are jointly estimated with other parameters of interest. The GMM approach can overcome all these difficulties. We use exogenous and predetermined variables as instruments for linear moments, along with several levels of their neighboring variables and additional quadratic moments. We stack up the data and construct the best linear and quadratic moment conditions. An alternative approach is to use separate moment conditions for each period, which gives rise to many moments estimation. We show that these GMM estimators are nT consistent, asymptotically normal, and can be relatively efficient. We compare these approaches on their finite sample performance by Monte Carlo. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
	author = {Lee, Lung Fei and Yu, Jihai},
	doi = {10.1016/j.jeconom.2014.03.003},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yu - 2014 - Efficient GMM estimation of spatial dynamic panel data models with fixed effects.pdf:pdf},
	isbn = {0304-4076},
	issn = {03044076},
	journal = {Journal of Econometrics},
	keywords = {Dynamic panels,Fixed effects,Generalized method of moment,Many moments,Spatial autoregression},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {2},
	pages = {174--197},
	publisher = {Elsevier B.V.},
	title = {{Efficient GMM estimation of spatial dynamic panel data models with fixed effects}},
	url = {http://dx.doi.org/10.1016/j.jeconom.2014.03.003},
	volume = {180},
	year = {2014}
}
@article{Goodwin2014,
	abstract = {The federal crop insurance program has been a major fixture of U.S. agricultural policy since the 1930s, and continues to grow in size and importance. Indeed, it now represents the most prominent farm policy instrument, accounting for more government spending than any other farm commodity program. The 2014 Farm Bill further expanded the crop insurance program and introduced a number of new county-level revenue insurance plans. In 2013, over {\$}123 billion in crop value was insured under the program. Crop revenue insurance, first introduced in the 1990s, now accounts for nearly 70{\%} of the total liability in the program. The available plans cover losses that result from a revenue shortfall that can be triggered by multiple, dependent sources of risk—either low prices, low yields, or a combination of both. The actuarial practices currently applied when rating these plans essentially involve the application of a Gaussian copula model to the pricing of dependent risks. We evaluate the suitability of this assumption by considering a number of alternative copula models. In particular, we use combinations of pair-wise copulas of conditional distributions to model multiple sources of risk. We find that this approach is generally preferred by model-fitting criteria in the applications considered here. We demonstrate that alternative approaches to modeling dependencies in a portfolio of risks may have significant implications for premium rates in crop insurance.},
	author = {Goodwin, Barry K. and Hungerford, Ashley},
	doi = {10.1093/ajae/aau086},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodwin, Hungerford - 2014 - Copula-based models of systemic risk in U.S. Agriculture Implications for crop insurance and reinsurance co.pdf:pdf},
	issn = {14678276},
	journal = {American Journal of Agricultural Economics},
	keywords = {Copula models,Crop insurance,Systemic risk},
	mendeley-groups = {My Research/Theta/Review},
	number = {3},
	pages = {879--896},
	title = {{Copula-based models of systemic risk in U.S. Agriculture: Implications for crop insurance and reinsurance contracts}},
	volume = {97},
	year = {2014}
}
@article{??sgeirsd??ttir2014,
	abstract = {This study uses the 2008 economic crisis in Iceland to identify the effects of a macroeconomic downturn on a range of health behaviors. We use longitudinal survey data that include pre- and post-reports from the same individuals on a range of health-compromising and health-promoting behaviors. We find that the crisis led to large and significant reductions in health-compromising behaviors (such as smoking, drinking alcohol or soft drinks, and eating sweets) and certain health-promoting behaviors (consumption of fruits and vegetables), but to increases in other health-promoting behaviors (consumption of fish oil and recommended sleep). The magnitudes of effects for smoking are somewhat larger than what has been found in past research in other contexts, while those for alcohol, fruits, and vegetables are in line with estimates from other studies. Changes in work hours, real income, financial assets, mortgage debt, and mental health, together, explain the effects of the crisis on some behaviors (such as consumption of sweets and fast food), while the effects of the crisis on most other behaviors appear to have operated largely through price increases. ?? 2013 Elsevier B.V.},
	author = {{\'{A}}sgeirsd{\'{o}}ttir, Tinna Laufey and Corman, Hope and Noonan, Kelly and {\'{O}}lafsd{\'{o}}ttir, Þ{\'{o}}rhildur and Reichman, Nancy E.},
	doi = {10.1016/j.ehb.2013.03.005},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\'{A}}sgeirsd{\'{o}}ttir et al. - 2014 - Was the economic crisis of 2008 good for Icelanders Impact on health behaviors.pdf:pdf},
	isbn = {1570-677X},
	issn = {1570677X},
	journal = {Economics and Human Biology},
	keywords = {Economic crisis,Health behaviors,Iceland,Recessions},
	mendeley-groups = {My Research/Zeta},
	number = {1},
	pages = {1--19},
	pmid = {23659821},
	title = {{Was the economic crisis of 2008 good for Icelanders? Impact on health behaviors}},
	volume = {13},
	year = {2014}
}
@incollection{Rossi2015,
	address = {London},
	author = {Rossi, Emanuele and Stepic, Rok},
	doi = {10.1057/9781137524041_4},
	editor = {Rossi, Emanuele and Stepic, Rok},
	isbn = {978-1-137-52404-1},
	pages = {52--62},
	publisher = {Palgrave Macmillan UK},
	title = {{Bank Conventional Lending versus Project Bond Solution BT  - Infrastructure Project Finance and Project Bonds in Europe}},
	url = {http://dx.doi.org/10.1057/9781137524041{\_}4},
	year = {2015}
}
@article{Yang2015,
	abstract = {Motivated by a recent study of Bao and Ullah (2007a) on finite sample properties of MLE in the pure SAR (spatial autoregressive) model, a general method for third-order bias and variance corrections on a nonlinear estimator is proposed based on stochastic expansion and bootstrap. Working with concentrated estimating equation simplifies greatly the high-order expansions for bias and variance; a simple bootstrap procedure overcomes a major difficulty in analytically evaluating expectations of various quantities in the expansions. The method is then studied in detail using a more general SAR model, with its effectiveness in correcting bias and improving inference fully demonstrated by extensive Monte Carlo experiments. Compared with the analytical approach, the proposed approach is much simpler and has a much wider applicability. The validity of the bootstrap procedure is formally established. The proposed method is then extended to the case of more than one nonlinear estimator.},
	author = {Yang, Zhenlin},
	doi = {10.1016/j.jeconom.2014.07.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Small Sample Results/Yang  (2015) A general method for third-order bias and variance corrections on a nonlinear estimator.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Bootstrap,Concentrated estimating equation,Monte Carlo,Spatial layout,Stochastic expansion,Third-order bias,Third-order variance},
	mendeley-groups = {Small Sample Results},
	number = {1},
	pages = {178--200},
	publisher = {Elsevier B.V.},
	title = {{A general method for third-order bias and variance corrections on a nonlinear estimator}},
	volume = {186},
	year = {2015}
}
@article{2015,
	author = {Преобразований, Назревших},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Преобразований - 2015 - Научные сообщения. форум.pdf:pdf},
	mendeley-groups = {My Research/Theta/Review/R},
	pages = {93--100},
	title = {{Научные сообщения. форум}},
	year = {2015}
}
@book{UNECE2015,
	address = {Geneva, Swtizerland},
	author = {UNECE and FAO},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/UNECE, FAO - 2015 - Forests in the ECE Region Trends and Challenges in Achieving the Global Objectives on Forests.pdf:pdf},
	isbn = {9789211170887},
	publisher = {UNITED NATIONS ECONOMIC COMMISSION FOR EUROPE},
	title = {{Forests in the ECE Region Trends and Challenges in Achieving the Global Objectives on Forests}},
	year = {2015}
}
@article{Chen2015,
	author = {Chen, Ying Erh and Goodwin, Barry K.},
	doi = {10.1371/journal.pone.0145384},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodwin - 2015 - Policy design of multi-year crop insurance contracts with partial payments.pdf:pdf},
	issn = {19326203},
	journal = {PLoS ONE},
	mendeley-groups = {My Research/Theta/Review},
	number = {12},
	pages = {1--15},
	title = {{Policy design of multi-year crop insurance contracts with partial payments}},
	volume = {10},
	year = {2015}
}
@article{2015,
	author = {Хожаинов, Н.Т. and Назарова, А.А.},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Хожаинов, Назарова - 2015 - Совершенствование государственной поддержки сельскохозяйственного страхования в России.pdf:pdf},
	journal = {Теория и практика общественного развития},
	mendeley-groups = {My Research/Theta/Review/R},
	pages = {52--54},
	title = {{Совершенствование государственной поддержки сельскохозяйственного страхования в России}},
	url = {http://istina.msu.ru/publications/article/8954253/},
	volume = {6},
	year = {2015}
}
@article{Shi2015,
	author = {Shi, Xiaoxia},
	doi = {10.3982/qe382},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi - 2015 - A Nondegenerate Vuong Test.pdf:pdf},
	journal = {Quantitative Economics},
	keywords = {Asymptotic size,C12,C52,Vuong test,model comparison,nonnested models,voter turnout},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	number = {1},
	pages = {85--121},
	title = {{A Nondegenerate Vuong Test}},
	volume = {6},
	year = {2015}
}
@article{Maryudi2015,
	abstract = {Abstract Smallholder tree plantation, now on the increase in Indonesia, has long been practiced by rural farmers as a strategy to optimize the expected utility of land, labor and other constraints. Increasing demand for timber has driven a shift toward commercialization of smallholder forestry. However, smallholders face huge challenges when they seek for commercial markets in the form of complex regulatory frameworks applied to smallholder plantations. This paper discusses the case of smallholder plantations in Gunungkidul District (Indonesia), considered one of the most commercialized timber marketing hubs for local, national and international markets. It analyzes how opportunities and challenges, resulted from different regulatory frameworks, affect the competitiveness of smallholder forestry practices. In this paper, regulatory frameworks are defined as not only regulations issued by public administrators at the domestic (local and national) level, but also cover the emerging market-based regulatory frameworks, i.e. voluntary certification of sustainable forestry and mandatory timber legality verification.},
	author = {Maryudi, Ahmad and Nawir, Ani A and Permadi, Dwiko B and Purwanto, Ris H and Pratiwi, Dian and Syofi'i, Ahmad and Sumardamto, Purnomo},
	doi = {http://dx.doi.org/10.1016/j.forpol.2015.05.010},
	issn = {1389-9341},
	journal = {Forest Policy and Economics},
	keywords = {Commercial markets,Regulatory,Smallholder},
	month = {oct},
	pages = {1--6},
	title = {{Complex regulatory frameworks governing private smallholder tree plantations in Gunungkidul District, Indonesia}},
	url = {http://www.sciencedirect.com/science/article/pii/S1389934115300095},
	volume = {59},
	year = {2015}
}
@article{Annan2015,
	author = {Annan, Francis and Schlenker, Wolfram},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Annan, Schlenker - 2015 - Federal Crop Insurance and the Disincentives to Adapt to Extreme Heat.pdf:pdf},
	journal = {American Economic Review},
	mendeley-groups = {My Research/Theta/Review},
	number = {May},
	pages = {262--266},
	title = {{Federal Crop Insurance and the Disincentives to Adapt to Extreme Heat}},
	volume = {105},
	year = {2015}
}
@article{Mollick2015,
	abstract = {Using a large survey with 47,188 backers of Kickstarter projects, I examined the factors that led to projects failing to deliver their promised rewards. Among funded projects, a failure to deliver seems relatively rare, accounting for around 9{\%} of all projects, with a possible range of 5{\%} to 14{\%}. There are few indicators at the time of project funding as to which projects might ultimately fail to deliver rewards, though small projects (and to a lesser extent very large projects) are more likely to fail to deliver rewards, as are some project categories. The demographics of project creators (including gender, education level, and family status) did not significantly affect the chance of a project succeeding.},
	author = {Mollick, Ethan R.},
	doi = {10.2139/ssrn.2699251},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mollick - 2015 - Delivery Rates on Kickstarter.pdf:pdf},
	issn = {1556-5068},
	pages = {7},
	title = {{Delivery Rates on Kickstarter}},
	url = {http://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2699251},
	year = {2015}
}
@article{DeGoeij2015,
	abstract = {Economic crises are complex events that affect behavioral patterns (including alcohol consumption) via opposing mechanisms. With this realist systematic review, we aimed to investigate evidence from studies of previous or ongoing crises on which mechanisms (How?) play a role among which individuals (Whom?). Such evidence would help understand and predict the potential impact of economic crises on alcohol consumption. Medical, psychological, social, and economic databases were used to search for peer-reviewed qualitative or quantitative empirical evidence (published January 1, 1990-May 1, 2014) linking economic crises or stressors with alcohol consumption and alcohol-related health problems. We included 35 papers, based on defined selection criteria. From these papers, we extracted evidence on mechanism(s), determinant, outcome, country-level context, and individual context. We found 16 studies that reported evidence completely covering two behavioral mechanisms by which economic crises can influence alcohol consumption and alcohol-related health problems. The first mechanism suggests that psychological distress triggered by unemployment and income reductions can increase drinking problems. The second mechanism suggests that due to tighter budget constraints, less money is spent on alcoholic beverages. Across many countries, the psychological distress mechanism was observed mainly in men. The tighter budget constraints mechanism seems to play a role in all population subgroups across all countries. For the other three mechanisms (i.e., deterioration in the social situation, fear of losing one's job, and increased non-working time), empirical evidence was scarce or absent, or had small to moderate coverage. This was also the case for important influential contextual factors described in our initial theoretical framework. This realist systematic review suggests that among men (but not among women), the net impact of economic crises will be an increase in harmful drinking. Such a different net impact between men and women could potentially contribute to growing gender-related health inequalities during a crisis.},
	author = {de Goeij, Moniek C M and Suhrcke, Marc and Toffolutti, Veronica and van de Mheen, Dike and Schoenmakers, Tim M. and Kunst, Anton E.},
	doi = {10.1016/j.socscimed.2015.02.025},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Goeij et al. - 2015 - How economic crises affect alcohol consumption and alcohol-related health problems A realist systematic review.pdf:pdf},
	isbn = {1873-5347$\backslash$r0277-9536},
	issn = {18735347},
	journal = {Social Science and Medicine},
	keywords = {Alcohol consumption,Alcohol-related health problems,Economic crisis,Realist systematic review},
	mendeley-groups = {My Research/Zeta},
	pages = {131--146},
	pmid = {25771482},
	publisher = {Elsevier Ltd},
	title = {{How economic crises affect alcohol consumption and alcohol-related health problems: A realist systematic review}},
	url = {http://dx.doi.org/10.1016/j.socscimed.2015.02.025},
	volume = {131},
	year = {2015}
}
@article{Song2015,
	abstract = {During the past decade, penalized likelihood methods have been widely used in variable selection problems, where the penalty functions are typically symmetric about 0, continuous and nondecreasing in (0, ∞). We propose a new penalized likelihood method, reciprocal Lasso (or in short, rLasso), based on a new class of penalty functions which are decreasing in (0, ∞), discontinuous at 0, and converge to infinity when the coefficients approach zero. The new penalty functions give nearly zero coefficients infinity penalties; in contrast, the conventional penalty functions give nearly zero coefficients nearly zero penalties (e.g., Lasso and SCAD) or constant penalties (e.g., L0 penalty). This distinguishing feature makes rLasso very attractive for variable selection: It can effectively avoid to select overly dense models. We establish the consistency of the rLasso for variable selection and coefficient estimation under both the low and high dimensional settings. Since the rLasso penalty functions induce an objective function with multiple local minima, we also propose an efficient Monte Carlo optimization algorithm to solve the involved minimization problem. Our simulation results show that the rLasso outperforms other popular penalized likelihood methods, such as Lasso, SCAD, MCP, SIS, ISIS and EBIC: It can produce sparser and more accurate coefficient estimates, and catch the true model with a higher probability.$\backslash$nDuring the past decade, penalized likelihood methods have been widely used in variable selection problems, where the penalty functions are typically symmetric about 0, continuous and nondecreasing in (0, ∞). We propose a new penalized likelihood method, reciprocal Lasso (or in short, rLasso), based on a new class of penalty functions which are decreasing in (0, ∞), discontinuous at 0, and converge to infinity when the coefficients approach zero. The new penalty functions give nearly zero coefficients infinity penalties; in contrast, the conventional penalty functions give nearly zero coefficients nearly zero penalties (e.g., Lasso and SCAD) or constant penalties (e.g., L0 penalty). This distinguishing feature makes rLasso very attractive for variable selection: It can effectively avoid to select overly dense models. We establish the consistency of the rLasso for variable selection and coefficient estimation under both the low and high dimensional settings. Since the rLasso penalty functions induce an objective function with multiple local minima, we also propose an efficient Monte Carlo optimization algorithm to solve the involved minimization problem. Our simulation results show that the rLasso outperforms other popular penalized likelihood methods, such as Lasso, SCAD, MCP, SIS, ISIS and EBIC: It can produce sparser and more accurate coefficient estimates, and catch the true model with a higher probability.},
	author = {Song, Qifan and Liang, Faming},
	doi = {10.1080/01621459.2014.984812},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Liang - 2015 - High-Dimensional Variable Selection With Reciprocal L1-Regularization.pdf:pdf},
	issn = {1537274X},
	journal = {Journal of the American Statistical Association},
	keywords = {Lasso,Penalized likelihood methods,Reciprocal lasso,Stochastic approximation annealing},
	number = {512},
	pages = {1607--1620},
	title = {{High-Dimensional Variable Selection With Reciprocal L1-Regularization}},
	volume = {110},
	year = {2015}
}
@article{Hale2015,
	author = {Hale, Galina and Philipov, Alexej},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hale, Philipov - 2015 - Is Transition to Inflation Targeting Good for Growth.pdf:pdf},
	journal = {FRBSF ECONOMIC LETTER},
	number = {2012},
	pages = {1--5},
	title = {{Is Transition to Inflation Targeting Good for Growth?}},
	url = {http://www.frbsf.org/economic-research/files/el2015-14.pdf},
	year = {2015}
}
@book{Gelman2015,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1011.1669v3},
	author = {Gelman, Andrew},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {arXiv:1011.1669v3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 2015 - Bayesian Data Analysis.pdf:pdf},
	isbn = {9788578110796},
	issn = {16130073},
	keywords = {Mobile,Named entity disambiguation,Natural language processing,News,Recommender system},
	pages = {1--656},
	pmid = {25246403},
	title = {{Bayesian Data Analysis}},
	year = {2015}
}
@article{Paul2015,
	author = {Paul, J{\'{e}}r{\^{o}}me and Ambrosio, Roberto D and Dupont, Pierre},
	doi = {10.1016/j.neucom.2014.12.098},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Paul, D'Ambrosio, Dupont (2015) Kernel methods for heterogeneous feature selection.pdf:pdf},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Heterogeneous feature selection,Kernel methods,Mixed data,Multiple kernel learning,Recursive feature elimination,Support vector machine,heterogeneous feature selection},
	mendeley-groups = {Model Selection and Averaging/Model Selection},
	pages = {187--195},
	publisher = {Elsevier},
	title = {{Kernel Methods for Heterogeneous Feature Selection}},
	url = {http://dx.doi.org/10.1016/j.neucom.2014.12.098},
	volume = {169},
	year = {2015}
}
@article{Mavroeidis2015,
	author = {Mavroeidis, Sophocles and Sasaki, Yuya and Welch, Ivo},
	doi = {10.1016/j.jeconom.2015.05.001},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Mavroeidis, Sasaki, Welch (2015) Estimation of heterogeneous autoregressive parameters with short panel data..pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {219--235},
	publisher = {Elsevier B.V.},
	title = {{Estimation of heterogeneous autoregressive parameters with short panel data}},
	url = {http://dx.doi.org/10.1016/j.jeconom.2015.05.001},
	volume = {188},
	year = {2015}
}
@article{Bonhomme2015,
	abstract = {This paper introduces time-varying grouped patterns of heterogeneity in linear panel data models. A distinctive feature of our approach is that group membership is left unrestricted. We estimate the parameters of the model using a “grouped fixed-effects” estimator that minimizes a least squares criterion with respect to all possible groupings of the cross-sectional units. Recent advances in the clustering literature allow for fast and efficient computation. We provide conditions under which our estimator is consistent as both dimensions of the panel tend to infinity, and we develop inference methods. Finally, we allow for grouped patterns of unobserved heterogeneity in the study of the link between income and democracy across countries.},
	author = {Bonhomme, St{\'{e}}phane and Manresa, Elena},
	doi = {10.3982/ecta11319},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Bonhomme, Manresa (2015) Grouped Patterns of Heterogeneity in Panel Data.pdf:pdf},
	issn = {1468-0262},
	journal = {Econometrica},
	keywords = {democracy,discrete heterogeneity,fixed effects,panel data},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {3},
	pages = {1147--1184},
	title = {{Grouped Patterns of Heterogeneity in Panel Data}},
	volume = {83},
	year = {2015}
}
@article{Lee2015,
	abstract = {This paper considers model selection in panels where incidental parameters are present. Primary interest centers on selecting a model that best approximates the underlying structure involving parameters that are common within the panel. It is well known that conventional model selection procedures are often inconsistent in panel models and this can be so even without nuisance parameters. Modifications are then needed to achieve consistency. New model selection information criteria are developed here that use either the Kullback-Leibler information criterion based on the profile likelihood or the Bayes factor based on the integrated likelihood with a bias-reducing prior. These model selection criteria impose heavier penalties than those associated with standard information criteria such as AIC and BIC. The additional penalty, which is data-dependent, properly reflects the model complexity arising from the presence of incidental parameters. A particular example is studied in detail involving lag order selection in dynamic panel models with fixed effects. The new criteria are shown to control for over/under-selection probabilities in these models and lead to consistent order selection criteria.},
	author = {Lee, Yoonseok and Phillips, Peter C.B.},
	doi = {10.1016/j.jeconom.2015.03.012},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Lee, Phillips (2015) Model selection in the presence of incidental parameters.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {C52,JEL classification C23},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {474--489},
	title = {{Model selection in the presence of incidental parameters}},
	volume = {188},
	year = {2015}
}
@article{Grant2015,
	author = {Grant, Andrew and Razdan, Rohit and Shang, Thongchie},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grant, Razdan, Shang - 2015 - Coordinates for Change How GIS Technology and Geospatial Analytics Can Improve City Services.pdf:pdf},
	journal = {McKinsey Government Designed for New Times},
	mendeley-groups = {My Research/Eta},
	pages = {32--43},
	title = {{Coordinates for Change : How GIS Technology and Geospatial Analytics Can Improve City Services}},
	year = {2015}
} 
 
@article{Ackerberg2015IdentificationPropertiesRecent,
  title = {Identification {{Properties}} of {{Recent Production Function Estimators}}},
  author = {Ackerberg, Daniel A. and Caves, Kevin and Frazer, Garth},
  year = {2015},
  journal = {Econometrica},
  volume = {83},
  number = {6},
  pages = {2411--2451},
  issn = {0012-9682},
  doi = {10.3982/ecta13408},
  abstract = {This paper examines some of the recent literature on the estimation of production functions. We focus on techniques suggested in two recent papers, Olley and Pakes (1996) and Levinsohn and Petrin (2003). While there are some solid and intuitive iden-tification ideas in these papers, we argue that the techniques can suffer from functional dependence problems. We suggest an alternative approach that is based on the ideas in these papers, but does not suffer from the functional dependence problems and pro-duces consistent estimates under alternative data generating processes for which the original procedures do not.},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Econometrics)\Production Function Estimation\Ackerberg et al. - 2015 - Identification Properties of Recent Production Function Estimators.pdf}
}


@inproceedings{Wang2015,
	author = {Wang, Dakuo and Olson, Judith S and Zhang, Jingwen and Nguyen, Trung and Olson, Gary M},
	booktitle = {iConference 2015 Proceedings},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2015 - How Students Collaboratively Write using Google Docs.pdf:pdf},
	keywords = {2015,citation,collaboration,college student,copyright,copyright is held by,d,g,google docs,how students collaboratively write,in iconference 2015 proceedings,j,m,nguyen,olson,s,t,the author,using google docs,visualization,wang,writing,zhang},
	pages = {1--5},
	publisher = {iSchools},
	title = {{How Students Collaboratively Write using Google Docs}},
	url = {http://hdl.handle.net/2142/73736},
	year = {2015}
}
@article{Hughes2015,
	abstract = {Abstract Electric utilities, local and state governments utilize a variety of subsidies to promote energy efficiency and renewable energy. We study the California Solar Initiative and find that upfront rebates have a large effect on residential solar installations. We exploit variation in rebate rates across electric utilities over time and control for time-varying factors that affect PV adoption. Our preferred estimates suggest increasing average rebates from {\$}5,600 to {\$}6,070 would increase installations by 13 percent. Overall, we predict 58 percent fewer installations would have oc- curred without subsidies. Over 20 years, we estimate these additional installations reduce carbon dioxide emissions between 2.98 and 3.7 million metric tons and local air pollutants (NOx) by 1,100 to 1,900 metric tons, about as much as is produced by a small to mid-sized natural gas power plant. Of the {\$}437 million in rebates awarded, {\$}98 million were rents to installations that would have taken place absent rebates. Back of the envelope calculations suggest program costs of {\$}0.05 per kilowatt hour and between {\$}139 and {\$}147 per metric ton of carbon dioxide. ∗The},
	author = {Hughes, Jonathan E. and Podolefsky, Molly},
	doi = {10.1086/681131},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hughes, Podolefsky - 2015 - Getting Green with Solar Subsidies Evidence from the California Solar Initiative.pdf:pdf},
	issn = {23335955},
	journal = {Journal of the Association of Environmental and Resource Economists},
	keywords = {also adopted policies to,and emissions of greenhouse,electric utilities have,energy,fi ciency and renewable,gases,involved in efforts to,l governments have become,many state and loca,promote residential energy ef,reduce local air pollution,renewable energy,solar,subsidies},
	mendeley-groups = {My Research/Eta},
	number = {2},
	pages = {235--275},
	title = {{Getting Green with Solar Subsidies: Evidence from the California Solar Initiative}},
	volume = {2},
	year = {2015}
}
@book{Joe2015,
	abstract = {Dependence Modeling with Copulas},
	author = {Joe, Harry},
	booktitle = {CRC Press},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joe - 2015 - Dependence Modeling with Copulas.pdf:pdf},
	isbn = {9781466583221},
	pages = {479},
	title = {{Dependence Modeling with Copulas}},
	year = {2015}
}
@article{Liu2015,
	abstract = {The stock markets play a key role in both developing and advanced countries because it channelize idle money into productive investment and generate capital for businesses which boosts the economy up. Pakistan's stock market is an emerging stock market. The main objective of the study is to check the contribution of macroeconomic indicator to the stock market development. In this study, an attempt is made to capture the macroeconomic determinants that effect more or less in stock market development. Karachi Stock Exchange (KSE) is taken as a representative stock exchange of Pakistan. This study considered Gross Domestic Saving (GDS), Money Supply (MS) and Foreign Remittances (FR) as explanatory variables and stock market development (SMD) takes as dependent variable. The study employed Phillips and Perron (PP) test for Stationarity. Finally the study utilized the ARDL to co-integration approach because it is more dominant and robust procedure to examine the short run and long run dynamic relationship. Autoregressive distributed lag (ARDL) and Error Correction Model used to find the relationship between the variables of selected econometric model. The ARDL to Co-integration results showed that Gross domestic savings ,money supply positively contribute to the development of stock market in Pakistan in both short run and long run that are consistent with theoretical and conceptual framework and literature (See also; Raza et al., 2012; Adam and Tweneboah, 2009). Foreign remittances have insignificant effect in both short run and long run on stock market because most of the foreign remittances are used in consumption. CUSUM lines remained inside the critical bound at 5 percent significance level that guaranteed the stability of model.},
	archivePrefix = {arXiv},
	arxivId = {suresh govindarajan},
	author = {Liu, Chu-An},
	doi = {10.1227/01.NEU.0000349921.14519.2A},
	eprint = {suresh govindarajan},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Liu (2015) Distribution Theory of the Least Squares Averaging Estimator.pdf:pdf},
	isbn = {2007510134049},
	issn = {0148-396X},
	journal = {Journal of Econometrics},
	mendeley-groups = {Model Selection and Averaging},
	number = {1},
	pages = {142--159},
	pmid = {17411427},
	title = {{Distribution Theory of the Least Squares Averaging Estimator}},
	volume = {186},
	year = {2015}
}
@article{Annan2015,
	abstract = {Despite significant progress in average yields, the sensitivity of corn and soybean yields to extreme heat has remained relatively constant over time. We combine county-level corn and soybeans yields in the United States from 1989-2013 with the fraction of the planting area that is insured under the federal crop insurance program, which expanded greatly over this time period as premium subsidies increased from 20 percent to 60 percent. Insured corn and soybeans are significantly more sensitive to extreme heat that uninsured crops. Insured farmers do not have the incentive to engage in costly adaptation as insurance compensates them for potential losses. [ABSTRACT FROM AUTHOR]},
	author = {Annan, Francis and Schlenker, Wolfram},
	doi = {10.1257/aer.p20151031},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Annan, Schlenker - 2015 - Federal Crop Insurance and the Disincentive to Adapt to Extreme Heat.pdf:pdf},
	issn = {0002-8282},
	journal = {American Economic Review: Papers {\&} Proceedings},
	mendeley-groups = {My Research/Theta/Review},
	number = {5},
	pages = {262--266},
	title = {{Federal Crop Insurance and the Disincentive to Adapt to Extreme Heat}},
	volume = {105},
	year = {2015}
}
@article{Halevy2015,
	abstract = {A sequence of experiments documents static and dynamic “preference reversals” between sooner-smaller and later-larger rewards, when the sooner reward could be immediate. The theoretically motivated design permits separate identification of time consistent, stationary,and time invariant choices. At least half of the subjects are time consistent, but only three-quarters of them exhibit stationary choices. About half of subjects with time inconsistent choices have stationary preferences. These results challenge the view that present-bias preferences are the main source of time inconsistent choices.},
	author = {Halevy, Yoram},
	doi = {10.3982/ECTA10872},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Halevy - 2015 - Time Consistency Stationarity and Time Invariance.pdf:pdf},
	isbn = {00129682},
	issn = {00129682},
	journal = {Econometrica},
	keywords = {Time discounting, diminishing impatiences, interte},
	number = {1},
	pages = {335--352},
	title = {{Time Consistency: Stationarity and Time Invariance}},
	url = {http://doi.wiley.com/10.3982/ECTA10872},
	volume = {83},
	year = {2015}
}
@book{Su2015,
	abstract = {We propose quasi maximum likelihood (QML) estimation of dynamic panel models with spatial errors when the cross-sectional dimension n is large and the time dimension T is fixed. We consider both the random effects and fixed effects models, and prove consistency and derive the limiting distributions of the QML estimators under different assumptions on the initial observations. We propose a residual-based bootstrap method for estimating the standard errors of theQMLestimators. Monte Carlo simulation shows that both the QML estimators and the bootstrap standard errors perform well in finite samples under a correct assumption on initial observations, but may perform poorly when this assumption is not met.},
	author = {Su, Liangjun and Yang, Zhenlin},
	booktitle = {Journal of Econometrics},
	doi = {10.1016/j.jeconom.2014.11.002},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su, Yang - 2015 - QML estimation of dynamic panel data models with spatial errors.pdf:pdf},
	isbn = {6568280852},
	issn = {18726895},
	keywords = {Bootstrap standard errors,Dynamic panel,Fixed effects,Initial observations,Quasi maximum likelihood,Random effects,Spatial error dependence},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {1},
	pages = {230--258},
	publisher = {Elsevier B.V.},
	title = {{QML estimation of dynamic panel data models with spatial errors}},
	url = {http://dx.doi.org/10.1016/j.jeconom.2014.11.002},
	volume = {185},
	year = {2015}
}

@unpublished{Correia2016FeasibleEstimatorLinear,
  title = {A {{Feasible Estimator}} for {{Linear Models}} with {{Multi-Way Fixed Effects}}},
  author = {Correia, Sergio},
  year = {2016},
  abstract = {I propose a feasible and computationally efficient estimator of linear models with multiple levels of fixed effects. First, I show that solving the two--way fixed effects model is equivalent to solving a linear system on a weighted graph, and apply recent advances in spectral graph theory to obtain a nearly--linear time estimator (Kelner et al, 2013). Second, I embed this estimator into an improved version of the generalized within--estimator of Guimar{\~a}es and Portugal (2010) and Gaure (2013), replacing their projections with symmetric ones amenable to conjugate gradient acceleration, guaranteeing monotonic convergence. The proposed estimator has the fastest known asymptotic running time, and performs particularly well with large datasets and high--dimensional fixed effects. Morever, by combining insights from graph theory, it opens the door to further improvements on the estimation and inference of models with multi--way fixed effects.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Panel Data\High-Dimensional Panels\Correia - A Feasible Estimator for Linear Models with Multi-Way Fixed Effects.pdf}
}

@article{Koster2015,
	abstract = {Abstract The bi-directional influences between emotion and food consumption are discussed in view of recent efforts to find emotional factors that influence food choice and eating- and drinking behaviour independently from traditional factors as liking, wanting and appropriateness. Distinctions are made between conscious and unconscious emotions and their relative importance in food-related behaviour is discussed. In response to eating disorders like obesity, much more is known about the influence of emotion and mood on food choice and intake than about the influence of food on mood and emotion, which only recently gained prominence in food-related emotion research. This led to a number of emotion measurement methods that differ strongly in their explicit or implicit measurement approach and in the extent to which they demand conscious emotion awareness and verbal understanding on the part of the participants. These methods are critically discussed and questions are raised about the specificity of their emotional contents and about their use at different moments in time, such as before, during and at different moments after consumption. Furthermore, doubts were raised about the independency of their contributions from the traditional measurements (liking, wanting and appropriateness) and suggestions are made for improving the practical applicability of an efficient emotion measurement.},
	author = {K{\"{o}}ster, Egon P and Mojet, Jozina},
	doi = {http://dx.doi.org/10.1016/j.foodres.2015.04.006},
	issn = {0963-9969},
	journal = {Food Research International},
	keywords = {Eating and drinking behaviour,Emotion,Emotion measurement,Explicit vs. implicit methods},
	month = {oct},
	pages = {180--191},
	title = {{From Mood to Food and from Food to Mood: A psychological perspective on the measurement of food-related emotions in consumer research}},
	url = {http://www.sciencedirect.com/science/article/pii/S096399691500157X},
	volume = {76, Part 2},
	year = {2015}
}

@article{Borgschulte2024AirPollutionLabor,
  title = {Air {{Pollution}} and the {{Labor Market}}: {{Evidence}} from {{Wildfire Smoke}}},
  shorttitle = {Air {{Pollution}} and the {{Labor Market}}},
  author = {Borgschulte, Mark and Molitor, David and Zou, Eric Yongchen},
  year = {2024},
  month = nov,
  journal = {Review of Economics and Statistics},
  volume = {106},
  number = {6},
  pages = {1558--1575},
  issn = {0034-6535, 1530-9142},
  doi = {10.1162/rest_a_01243},
  urldate = {2025-05-12},
  abstract = {We study how air pollution impacts the U.S. labor market by analyzing the effects of drifting wildfire smoke. We link satellite-based smoke plume data with labor market outcomes to estimate that an additional day of smoke exposure reduces quarterly earnings by about 0.1\%. Extensive margin responses, including employment reductions and labor force exits, explain 13\% of the overall earnings losses. The implied welfare costs from lost earnings due to air pollution exposure is on par with standard valuations of the mortality burden. The findings highlight the importance of labor market channels in air pollution policy responses.},
  langid = {english},
  file = {C:\Users\moren\My Drive\Econometrics Cloud\Articles\Applied (By Field)\Enviro\Borgschulte et al. - 2024 - Air Pollution and the Labor Market Evidence from Wildfire Smoke.pdf}
}

@article{Chung2015,
	abstract = {The price of photovoltaic (PV) systems in the United States (i.e., the cost to the system owner) has continued to decline across all major market sectors. This report provides a Q1 2015 update regarding the prices of residential, commercial, and utility scale PV systems, based on an objective methodology that closely approximates the book value of a PV system. Several cases are benchmarked to represent common variations in business models, labor rates, and system architecture choice. We estimate a weighted-average cash purchase price of {\$}3.09/W for residential scale rooftop systems, {\$}2.15/W for commercial scale rooftop systems, {\$}1.77/W for utility scale systems with fixed mounting structures, and {\$}1.91/W for utility scale systems using single-axis trackers. All systems are modeled assuming standard-efficiency, polycrystalline-silicon PV modules, and further assume installation within the United States.},
	author = {Chung, Donald and Davidson, Carolyn and Fu, Ran and Ardani, Kristen and Margolis, Robert},
	doi = {NREL/TP-6A20-64746},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung et al. - 2015 - U.S. Photovoltaic Prices and Cost Breakdowns Q1 2015 Benchmarks for Residential , Commercial , and Utility-Scale.pdf:pdf},
	journal = {NREL Technical Report},
	keywords = {Commercial Rooftop,Costs,Customer Acquisition,Ground Mount,Installation,Installed,Installed Price,NREL/TP-6A20-64746,PV,Photovoltaics,Price,Residential Rooftop,September 2015,Silicon PV,Soft Costs,Solar,Solar Farm,Solar System,Tracker,Tracking},
	mendeley-groups = {My Research/Eta},
	number = {September},
	pages = {42},
	title = {{U.S. Photovoltaic Prices and Cost Breakdowns : Q1 2015 Benchmarks for Residential , Commercial , and Utility-Scale Systems}},
	url = {http://www.nrel.gov/docs/fy15osti/64746.pdf},
	year = {2015}
}
@article{Paudyal2015,
	abstract = {Abstract Community-managed forests (CMF) provide vital ecosystem services (ES) for local communities. However, the status and trend of ES in CMF have not been assessed in many developing countries because of a lack of appropriate data, tools, appropriate policy or management framework. Using a case study of community-managed forested landscape in central Nepal, this paper aims to identify and map priority ES and assess the temporal change in the provision of ES between 1990 and 2013. Semi-structured interviews, focus group discussions, transect walks and participatory mapping were used to identify and assess priority ES. The results indicated that community forestry has resulted in the substantial restoration of forests on degraded lands over the period of 1990–2013. Local community members and experts consider that this restoration has resulted in a positive impact on various ES beneficial for local, regional, national and international users. Priority ES identified in the study included timber, firewood, freshwater, carbon sequestration, water regulation, soil protection, landscape beauty as well as biodiversity. There were strong variations in the valuation of different ES between local people and experts, between genders and between different status and income classes in the local communities. In general, whereas CMF provide considerable benefits at larger scales, local people have yet to perceive the real value of these different ES provided by their forest management efforts. The study demonstrated that participatory tools, combined with free-access satellite images and repeat photography are suitable approaches to engage local communities in discussions regarding ES and to map and prioritise ES values.},
	author = {Paudyal, Kiran and Baral, Himlal and Burkhard, Benjamin and Bhandari, Santosh P and Keenan, Rodney J},
	doi = {http://dx.doi.org/10.1016/j.ecoser.2015.01.007},
	issn = {2212-0416},
	journal = {Ecosystem Services},
	keywords = {Developing countries,Ecosystem services assessment,Expert opinion,Local knowledge,Participatory tools,Repeat photography},
	month = {jun},
	pages = {81--92},
	title = {{Participatory assessment and mapping of ecosystem services in a data-poor region: Case study of community-managed forests in central Nepal}},
	url = {http://www.sciencedirect.com/science/article/pii/S221204161500008X},
	volume = {13},
	year = {2015}
}
@article{Eo2015,
	abstract = {We propose the use of likelihood-based confidence sets for the timing of structural breaks in parameters from time series regression models. The confidence sets are valid for the broad setting of a system of multivariate linear regression equations under fairly general assumptions about the error and regressors and allowing for multiple breaks in mean and variance parameters. In our asymptotic analysis, we determine the critical values for a likelihood ratio test of a break date and the expected length of a likelihood-based confidence set constructed by inverting the likelihood ratio test. Notably, the likelihood-based confidence set is considerably shorter than for other methods employed in the literature. Monte Carlo analysis confirms better performance than other methods in terms of length and coverage accuracy in finite samples, including when the magnitude of breaks is small. An application to postwar U.S. real GDP and consumption leads to a much tighter 95{\%} confidence set for the timing of the “Great Moderation” in the mid-1980s than previously found. Furthermore, when taking cointegration between output and consumption into account, confidence sets for structural break dates are even more precise and suggest a sudden “productivity growth slowdown” in the early 1970s and an additional large, abrupt decline in long-run growth in the mid-1990s.},
	author = {Eo, Yunjong and Morley, James},
	doi = {10.2139/ssrn.2264009},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eo, Morley - 2015 - Likelihood-Based Confidence Sets for the Timing of Structural Breaks.pdf:pdf},
	journal = {Quantitative Economics},
	keywords = {C22,C32,E20,Great Moderation,Inverted Likelihood Ratio Confidence Sets,Multiple Breaks,Productivity Growth Slowdown},
	mendeley-groups = {My Research/LR-Based Confidence Sets},
	pages = {463--497},
	title = {{Likelihood-Based Confidence Sets for the Timing of Structural Breaks}},
	volume = {6},
	year = {2015}
}
@article{Moon2015,
	abstract = {In this paper we study the least squares (LS) estimator in a linear panel regression model with unknown number of factors appearing as interactive fixed effects. Assuming that the number of factors used in estimation is larger than the true number of factors in the data we establish the limiting distribution of the LS estimator for the regression coefficients, as the number of time periods and the number of crosssectional units jointly go to infinity. The main result of the paper is that under certain assumptions the limiting distribution of the LS estimator is independent of the number of factors used in the estimation, as long as this number is not underestimated. The important practical implication of this result is that for inference on the regression coefficients one does not necessarily need to estimate the number of interactive fixed effects consistently.},
	author = {Moon, Hyungsik Roger and Weidner, Martin},
	doi = {10.2139/ssrn.2546114},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Moon (2015) Unknown Number of Factors.pdf:pdf},
	isbn = {1468-0262},
	issn = {0012-9682},
	journal = {Econometrica},
	keywords = {C23,C33,Panel data,factor models,interactive fixed effects,perturbation theory of linear operators,random matrix theory},
	mendeley-groups = {Panel Data/Factor Models and Interactive Effects},
	number = {4},
	pages = {1543--1579},
	title = {{Linear Regression for Panel with Unknown Number of Factors as Interactive Fixed Effects}},
	volume = {83},
	year = {2015}
}
@article{Coady2015,
	abstract = {This paper provides a comprehensive, updated picture of energy subsidies at the global and regional levels. It focuses on the broad notion of post-tax energy subsidies, which arise when consumer prices are below supply costs plus a tax to reflect environmental damage and an additional tax applied to all consumption goods to raise government revenues. Post-tax energy subsidies are dramatically higher than previously estimated and are projected to remain high. These subsidies primarily reflect underpricing from a domestic (rather than global) perspective, so that unilateral price reform is in countries' own interests. The potential fiscal, environmental, and welfare impacts of energy subsidy reform are substantial.},
	author = {Coady, David and Parry, Ian and Sears, Louis and Shang, Baoping},
	doi = {10.5089/9781513532196.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coady et al. - 2015 - How large are global energy subsidies.pdf:pdf},
	isbn = {{\textless}null{\textgreater}},
	issn = {1018-5941},
	journal = {International Monetary Fund},
	keywords = {deadweight,efficient taxation,energy subsidies,environment,loss,revenue},
	number = {105},
	pages = {30},
	title = {{How large are global energy subsidies?}},
	url = {https://www.imf.org/external/pubs/ft/wp/2015/wp15105.pdf},
	volume = {Fiscal Aff},
	year = {2015}
}
@article{Dissanayake2015,
	abstract = {A significant portion of the world's forests that are eligible for Reducing Emission from Deforestation and Forest Degradation, known as REDD , payments are community managed forests. However, there is little knowledge about preferences of households living in community managed forests for REDD contracts, or the opportunity costs of accepting REDD contracts for these communities. This paper uses a choice experiment survey of rural communities in Nepal to understand respondents' preferences toward the institutional structure of REDD contracts. The sample is split across communities with community managed forests groups and those without community managed forest groups to see how prior involvement in community managed forest groups affects preferences. The results show that respondents care about how the payments are divided between households and communities, the severity of restrictions on firewood use, the restrictions on grazing, and the fairness of access to community managed forest resources as well as the level of payments. The preferences for REDD contracts are in general similar between community managed and non-community managed forest resource respondents, but there are differences, in particular with regard to how beliefs influence the likelihood of accepting the contracts. Finally, the paper finds that the opportunity cost of REDD payments, although cheaper than many other carbon dioxide abatement options, is higher than previously suggested in the literature.},
	author = {Dissanayake, Sahan T. M. and Jha, Prakash and Adhikari, Bhim and Bista, Rajesh and Bluffstone, Randall a. and Luintel, Harisharan and Martinsson, Peter and Paudel, Naya S. and Somanathan, Eswaran and Toman, Michael},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dissanayake et al. - 2015 - Community Managed Forest Groups and Preferences for REDD Contract Attributes A Choice Experiment Survey of C.pdf:pdf},
	keywords = {Climate Change Mitigation and Green House Gases,Climate Change and Environment,Environmental Economics {\&} Policies,Forestry Management,Wildlife Resources},
	number = {7326},
	title = {{Community Managed Forest Groups and Preferences for REDD Contract Attributes: A Choice Experiment Survey of Communities in Nepal}},
	url = {http://papers.ssrn.com/abstract=2621880},
	year = {2015}
}
@article{Gagnon2016,
	abstract = {How much energy could be generated if PV modules were installed on all of the suitable roof area in the nation? To answer this question, we first use GIS methods to process a lidar dataset and determine the amount of roof area that is suitable for PV deployment in 128 cities nationwide, containing 23{\%} of U.S. buildings, and provide PV-generation results for a subset of those cities. We then extend the insights from that analysis to the entire continental United States. We develop two statistical models--one for small buildings and one for medium and large buildings--and populate them with geographic variables that correlate with rooftop's suitability for PV. We simulate the productivity of PV installed on the suitable roof area, and present the technical potential of PV on both small buildings and medium/large buildings for every state in the continental US. Within the 128 cities covered by lidar data, 83{\%} of small buildings have a location suitable for a PV installation, but only 26{\%} of the total rooftop area of small buildings is suitable for development. The sheer number of buildings in this class, however, gives small buildings the greatest technical potential. Small building rooftops could accommodate 731 GW of PV capacity and generate 926 TWh/year of PV energy, approximately 65{\%} of rooftop PV's total technical potential. We conclude by summing the PV-generation results for all building sizes and therefore answering our original question, estimating that the total national technical potential of rooftop PV is 1,118 GW of installed capacity and 1,432 TWh of annual energy generation. This equates to 39{\%} of total national electric-sector sales.},
	author = {Gagnon, Pieter and Margolis, Robert and Melius, Jennifer and Phillips, Caleb and Elmore, Ryan},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gagnon et al. - 2016 - Rooftop Solar Photovolatic Technical Potential in the United States A Detailed Assessment.pdf:pdf},
	journal = {NREL},
	keywords = {GIS,January 2016,LiDAR,NREL/TP-6A20-65298,PV,Photovoltaic,Rooftop,Technical Potential,city,energy generation,installed capacity,large buildings,medium buildings,nation,small buildings,state},
	mendeley-groups = {My Research/Eta},
	number = {January},
	pages = {82},
	title = {{Rooftop Solar Photovolatic Technical Potential in the United States: A Detailed Assessment}},
	url = {http://www.nrel.gov/docs/fy16osti/65298.pdf},
	year = {2016}
}
@article{Cannon2016,
	abstract = {Abstract We show that tests for adverse selection in annuity markets using prices are not identified. Within the UK annuity market, different annuity products create the potential for a Rothschild-Stiglitz separating equilibrium as different risk types could choose different annuities. Empirical analyses using the “money's worth” suggest that prices are indeed consistent with this explanation. However, we show that this pattern of annuity prices would also result from the actions of regulated annuity providers who must reserve against cohort mortality risk. Annuity products that might attract different consumer risk types also have different risks for the provider.},
	author = {Cannon, Edmund and Tonks, Ian},
	doi = {http://dx.doi.org/10.1016/j.jpubeco.2016.07.002},
	issn = {0047-2727},
	journal = {Journal of Public Economics},
	keywords = {Adverse selection,Annuities,Insurance markets},
	month = {sep},
	pages = {68--81},
	title = {{Cohort mortality risk or adverse selection in annuity markets?}},
	url = {http://www.sciencedirect.com/science/article/pii/S0047272716300755},
	volume = {141},
	year = {2016}
}
@article{Gao2016,
	abstract = {This paper develops a frequentist model averaging method based on the leave-subject-out cross-validation. This method is applicable not only to averaging longitudinal data models, but also to averaging time series models which can have heteroscedastic errors. The resulting model averaging estimators are proved to be asymptotically optimal in the sense of achieving the lowest possible squared errors. Both simulation study and empirical example show the superiority of the proposed estimators over their competitors.},
	author = {Gao, Yan and Zhang, Xinyu and Wang, Shouyang and Zou, Guohua},
	doi = {10.1016/j.jeconom.2015.07.006},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Gao, Zhang, Wang,Zou (2016) Model averaging based on leave-subject-out cross-validation.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Asymptotic optimality,Leave-subject-out cross-validation,Longitudinal data,Model averaging,Time series},
	mendeley-groups = {Model Selection and Averaging,Model Selection and Averaging/Panel},
	number = {1},
	pages = {139--151},
	publisher = {Elsevier B.V.},
	title = {{Model averaging based on leave-subject-out cross-validation}}, 
	volume = {192},
	year = {2016}
}
@article{Belloni2016,
	abstract = {We consider estimation and inference in panel data models with additive unobserved individual specific heterogeneity in a high-dimensional setting. The setting allows the number of time-varying regressors to be larger than the sample size. To make informative estimation and inference feasible, we require that the overall contribution of the time-varying variables after eliminating the individual specific heterogeneity can be captured by a relatively small number of the available variables whose identities are unknown. This restriction allows the problem of estimation to proceed as a variable selection problem. Importantly, we treat the individual specific heterogeneity as fixed effects which allows this heterogeneity to be related to the observed time-varying variables in an unspecified way and allows that this heterogeneity may differ for all individuals. Within this framework, we provide procedures that give uniformly valid inference over a fixed subset of parameters in the canonical linear fixed effects model and over coefficients on a fixed vector of endogenous variables in panel data instrumental variable models with fixed effects and many instruments. We present simulation results in support of the theoretical developments and illustrate the use of the methods in an application aimed at estimating the effect of gun prevalence on crime rates.},
	archivePrefix = {arXiv},
	arxivId = {1411.6507},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian and Kozbur, Damian},
	doi = {10.1080/07350015.2015.1102733},
	eprint = {1411.6507},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/High-Dimensional Panels/Belloni, Chernozhukov, Hansen, Kozbur (2016) Inference in High-Dimensional Panel Models With an Application to Gun Control.pdf:pdf},
	issn = {15372707},
	journal = {Journal of Business and Economic Statistics},
	keywords = {Clustered standard errors,Fixed effects,High-dimensional-sparse regression,Inference under imperfect model selection,Instrumental variables,Panel data,Partially linear model,Uniformly valid inference after model selection},
	mendeley-groups = {Panel Data/High-Dimensional Panels},
	number = {4},
	pages = {590--605},
	title = {{Inference in High-Dimensional Panel Models With an Application to Gun Control}},
	volume = {34},
	year = {2016}
}
@article{Imrohoroglu2016,
	abstract = {Japan is aging and has the highest government debt-to-output ratio among advanced economies. In this article, we build amicro data-based, large-scale overlapping generations model for Japan in which individuals differ in age, gender, employment type, income, and asset holdings, and incorporate the Japanese pension rules.Using existing pension law, current fiscal policy, and medium variants of demographic projections, we produce future paths for government expenditures and tax revenues, with implications for government debt and the public pension fund. Additional pension reform, a higher consumption tax, and higher female labor force participation help achieve fiscal stability.},
	author = {Imrohoroğlu, Selahattin and Kitao, Sagiri and Yamada, Tomoaki},
	doi = {10.1111/iere.12150},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Imrohoroğlu, Kitao, Yamada - 2016 - Achieving fiscal balance in Japan.pdf:pdf},
	isbn = {00206598},
	issn = {14682354},
	journal = {International Economic Review},
	mendeley-groups = {My Research/Pensions},
	number = {1},
	pages = {117--154},
	title = {{Achieving fiscal balance in Japan}},
	volume = {57},
	year = {2016}
}
@article{Hedengren2016,
	author = {Hedengren, David and Stratmann, Thomas},
	doi = {10.1111/ecin.12212},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hedengren, Stratmann - 2016 - Is There Adverse Selection in Life Insurance Markets.pdf:pdf},
	isbn = {7039931133},
	issn = {00952583},
	journal = {Economic Inquiry},
	number = {1},
	pages = {450--463},
	title = {{Is There Adverse Selection in Life Insurance Markets?}},
	url = {http://doi.wiley.com/10.1111/ecin.12212},
	volume = {54},
	year = {2016}
}
@article{Lecue2016,
	author = {Lecu{\'{e}}, Guillaume and Mendelson, Shahar},
	doi = {10.3150/15-BEJ701},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecu{\'{e}}, Mendelson - 2016 - Performance of empirical risk minimization in linear aggregation.pdf:pdf},
	journal = {Bernoulli},
	keywords = {1,aggregation theory,and put y to,be a probability space,be an,distributed according to $\mu$,empirical processes,empirical risk minimization,introduction and main results,learning theory,let,set x to be,unknown target random variable,x,$\mu$},
	mendeley-groups = {UPF/Research Seminar},
	number = {3},
	pages = {1520--1534},
	title = {{Performance of empirical risk minimization in linear aggregation}},
	volume = {22},
	year = {2016}
}
@article{Lee2016,
	author = {Lee, Lung-fei and Yu, Jihai},
	doi = {10.1002/jae},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yu - 2016 - Identification of Spatial Durbin Panel Models.pdf:pdf},
	journal = {Journal of Applied Econometrics},
	keywords = {10.1002/jae.2450},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {1},
	pages = {133--162},
	title = {{Identification of Spatial Durbin Panel Models}},
	volume = {31},
	year = {2016}
}
@article{Baltagi2016,
	author = {Baltagi, Badi H. and Feng, Qu and Kao, Chihwa},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Baltagi, Feng, Kao (2016) Estimation of Heterogeneous Panels with Structural Breaks.pdf:pdf},
	journal = {Journal of Econometrics},
	keywords = {c23,c33,cation,common correlated e,cross-sectional dependence,ects,han hong,heterogeneous panels,jel classi,structural breaks,the associate editor and,the authors would like,three anonymous referees,to thank the editor},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {176--195},
	title = {{Estimation of Heterogeneous Panels with Structural Breaks}},
	volume = {191},
	year = {2016}
}
@article{Bester2016,
	author = {Bester, C Alan and Hansen, Christian B},
	doi = {10.1016/j.jeconom.2012.08.022},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Bester, Hansen (2016) Grouped effects estimators in fixed effects models.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {197--208},
	publisher = {Elsevier B.V.},
	title = {{Grouped Effects Estimators in Fixed Effects Models}},
	url = {http://dx.doi.org/10.1016/j.jeconom.2012.08.022},
	volume = {190},
	year = {2016}
}
@article{Asgeirsdottir2016,
	abstract = {This study uses individual-level longitudinal data from Iceland, a country that experienced a severe economic crisis in 2008 and substantial recovery by 2012, to investigate the extent to which the effects of a recession on health behaviors are lingering or short-lived and to explore trajectories in health behaviors from pre-crisis boom, to crisis, to recovery. Health-compromising behaviors (smoking, heavy drinking, sugared soft drinks, sweets, fast food, and tanning) declined during the crisis, and all but sweets continued to decline during the recovery. Health-promoting behaviors (consumption of fruit, fish oil, and vitamins/minerals and getting recommended sleep) followed more idiosyncratic paths. Overall, most behaviors reverted back to their pre-crisis levels or trends during the recovery, and these short-term deviations in trajectories were probably too short-lived in this recession to have major impacts on health or mortality. A notable exception is for binge drinking, which declined by 10{\%} during the 2 crisis years, continued to fall (at a slower rate of 8{\%}) during the 3 recovery years, and did not revert back to the upward pre-crisis trend during our observation period. These lingering effects, which directionally run counter to the pre-crisis upward trend in consumption and do not reflect price increases during the recovery period, suggest that alcohol is a potential pathway by which recessions improve health and/or reduce mortality.},
	author = {{\'{A}}sgeirsd{\'{o}}ttir, Tinna Laufey and Corman, Hope and Noonan, Kelly and Reichman, Nancy E.},
	doi = {10.1016/j.ehb.2015.11.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\'{A}}sgeirsd{\'{o}}ttir et al. - 2016 - Lifecycle effects of a recession on health behaviors Boom, bust, and recovery in Iceland.pdf:pdf},
	issn = {18736130},
	journal = {Economics and Human Biology},
	keywords = {Economic crisis,Economic recovery,Health behaviors,Iceland,Recessions},
	mendeley-groups = {My Research/Zeta},
	pages = {90--107},
	pmid = {26687768},
	title = {{Lifecycle effects of a recession on health behaviors: Boom, bust, and recovery in Iceland}},
	volume = {20},
	year = {2016}
}
@article{Ker2016,
	abstract = {The Agricultural Act of 2014 solidified insurance as the cornerstone of U.S. agricultural policy. The Congressional Budget Office (2014) estimates that this act will increase spending on agri-cultural insurance programs by {\$}5.7 billion to a total of {\$}89.8 billion over the next decade. In light of the sizable resources directed toward these programs, accurate rating of insurance con-tracts is of the utmost importance to producers, private insurance companies, and the federal government. Unlike most forms of insurance, agricultural insurance is plagued by a paucity of spatially correlated data. A novel interpretation of Bayesian Model Averaging is used to estimate a set of possibly similar densities that offers greater efficiency if the set of densities are similar while seemingly not losing any if the set of densities are dissimilar. Simulations indicate that finite sample performance—in particular small sample performance—is quite promising. The proposed approach does not require knowledge of the form or extent of any possible similarities, is relatively easy to implement, admits correlated data, and can be used with either parametric or nonparametric estimators. We use the proposed approach to estimate U.S. crop insurance pre-mium rates for area-type programs and develop a test to evaluate its efficacy. An out-of-sample game between private insurance companies and the federal government highlights the policy implications for a variety of crop-state combinations. Consistent with the simulation results, the performance of the proposed approach with respect to rating area-type insurance—in particular small sample performance—remains quite promising.},
	author = {Ker, Alan P. and Tolhurst, Tor N. and Liu, Yong},
	doi = {10.1093/ajae/aav065},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ker, Tolhurst, Liu - 2016 - Bayesian Estimation of Possibly Similar Yield Densities Implications for Rating Crop Insurance Contracts.pdf:pdf},
	issn = {14678276},
	journal = {American Journal of Agricultural Economics},
	keywords = {Bayesian model averaging,Rating crop insurance contracts,multiple density estimation,small sample estimation,spatial correlation},
	mendeley-groups = {My Research/Theta/Review},
	number = {2},
	pages = {360--382},
	title = {{Bayesian Estimation of Possibly Similar Yield Densities: Implications for Rating Crop Insurance Contracts}},
	volume = {98},
	year = {2016}
}
@article{Liu2016,
	abstract = {This paper considers the problem of forecasting a collection of short time series using cross sectional information in panel data. We construct point predictors using Tweedie's formula for the posterior mean of heterogeneous coefficients under a correlated random effects distribution. This formula utilizes cross-sectional information to transform the unit-specifc (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coefficients. We show that the risk of a predictor based on a non-parametric estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated-random-effects distribution as known (ratio-optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions. },
	author = {Liu, Laura and Moon, Hyungsik Roger},
	doi = {10.2139/ssrn.2889000},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/forecasting-with-dynamic-panel-data-models.pdf:pdf},
	journal = {SSRN Electronic Journal},
	title = {{Forecasting with Dynamic Panel Data Models}},
	year = {2016}
}
@article{DiTraglia2016,
	abstract = {In finite samples, the use of a slightly endogenous but highly relevant instrument can reduce mean-squared error (MSE). Building on this observation, I propose a novel moment selection procedure for GMM–the Focused Moment Selection Criterion (FMSC)–in which moment conditions are chosen not based on their validity but on the MSE of their associated estimator of a user-specified target parameter. The FMSC mimics the situation faced by an applied researcher who begins with a set of relatively mild “baseline” assumptions and must decide whether to impose any of a collection of stronger but more controversial “suspect” assumptions. When the (correctly specified) baseline moment conditions identify the model, the FMSC provides an asymptotically unbiased estimator of asymptotic MSE, allowing us to select over the suspect moment conditions. I go on to show how the framework used to derive the FMSC can address the problem of inference post-moment selection. Treating post-selection estimators as a special case of moment-averaging, in which estimators based on different moment sets are given data-dependent weights, I propose simulation-based procedures for inference that can be applied to a variety of formal and informal moment-selection and averaging procedures. Both the FMSC and confidence interval procedures perform well in simulations. I conclude with an empirical example examining the effect of instrument selection on the estimated relationship between malaria and income per capita.},
	archivePrefix = {arXiv},
	arxivId = {1408.0705},
	author = {DiTraglia, Francis J.},
	doi = {10.1016/j.jeconom.2016.07.006},
	eprint = {1408.0705},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/DiTraglia (2016) Using Invalid Instruments on Purpose Focused Moment Selection and Averaging for GMM.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Focused Information Criterion,GMM estimation,Model averaging,Moment selection,Post-selection estimators},
	mendeley-groups = {Model Selection and Averaging},
	number = {2},
	pages = {187--208},
	title = {{Using invalid instruments on purpose: Focused moment selection and averaging for GMM}},
	volume = {195},
	year = {2016}
}
@article{Muller2016,
	abstract = {{\textcopyright} 2016 The Econometric Society Confidence intervals are commonly used to describe parameter uncertainty. In nonstandard problems, however, their frequentist coverage property does not guarantee that they do so in a reasonable fashion. For instance, confidence intervals may be empty or extremely short with positive probability, even if they are based on inverting powerful tests. We apply a betting framework and a notion of bet-proofness to formalize the “reasonableness” of confidence intervals as descriptions of parameter uncertainty, and use it for two purposes. First, we quantify the violations of bet-proofness for previously suggested confidence intervals in nonstandard problems. Second, we derive alternative confidence sets that are bet-proof by construction. We apply our framework to several nonstandard problems involving weak instruments, near unit roots, and moment inequalities. We find that previously suggested confidence intervals are not bet-proof, and numerically determine alternative bet-proof confidence sets.},
	author = {M{\"{u}}ller, Ulrich K. and Norets, Andriy},
	doi = {10.3982/ecta14023},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Confidence Sets/M{\"{u}}ller {\&} Norets (2016) - Credibility of confidence sets in nonstandard econometric problems.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	mendeley-groups = {Confidence Sets,Confidence Sets/Nonstandard Problems},
	number = {6},
	pages = {2183--2213},
	title = {{Credibility of Confidence Sets in Nonstandard Econometric Problems}},
	volume = {84},
	year = {2016}
}
@article{Bodnar2016,
	abstract = {We introduce a dynamic model for multivariate processes of (non-negative) high-frequency trading variables revealing time-varying conditional variances and correlations. Modeling the variables' conditional mean processes using a multiplicative error model, we map the resulting residuals into a Gaussian domain using a copula-type transformation. Based on high-frequency volatility, cumulative trading volumes, trade counts and market depth of various stocks traded at the NYSE, we show that the proposed transformation is supported by the data and allows capturing (multivariate) dynamics in higher order moments. The latter are modeled using a DCC-GARCH specification. We suggest estimating the model by composite maximum likelihood which is sufficiently flexible to be applicable in high dimensions. Strong empirical evidence for time-varying conditional (co-)variances in trading processes supports the usefulness of the approach. Taking these higher-order dynamics explicitly into account significantly improves the goodness-of-fit and out-of-sample forecasts of the multiplicative error model.},
	author = {Bodnar, Taras and Hautsch, Nikolaus},
	doi = {10.1016/j.jempfin.2015.12.002},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bodnar, Hautsch - 2016 - Dynamic conditional correlation multiplicative error processes.pdf:pdf},
	issn = {09275398},
	journal = {Journal of Empirical Finance},
	keywords = {DCC-GARCH,Gaussian domain,Liquidity risk,Multiplicative error model,Trading processes},
	mendeley-groups = {My Research/Balts},
	pages = {41--67},
	title = {{Dynamic conditional correlation multiplicative error processes}},
	volume = {36},
	year = {2016}
}
@misc{Tessler2016,
	author = {Tessler, Mark},
	doi = {10.3886/ICPSR32302.v6},
	mendeley-groups = {My Research/Prima},
	publisher = {Inter-university Consortium for Political and Social Research (ICPSR) [distributor]},
	title = {{Carnegie Middle East Governance and Islam Dataset, 1988-2014}},
	url = {http://doi.org/10.3886/ICPSR32302.v6},
	year = {2016}
}
@unpublished{Morozov2016,
	address = {Moscow},
	author = {Morozov, Vladislav and Loseva, Anna},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morozov, Loseva - 2016 - A Simple Universal Dynamic Framework for Endogenous Preference Change(2).pdf:pdf},
	institution = {Lomonosov Moscow State University},
	keywords = {CES,Endogenous preference change,ces utility,elasticity demand system,elasticity of substitution,endogenous preference change,habit formation,seds,substitution},
	mendeley-tags = {CES,Endogenous preference change},
	pages = {1--19},
	title = {{A Simple Universal Dynamic Framework for Endogenous Preference Change}},
	year = {2016}
}
@unpublished{Morozov2016,
	address = {Moscow},
	author = {Morozov, Vladislav and Loseva, Anna},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morozov, Loseva - 2016 - A Simple Universal Dynamic Framework for Endogenous Preference Change.pdf:pdf},
	institution = {Lomonosov Moscow State University},
	keywords = {ces utility,elasticity demand system,elasticity of substitution,endogenous preference change,habit formation,seds,substitution},
	mendeley-groups = {My Research/Alpha},
	pages = {1--19},
	title = {{A Simple Universal Dynamic Framework for Endogenous Preference Change}},
	url = {https://ssrn.com/abstract=2856000},
	year = {2016}
}
@article{Kakeu2016,
	author = {Kakeu, Johnson and Nguimkeu, Pierre},
	doi = {10.1016/j.eneco.2017.03.013},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakeu, Nguimkeu - 2016 - Habit Formation and Exhaustible Resource Risk-Pricing.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	keywords = {exhaustible,g12,habit formation,jel classification,long-run risk,oil,q30,q4,risk pricing,short-run risk},
	number = {September},
	publisher = {Elsevier B.V.},
	title = {{Habit Formation and Exhaustible Resource Risk-Pricing :}},
	year = {2016}
}
@article{Hansen2016,
	abstract = {This paper introduces shrinkage for general parametric models. We show how to shrink maximum likelihood estimators towards parameter subspaces defined by general nonlinear restrictions. We derive the asymptotic distribution and risk of our shrinkage estimator using a local asymptotic framework. We show that if the shrinkage dimension exceeds two, the asymptotic risk of the shrinkage estimator is strictly less than that of the maximum likelihood estimator (MLE). This reduction holds globally in the parameter space. We show that the reduction in asymptotic risk is substantial, even for moderately large values of the parameters. We also provide a new high-dimensional large sample local minimax efficiency bound. The bound is the lowest possible asymptotic risk, uniformly in a local region of the parameter space. Local minimax bounds are a stronger efficiency characterization than global minimax bounds. We show that our shrinkage estimator asymptotically achieves this local asymptotic minimax bound, while the MLE does not. Thus the shrinkage estimator, unlike the MLE, is locally minimax efficient. This theory is a combination and extension of standard asymptotic efficiency theory (H{\'{a}}jek, 1972) and local minimax efficiency theory for Gaussian models (Pinsker, 1980).},
	author = {Hansen, Bruce E.},
	doi = {10.1016/j.jeconom.2015.09.003},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Shrinkage/Hansen (2016) Efficient Shrinkage in Parametric Models.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {James-Stein,Maximum likelihood,Minimax,Nonlinear},
	mendeley-groups = {Model Selection and Averaging},
	number = {1},
	pages = {115--132},
	publisher = {Elsevier B.V.},
	title = {{Efficient shrinkage in parametric models}},
	volume = {190},
	year = {2016}
}
@book{Rubinstein2016,
	author = {Rubinstein, Ariel},
	booktitle = {Lecture Notes in Microeconomic Theory},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubinstein - 2016 - Lecture Notes in Microeconomic Theory.pdf:pdf},
	isbn = {9780691120300},
	pages = {175},
	title = {{Lecture Notes in Microeconomic Theory}},
	year = {2016}
}
@article{Baraud2016,
	archivePrefix = {arXiv},
	arxivId = {1411.5571},
	author = {Baraud, Yannick},
	eprint = {1411.5571},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baraud - 2016 - Bounding the expectation of the supremum of an empirical process over a (weak) VC-major class.pdf:pdf},
	journal = {Electronic Journal of Statistics},
	mendeley-groups = {UPF/Research Seminar},
	number = {2},
	pages = {1709--1728},
	title = {{Bounding the expectation of the supremum of an empirical process over a (weak) VC-major class}},
	volume = {10},
	year = {2016}
}
@book{Gilmour2016,
	abstract = {Since the 1970s and 1980s, community-based forestry has grown in popularity, based on the concept that local communities, when granted suf property rights over local forest commons, can organize autonomously and develop local institutions to regulate the use of natural resources and manage them sustainably. Over time, various forms of community-based forestry have evolved in different countries, but all have at their heart the notion of some level of participation by smallholders and community groups in planning and implementation. This publication is FAO's rst comprehensive look at the impact of community-based forestry since previous reviews in 1991 and 2001. It considers both collaborative regimes (forestry practised on land with formal communal tenure requiring collective action) and smallholder forestry (on land that is generally privately owned). The publication examines the extent of community-based forestry globally and regionally and assesses its effectiveness in delivering on key biophysical and socioeconomic outcomes, i.e. moving towards sustainable forest management and improving local livelihoods. The report is targeted at policy-makers, practitioners, researchers, communities and civil society.},
	author = {Gilmour, Don},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gilmour - 2016 - Forty years of community-based forestry.pdf:pdf},
	isbn = {978-92-5-109095-4},
	issn = {0258-6150},
	pages = {140},
	publisher = {FAO Forestry Papers},
	title = {{Forty years of community-based forestry}},
	year = {2016}
}
@article{Cao2017,
	author = {Cao, By Dan and Nie, Guangyu},
	doi = {10.1257/mac.20150219},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Nie - 2017 - Amplification and Asymmetric Effects without Collateral Constraints.pdf:pdf},
	journal = {American Economic Journal: Macroeconomics},
	keywords = {amplifica-,asymmetric effects,collateral constraint,dynamic general equilibrium,economy propagate and am-,exogenous shocks to the,global,incomplete markets,nonlinear solution methods,the mechanisms by which,tion},
	mendeley-groups = {Financial Frictions},
	number = {3},
	pages = {222--66},
	title = {{Amplification and Asymmetric Effects without Collateral Constraints}},
	volume = {9},
	year = {2017}
}
@book{Han2017,
	abstract = {In this paper we consider estimation and model selection of higher-order spatial autoregressive model by an efficient Bayesian approach. Based upon the exchange algorithm, we develop an efficient MCMC sampler, which does not rely on special features of spatial weights matrices and does not require the evaluation of the Jacobian determinant in the likelihood function. We also propose a computationally simple procedure to tackle nested model selection issues of higher-order spatial autoregressive models. We find that the exchange algorithm can be utilized to simplify the computation of Bayes factor through the Savage-Dickey density ratio. We apply the efficient estimation algorithm and the model selection procedure to study the “tournament competition” across Chinese cities and the spatial dependence of county-level voter participation rates in the 1980 U.S. presidential election.},
	author = {Han, Xiaoyi and Hsieh, Chih Sheng and fei Lee, Lung},
	booktitle = {Regional Science and Urban Economics},
	doi = {10.1016/j.regsciurbeco.2016.12.003},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Hsieh, Lee - 2017 - Estimation and model selection of higher-order spatial autoregressive model An efficient Bayesian approach.pdf:pdf},
	isbn = {8618350281},
	issn = {18792308},
	keywords = {Bayes factor,Bayesian estimation,Exchange algorithm,Higher-order spatial autoregressive model,Savage-Dickey density ratio},
	mendeley-groups = {Panel Data/Spatial Panels},
	pages = {97--120},
	publisher = {Elsevier},
	title = {{Estimation and model selection of higher-order spatial autoregressive model: An efficient Bayesian approach}},
	url = {http://dx.doi.org/10.1016/j.regsciurbeco.2016.12.003},
	volume = {63},
	year = {2017}
}
@article{Sueyoshi2017,
	author = {Sueyoshi, Toshiyuki and Goto, Mika},
	doi = {10.1016/j.eneco.2017.03.028},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sueyoshi, Goto - 2017 - Measurement of returns to scale on large photovoltaic power stations in the United States and Germany.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	pages = {306--320},
	publisher = {Elsevier B.V.},
	title = {{Measurement of returns to scale on large photovoltaic power stations in the United States and Germany}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0140988317301056},
	volume = {64},
	year = {2017}
}
@article{Baker2017,
	abstract = {We use the 2013 federal government shutdown and a rich data set from an online personal finance website to study the effects of changes in income on changes in consumption for thousands of affected workers. The 2013 shutdown represented a significant and unanticipated income shock for federal government workers, with no direct effect on permanent income. We exploit both the differences between unaffected state government employees and affected federal employees as well as between federal employees required to remain at work and those required to stay at home to generate variation in income and available time. We find strong evidence for excess sensitivity of consumption patterns, violating the permanent income hypothesis. We demonstrate that increased home production, changes in spending allocations, and credit constraints all play significant roles in driving spending. We discern detailed categories of household spending with widely varying elasticities. The results demonstrate the importance of household liquidity, leisure, and home production when constructing stimulus or social insurance policy.},
	author = {Baker, Scott R. and Yannelis, Constantine},
	doi = {10.1016/j.red.2016.09.005},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baker, Yannelis - 2017 - Income changes and consumption Evidence from the 2013 federal government shutdown.pdf:pdf},
	issn = {10942025},
	journal = {Review of Economic Dynamics},
	keywords = {Government shutdown,Home production,Income shock,Leisure,Liquidity},
	mendeley-groups = {UPF/Adv. Macro II/Consumption},
	pages = {99--124},
	publisher = {Elsevier Inc.},
	title = {{Income changes and consumption: Evidence from the 2013 federal government shutdown}},
	url = {http://dx.doi.org/10.1016/j.red.2016.09.005},
	volume = {23},
	year = {2017}
}
@article{Boubaker2017,
	author = {Boubaker, Heni and Raza, Syed Ali},
	doi = {10.1016/j.eneco.2017.01.026},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boubaker, Raza - 2017 - A wavelet analysis of mean and volatility spillovers between oil and BRICS stock markets.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	keywords = {BRICS stock market,Contagion effect,Hedge ratio,Oil price changes,Volatility spillovers,Wavelet coherence},
	pages = {105--117},
	publisher = {Elsevier B.V.},
	title = {{A wavelet analysis of mean and volatility spillovers between oil and BRICS stock markets}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0140988317300361},
	volume = {64},
	year = {2017}
}
@article{Shi2017,
	abstract = {This paper studies the estimation of a dynamic spatial panel data model with interactive individual and time effects with large n and T. The model has a rich spatial structure including contemporaneous spatial interaction and spatial heterogeneity. Dynamic features include individual time lag and spatial diffusion. The interactive effects capture heterogeneous impacts of time effects on cross sectional units. The interactive effects are treated as parameters, so as to allow correlations between the interactive effects and the regressors. We consider a quasi-maximum likelihood estimation and show estimator consistency and characterize its asymptotic distribution. The Monte Carlo experiment shows that the estimator performs well and the proposed bias correction is effective. We illustrate the empirical relevance of the model by applying it to examine the effects of house price dynamics on reverse mortgage origination rates in the US.},
	author = {Shi, Wei and fei Lee, Lung},
	doi = {10.1016/j.jeconom.2016.12.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi, Lee - 2017 - Spatial dynamic panel data models with interactive fixed effects.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Dynamics,Multiplicative individual and time effects,Spatial panel},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {2},
	publisher = {Elsevier B.V.},
	title = {{Spatial dynamic panel data models with interactive fixed effects}},
	url = {http://dx.doi.org/10.1016/j.jeconom.2016.12.001},
	volume = {197},
	year = {2017}
}
@article{Costa2017,
	author = {Costa, Oswaldo L.V. and {de Oliveira Ribeiro}, Celma and Rego, Erik Eduardo and Stern, Julio Michael and Parente, Virginia and Kileber, Solange},
	doi = {10.1016/j.eneco.2017.03.021},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Costa et al. - 2017 - Robust portfolio optimization for electricity planning An application based on the Brazilian electricity mix.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	pages = {158--169},
	publisher = {Elsevier B.V.},
	title = {{Robust portfolio optimization for electricity planning: An application based on the Brazilian electricity mix}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0140988317300877},
	volume = {64},
	year = {2017}
}
@article{Sapkota2017,
	author = {Sapkota, Pratikshya and Bastola, Umesh},
	doi = {10.1016/j.eneco.2017.04.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sapkota, Bastola - 2017 - Foreign direct investment , income , and environmental pollution in developing countries Panel data analysis.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	keywords = {Endogeneity,Environmental Kuznets curve,Fixed and random effects,Latin America,Pollution haven hypothesis,hypothesis},
	pages = {206--212},
	publisher = {Elsevier B.V.},
	title = {{Foreign direct investment , income , and environmental pollution in developing countries : Panel data analysis of Latin America}},
	url = {http://dx.doi.org/10.1016/j.eneco.2017.04.001},
	volume = {64},
	year = {2017}
}
@article{Konstantelos2017,
	author = {Konstantelos, Ioannis and Moreno, Rodrigo and Strbac, Goran},
	doi = {10.1016/j.eneco.2017.03.022},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Konstantelos, Moreno, Strbac - 2017 - Coordination and uncertainty in strategic network investment Case on the North Seas Grid.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	keywords = {Cross-border interconnection,Min-max regret planning,Net benefit allocation,Offshore coordination,Onshore and offshore transmission investment,cross-border interconnection,investment,offshore coordination,onshore and offshore transmission},
	pages = {131--148},
	publisher = {Elsevier B.V.},
	title = {{Coordination and uncertainty in strategic network investment: Case on the North Seas Grid}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0140988317300889},
	volume = {64},
	year = {2017}
}
@article{Bun2017,
	abstract = {We analyse the finite sample properties of maximum likelihood estimators for dynamic panel data models. In particular, we consider transformed maximum likelihood (TML) and random effects maximum likelihood (RML) estimation. We show that TML and RML estimators are solutions to a cubic first-order condition in the autoregressive parameter. Furthermore, in finite samples both likelihood estimators might lead to a negative estimate of the variance of the individual-specific effects. We consider different approaches taking into account the non-negativity restriction for the variance. We show that these approaches may lead to a solution different from the unique global unconstrained maximum. In an extensive Monte Carlo study we find that this issue is non-negligible for small values of T and that different approaches might lead to different finite sample properties. Furthermore, we find that the Likelihood Ratio statistic provides size control in small samples, albeit with low power due to the flatness of the log-likelihood function. We illustrate these issues modelling US state level unemployment dynamics.},
	author = {Bun, Maurice J.G. and Carree, Martin A. and Juodis, Artūras},
	doi = {10.1111/obes.12156},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Bun et al. (2017) -- MLE.pdf:pdf},
	issn = {14680084},
	journal = {Oxford Bulletin of Economics and Statistics},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {4},
	pages = {463--494},
	title = {{On Maximum Likelihood Estimation of Dynamic Panel Data Models}},
	volume = {79},
	year = {2017}
}
@article{Kelava2017,
	author = {Kelava, Augustin and Kohler, Michael and Krzyzak, Adam and Schaffland, Tim Fabian},
	doi = {10.1016/j.jmva.2016.10.006},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Nonparametric Estimation/Regression/Kelava et al (2017) Nonparametric estimation of a latent variable model.pdf:pdf},
	journal = {Journal of Multivariate Analysis},
	mendeley-groups = {Latent Variable Models},
	number = {February},
	pages = {112--134},
	title = {{Nonparametric Estimation of a Latent Variable Model}},
	volume = {154},
	year = {2017}
}
@article{Cardin2017,
	author = {Cardin, Michel-Alexandre and Zhang, Sizhe and Nuttall, William},
	doi = {10.1016/j.eneco.2017.03.023},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cardin, Zhang, Nuttall - 2017 - Strategic Real Option and Flexibility Analysis for Nuclear Power Plants Considering Uncertainty in Elect.pdf:pdf},
	issn = {01409883},
	journal = {Energy Economics},
	publisher = {Elsevier B.V.},
	title = {{Strategic Real Option and Flexibility Analysis for Nuclear Power Plants Considering Uncertainty in Electricity Demand and Public Acceptance}},
	url = {http://dx.doi.org/10.1016/j.eneco.2017.03.023},
	volume = {Submitted },
	year = {2017}
}
@article{Qu2017,
	abstract = {In spatial panel data models, when a spatial weights matrix is constructed from economic or social distance, spatial weights could be endogenous and also time varying. This paper presents model specification and proposes QMLE estimation of spatial dynamic panel data models with endogenous time varying spatial weights matrices. Asymptotic properties of the proposed QMLE are rigorously established. We extend the notion of spatial near-epoch dependence to allow time dependence. By using spatial-time LLN for near-epoch dependence process and CLT for martingale difference sequence, we establish the consistency and asymptotic normality of QMLE. Monte Carlo experiments show that the proposed estimators have satisfactory finite sample performance.},
	author = {Qu, Xi and fei Lee, Lung and Yu, Jihai},
	doi = {10.1016/j.jeconom.2016.11.004},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qu, Lee, Yu - 2017 - QML estimation of spatial dynamic panel data models with endogenous time varying spatial weights matrices.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Dynamic panels,Endogenous spatial weights matrix,Fixed effects,QMLE,Spatial autoregression},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {2},
	title = {{QML estimation of spatial dynamic panel data models with endogenous time varying spatial weights matrices}},
	volume = {197},
	year = {2017}
}
@article{Ledoit2017,
	abstract = {Markowitz (1952) portfolio selection requires an estimator of the covariance matrix of returns. To address this problem, we promote a nonlinear shrinkage estimator that is more flexible than previous linear shrinkage estimators and has just the right number of free parameters (that is, the Goldilocks principle). This number is the same as the number of assets. Our nonlinear shrinkage estimator is asymptotically optimal for portfolio selection when the number of assets is of the same magnitude as the sample size. In backtests with historical stock return data, it performs better than previous proposals and, in particular, it dominates linear shrinkage.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.1093/rfs/hhx052},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2017 - Nonlinear shrinkage of the covariance matrix for portfolio selection Markowitz meets goldilocks.pdf:pdf},
	isbn = {0893-9454},
	issn = {14657368},
	journal = {Review of Financial Studies},
	mendeley-groups = {UPF/ADTSI},
	number = {12},
	pages = {4349--4388},
	pmid = {1986607435},
	title = {{Nonlinear shrinkage of the covariance matrix for portfolio selection: Markowitz meets goldilocks}},
	volume = {30},
	year = {2017}
}
@article{Neto2017,
	abstract = {Abstract This study proposes a methodology for risk analysis and portfolio optimization of power generation assets with hydro, wind, and solar power, considering the Regulated Contracting Environment and the Mechanism for Reallocation of Energy in Brazil. Innovative stochastic models are used to generate synthetic time series for the random variables water inflow, wind speed, solar irradiance, temperature of the photovoltaic panel, and average generation capacity of the Mechanism for Reallocation of Energy. The simulation is implemented using the Monte Carlo method associated with a Cholesky decomposition. An economic approach is presented taking into account taxation and financing, as well as the Markowitz Portfolio theory. The results show that the initial correlation between the energy resources is altered by the cash flow model and mainly by the debt. In the diversification process, the complementarity between sources helps to reduce the economic risk. The increase in debt increases the correlation, decreases the return and risk and, consequently, affects the diversification process and economic results. The Mechanism for Reallocation of Energy significantly reduces the hydroelectric economic risk and increases the financial return, which directly benefits the formation of portfolios.},
	author = {Neto, Daywes Pinheiro and Domingues, Elder Geraldo and Coimbra, Ant{\'{o}}nio Paulo and de Almeida, An{\'{i}}bal Tra{\c{c}}a and Alves, Aylton Jos{\'{e}} and Calixto, Wesley Pacheco},
	doi = {http://doi.org/10.1016/j.eneco.2017.03.020},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neto et al. - 2017 - Portfolio optimization of renewable energy assets Hydro, wind, and photovoltaic energy in the regulated market in B.pdf:pdf},
	issn = {0140-9883},
	journal = {Energy Economics},
	keywords = {Mechanism for Reallocation of Energy,Monte Carlo Method,Portfolio Optimization,Renewable Energy,Risk Analysis},
	publisher = {Elsevier B.V.},
	title = {{Portfolio optimization of renewable energy assets: Hydro, wind, and photovoltaic energy in the regulated market in Brazil}},
	url = {http://www.sciencedirect.com/science/article/pii/S0140988317300865},
	year = {2017}
}
@article{Fernandez-Val2017,
	abstract = {This article reviews recent advances in fixed effect estimation of panel data models for long panels, where the number of time periods is relatively large. We focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable conditional on covariates and unobserved effects is specified parametrically, while the distribution of the unobserved effects is left unrestricted. Compared to existing reviews on long panels (Arellano and Hahn 2007; a section in Arellano and Bonhomme 2011) we discuss models with both individual and time effects, split-panel Jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. Understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects is our main focus, and the unifying theme is that the order of this bias is given by the simple formula p/n for all models discussed, with p the number of estimated parameters and n the total sample size.},
	archivePrefix = {arXiv},
	arxivId = {1709.08980},
	author = {Fern{\'{a}}ndez-Val, Iv{\'{a}}n and Weidner, Martin},
	eprint = {1709.08980},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Large Panels/Fern{\'{a}}ndez-Val, Weidner (2017) Fixed Effect Estimation of Large T Panel Data Models.pdf:pdf},
	institution = {cemmap},
	journal = {Annual Review of Economics},
	mendeley-groups = {Panel Data/Large Panels},
	title = {{Fixed Effect Estimation of Large T Panel Data Models}},
	url = {http://arxiv.org/abs/1709.08980},
	year = {2017}
}
@article{Castro2017,
	abstract = {This paper analyzes how oil price shocks are transmitted downstream to producer and consumer prices in the euro area at the highest disaggregate level. In doing so, we first generate an appropriate database that identifies each industrial production sector with its corresponding price of consumer goods for the euro area. We next estimate a constrained vector autoregressive model. Our findings show a statistically significant increase in producer prices after an oil price shock for branches with high oil consumptions, although this statistical pass-through is only partial. However, there is no evidence of a significant oil price pass-through to consumer prices for most branches, which suggests the adaptability of European producers from the most branches to higher oil price pressures without transmitting them to consumers (exceptions: mining, chemical and metal).},
	author = {Castro, Cesar and Jimenez-Rodriguez, Rebeca},
	doi = {10.5897/JAERD12.088},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castro, Jimenez-Rodriguez - 2017 - Oil price pass-through along the price chain in the euro area.pdf:pdf},
	isbn = {9781905846498},
	issn = {03014215},
	journal = {Energy Economics},
	number = {70227},
	pages = {1--20},
	pmid = {910},
	title = {{Oil price pass-through along the price chain in the euro area}},
	year = {2017}
}
@article{Obara2017,
	author = {Obara, Ichiro and Park, Jaeok},
	doi = {10.1016/j.jet.2017.09.008},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Obara, Park - 2017 - Repeated Games with General Discounting.pdf:pdf},
	issn = {00220531},
	journal = {Journal of Economic Theory},
	keywords = {future bias,minmax payoff,present bias,quasi-hyperbolic discounting,repeated games,time},
	pages = {348--375},
	publisher = {Elsevier Inc.},
	title = {{Repeated Games with General Discounting}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S002205311730100X},
	volume = {172},
	year = {2017}
}
@article{Zetlin-Jones2017,
	abstract = {Empirically, there is substantial cross-sectional variation in firms' use of external funds: roughly 80{\%} of investment by privately held firms is financed externally, compared to 20{\%} for publicly traded firms. In a model consistent with privately held and publicly traded firms' use of external funds, financial shocks generate only a modest response of output. This exercise casts doubt on the ability of financial shocks to generate significant economic fluctuations and emphasizes the role of non-financial linkages in understanding the importance of financial shocks.},
	author = {Zetlin-Jones, Ariel and Shourideh, Ali},
	doi = {10.1016/j.jmoneco.2017.08.001},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zetlin-Jones, Shourideh - 2017 - External financing and the role of financial frictions over the business cycle Measurement and theory.pdf:pdf},
	issn = {03043932},
	journal = {Journal of Monetary Economics},
	keywords = {External financing,Financial frictions,Financial shocks},
	mendeley-groups = {Financial Frictions},
	pages = {1--15},
	publisher = {Elsevier B.V.},
	title = {{External financing and the role of financial frictions over the business cycle: Measurement and theory}},
	url = {http://dx.doi.org/10.1016/j.jmoneco.2017.08.001},
	volume = {92},
	year = {2017}
}
@article{Athey2017,
	abstract = {This paper provides an assessment of the early contributions of machine learning to economics, as well as predictions about its future contributions. It begins by briefly overviewing some themes from the literature on ma-chine learning, and then draws some contrasts with traditional approaches to estimating the impact of counterfactual policies in economics. Next, we review some of the initial " off-the-shelf " applications of machine learning to economics, including applications in analyzing text and images. We then describe new types of questions that have been posed surrounding the appli-cation of machine learning to policy problems, including " prediction policy problems, " as well as considerations of fairness and manipulability. Next, we briefly review of some of the emerging econometric literature combin-ing machine learning and causal inference. Finally, we overview a set of predictions about the future impact of machine learning on economics.},
	archivePrefix = {arXiv},
	arxivId = {1801.00364},
	author = {Athey, Susan},
	doi = {10.1257/jep.31.2.87},
	eprint = {1801.00364},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Athey - 2017 - The Impact of Machine Learning on Economics.pdf:pdf},
	isbn = {08953309},
	issn = {0895-3309},
	journal = {NBER AI Workshop 2017},
	number = {Ml},
	pages = {1--27},
	pmid = {1522171656},
	title = {{The Impact of Machine Learning on Economics}},
	year = {2017}
}
@article{Peters2017,
	abstract = {{\textcopyright} 2017 Elsevier B.V.We revisit the gamma–gamma Bayesian chain-ladder (BCL) model for claims reserving in non-life insurance. This claims reserving model is usually used in an empirical Bayesian way using plug-in estimates for the variance parameters. The advantage of this empirical Bayesian framework is that allows us for closed form solutions. The main purpose of this paper is to develop the full Bayesian case also considering prior distributions for the variance parameters and to study the resulting sensitivities.},
	author = {Peters, G.W. and Targino, R.S. and W{\"{u}}thrich, M.V.},
	doi = {10.1016/j.insmatheco.2016.12.007},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, Targino, W{\"{u}}thrich - 2017 - Full Bayesian analysis of claims reserving uncertainty.pdf:pdf},
	issn = {01676687},
	journal = {Insurance: Mathematics and Economics},
	keywords = {[Chain-ladder method, Claims development result, C},
	mendeley-groups = {My Research/Theta/Review},
	pages = {1--20},
	title = {{Full Bayesian analysis of claims reserving uncertainty}},
	volume = {73},
	year = {2017}
}
@article{Buyse2017,
	abstract = {We study the effects of pension reform on hours worked, human capital, income and welfare in an open economy populated by four overlapping generations: three active generations (the young, the middle aged and the older) and one generation of retired. Within each generation we distinguish individuals with high, medium or low ability to build human capital. Our simulation results prefer a pay-as-you-go pension system with a particular earnings-related linkage above a fully-funded private system. This pay-as-you-go system conditions pension benefits on past individual labor income, with a high weight on labor income earned when older and a low weight on labor income earned when young. Uncorrected, however, such a system implies welfare losses for current low-ability generations and rising inequality. Complementing or replacing it by basic and/or minimum pension components is negative for aggregate employment and welfare. Better is to maintain the tight link between individual labor income and the pension also for low-ability individuals, but to strongly raise their replacement rate. An additional correction improving the welfare of low-ability individuals would be to maintain for these individuals equal weights on past labor income.},
	author = {Buyse, Tim and Heylen, Freddy and {Van De Kerckhove}, Renaat},
	doi = {10.1017/S1474747215000281},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buyse, Heylen, Van De Kerckhove - 2017 - Pension reform in an OLG model with heterogeneous abilities.pdf:pdf},
	isbn = {1474747215000},
	issn = {14753022},
	journal = {Journal of Pension Economics and Finance},
	keywords = {Employment by age,heterogeneous abilities,overlapping generations,pension reform,retirement},
	mendeley-groups = {My Research/Pensions},
	number = {2},
	pages = {144--172},
	title = {{Pension reform in an OLG model with heterogeneous abilities}},
	volume = {16},
	year = {2017}
}
@unpublished{Chudik2017,
	abstract = {This paper contributes to the GMM literature by introducing the idea of self-instrumenting target variables instead of searching for instruments that are uncorrelated with the errors, in cases where the correlation between the target variables and the errors can be derived. The advantage of the proposed approach lies in the fact that, by construction, the instruments have maximum correlation with the target variables and the problem of weak instrument is thus avoided. The proposed approach can be applied to estimation of a variety of models such as spatial and dynamic panel data models. In this paper we focus on the latter and consider both univariate and multivariate panel data models with short time dimension. Simple Bias-corrected Methods of Moments (BMM) estimators are proposed and shown to be consistent and asymptotically normal, under very general conditions on the initialization of the processes, individual-specific effects, and error variances allowing for heteroscedasticity over time as well as cross-sectionally. Monte Carlo evidence document BMM's good small sample performance across different experimental designs and sample sizes, including in the case of experiments where the system GMM estimators are inconsistent. We also find that the proposed estimator does not suffer size distortions and has satisfactory power performance as compared to other estimators.},
	author = {Chudik, Alexander and Pesaran, M. Hashem},
	doi = {10.2139/ssrn.3043407},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Chudik, Pesaran (2017) - Dynamic Panels, Self-Instrumenting and Correcting Bias.pdf:pdf},
	keywords = {C12,C13,C23,GMM,Monte Carlo Evidence.,Panel VARs,Quadratic Moment Conditions,Short-T Dynamic Panels,Weak Instrument Problem},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {June 2016},
	title = {{A Bias-Corrected Method of Moments Approach to Estimation of Dynamic Short-T Panels}},
	year = {2017}
}
@article{Song2018,
	abstract = {Polymers having the following general formula have been synthesised  where R is a long chain alkyl substituent such as -C16H33. The number of skeletal oxygen atoms n = 5 or 6 (coded C16O5 or C16O6) so that the helix formed by association of neighbouring side groups R, has 18 or 21 skeletal bonds per rotation and so imitates the PEOLi + or PEONa+ 21 helix. Complexes with LiBF4 in stoichiometries of 1 mol salt per structural (helical) repeat unit and 0.5 mol per repeat have been prepared. Small-angle- and wide-angle X-ray diffraction, thermal analysis and hot stage polarised light microscopy have been used to show that the complexes form smectic systems in the uncomplexed and complexed forms. In the complexes, cations are enclosed within the helices and the anions lie between planes of helices and the alkyl side chains so that the overall d spacing is 42.9 {\AA}. The -C16H33 side chains melt at ca. 40 °C in the complexed material, to give a liquid crystal phase. The liquid crystal-isotropic transition varies slightly according to polymer and stoichiometry and is 89 °C for C16O5:LiBF4 (1:1). The ionic conductivity-temperature relationship shows well-defined transitions at the phase changes and the complex impedance behaviour also reflects the several states of the system. The conductivity of the (1:1) complex is higher than the (1:0.5) complex and is comparable to P(EO)6:LiBF4 in the crystal and istropic phases, but is somewhat lower over the temperature range of the liquid crystal phase.},
	author = {Song, Qifan},
	doi = {10.1002/wics.1416},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song - 2018 - An overview of reciprocal L1-regularization for high dimensional regression data.pdf:pdf},
	issn = {19390068},
	journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
	keywords = {Monte Carlo optimization,Reciprocal Lasso,high dimension regression,model selection consistency},
	number = {1},
	pages = {1--16},
	title = {{An overview of reciprocal L1-regularization for high dimensional regression data}},
	volume = {10},
	year = {2018}
}
@article{Schweighofer-Kodritsch2018,
	author = {Schweighofer-Kodritsch, Sebastian},
	doi = {10.3982/ECTA14396},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schweighofer-Kodritsch - 2018 - Time Preferences and Bargaining.pdf:pdf},
	issn = {0012-9682},
	journal = {Econometrica},
	keywords = {alternating offers,c78,d03,d74,delay,discounting,dynamic incon-,earlier work,impatience,jel classification,my,non-stationary equilibrium,on time-inconsistency in bargaining,optimal punishment,simple penal codes,sistency,this paper generalizes my,time preference,which was based on},
	number = {1},
	pages = {173--217},
	title = {{Time Preferences and Bargaining}},
	volume = {86},
	year = {2018}
}
@article{Hsiao2018,
	author = {Hsiao, Cheng},
	doi = {10.1016/j.jeconom.2018.06.017},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Time-varying Effects/Hsiao (2018) Interactive Effects.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data,Panel Data/Factor Models and Interactive Effects},
	number = {2},
	pages = {645--673},
	publisher = {Elsevier B.V.},
	title = {{Panel models with interactive effects ✩}},
	url = {https://doi.org/10.1016/j.jeconom.2018.06.017},
	volume = {206},
	year = {2018}
}
@article{Bonhomme2018,
	author = {Bonhomme, St{\'{e}}phane and Weidner, Martin},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Robustness/Bonhomme, Weidner (2018) Minimizing Sensitivity to Model Misspecification.pdf:pdf},
	keywords = {ben connault,chris hansen,counterfactuals,els,esfandiar maasoumi,gary chamberlain,jin hahn,kei hirano,lars hansen,latent variables,max kasy,model misspecification,panel data,robustness,roger koenker,sensitivity analysis,structural mod-,thibaut lamadon,tim armstrong,tim christensen,we thank josh angrist},
	mendeley-groups = {Panel Data/Robustness},
	title = {{Minimizing Sensitivity to Model Misspecification}},
	year = {2018}
}
@article{Graham2018,
	author = {Graham, Bryan S and Hahn, Jinyong and Poirier, Alexandre and Powell, James L},
	doi = {10.1016/j.jeconom.2018.06.004},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Graham et al. (2018) A quantile correlated random coefficients panel data model.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {305--335},
	publisher = {Elsevier B.V.},
	title = {{A quantile correlated random coefficients panel data model}},
	url = {https://doi.org/10.1016/j.jeconom.2018.06.004},
	volume = {206},
	year = {2018}
}
@article{Okui2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1801.04672v2},
	author = {Okui, Ryo and Wang, Wendun},
	eprint = {arXiv:1801.04672v2},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Okui, Wang (2018) Heterogeneous structural breaks in panel data models.pdf:pdf},
	keywords = {grouped fixed ef-,grouped patterns,panel data,structural breaks},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	pages = {1--54},
	title = {{Heterogeneous structural breaks in panel data models}},
	year = {2018}
}
@article{Ledoit2018,
	abstract = {This paper introduces a new method for deriving covariance matrix estimators that are decision-theoretically optimal within a class of nonlinear shrinkage estimators. The key is to employ large-dimensional asymptotics: the matrix dimension and the sample size go to infinity together, with their ratio converging to a finite, nonzero limit. As the main focus, we apply this method to Stein's loss. Compared to the estimator of Stein (1975, 1986), ours has five theoretical advantages: (1) it asymptotically minimizes the loss itself, instead of an estimator of the expected loss; (2) it does not necessitate post-processing via an ad hoc algorithm (called “isotonization”) to restore the positivity or the ordering of the covariance matrix eigenvalues; (3) it does not ignore any terms in the function to be minimized; (4) it does not require normality; and (5) it is not limited to applications where the sample size exceeds the dimension. In addition to these theoretical advantages, our estimator also improves upon Stein's estimator in terms of finite-sample performance, as evidenced via extensive Monte Carlo simulations. To further demonstrate the effectiveness of our method, we show that some previously suggested estimators of the covariance matrix and its inverse are decision-theoretically optimal in the large-dimensional asymptotic limit with respect to the Frobenius loss function.},
	author = {Ledoit, Olivier and Wolf, Michael},
	doi = {10.2139/ssrn.2264903},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledoit, Wolf - 2018 - Optimal Estimation of a Large-Dimensional Covariance Matrix Under Stein's Loss.pdf:pdf},
	isbn = {0090-5364},
	issn = {0090-5364},
	journal = {Bernoulli},
	keywords = {C13,Large-dimensional asymptotics,Stein's loss,nonlinear shrinkage estimation,random matrix theory,rotation equivariance},
	mendeley-groups = {UPF/ADTSI},
	pages = {3791--3832},
	title = {{Optimal Estimation of a Large-Dimensional Covariance Matrix Under Stein's Loss}},
	volume = {24},
	year = {2018}
}
@unpublished{Huber2018,
	abstract = {In this paper, we provide a parsimonious means to estimate panel VARs with stochastic volatility. We assume that coefficients associated with domestic lagged endogenous variables arise from a Gaussian mixture model. Shrinkage on the cluster size is introduced through suitable priors on the component weights and cluster-relevant quantities are identified through novel shrinkage priors. To assess whether dynamic interdependencies between economies are needed, we moreover impose shrinkage priors on the coefficients related to other countries' endogenous variables. Finally, our model controls for static interdependencies by assuming that the reduced form shocks of the model feature a factor stochastic volatility structure. We assess the merits of the proposed approach by using synthetic data as well as a real data application. In the empirical application, we forecast Eurozone unemployment rates and show that our proposed approach works well in terms of predictions.},
	archivePrefix = {arXiv},
	arxivId = {1804.01554},
	author = {Huber, Florian and Pfarrhofer, Michael},
	eprint = {1804.01554},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huber, Pfarrhofer - 2018 - Dealing with cross-country heterogeneity in panel VARs using finite mixture models.pdf:pdf},
	keywords = {density predictions,factor,hierarchical modeling,multi-country models,stochastic volatility models},
	mendeley-groups = {Panel Data/VAR},
	number = {2010},
	title = {{Dealing with cross-country heterogeneity in panel VARs using finite mixture models}},
	url = {http://arxiv.org/abs/1804.01554},
	year = {2018}
}
@book{DESA2018,
	address = {New York},
	author = {UN-DESA},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Energy Statistics/IRES (2018).pdf:pdf},
	isbn = {9789211615845},
	mendeley-groups = {Climate Enviro and Energyu/Energy Statistics,Climate Enviro and Energyu},
	title = {{International Recommendations for Energy Statistics (IRES)}},
	year = {2018}
}
@article{Lee2018,
	abstract = {In this paper we consider the estimation of a dynamic panel autoregressive (AR) process of possibly infinite order in the presence of individual effects. We employ double asymptotics under which both the cross-sectional sample size and the length of time series tend to infinity and utilize the sieve AR approximation with its lag order increasing with the sample size. We establish the consistency and asymptotic normality of the fixed effects estimator and propose a bias-corrected fixed effects estimator based on a theoretical asymptotic bias term. Monte Carlo simulations demonstrate the usefulness of bias correction. As an illustration, the proposed methods are applied to dynamic panel estimation of the law of one price deviations among US cities.},
	author = {Lee, Yoon Jin and Okui, Ryo and Shintani, Mototsugu},
	doi = {10.1016/j.jeconom.2017.04.005},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Lee, Okui, Shintani (2018) - Asymptotic inference for dynamic panel estimators of infinite order AR processes.pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Autoregressive sieve estimation,Bias correction,Double asymptotics,Fixed effects estimator},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {2},
	pages = {147--158},
	publisher = {Elsevier B.V.},
	title = {{Asymptotic inference for dynamic panel estimators of infinite order autoregressive processes}},
	url = {https://doi.org/10.1016/j.jeconom.2017.04.005},
	volume = {204},
	year = {2018}
}
@article{Shi2018,
	abstract = {Many spatial panel data sets exhibit cross sectional and/or intertemporal dependence from spatial interactions or common factors. In an application of a spatial autoregressive model, a spatial weights matrix may be constructed from variables that may correlate with unobservables in the main equation and therefore is endogenous. Some common factors may be unobserved and correlate with included regressors in the equation. This paper presents a unified approach to model spatial panels with endogenous time varying spatial weights matrices and unobserved common factors. We show that the proposed QML estimator is consistent and asymptotically normal. As its limiting distribution may have a leading order bias, an analytical bias correction is proposed. Monte Carlo simulations demonstrate good finite sample properties of the estimators. This model is empirically applied to examine the effects of house price dynamics on reverse mortgage origination rates in the United States.},
	author = {Shi, Wei and Lee, Lung-fei},
	doi = {10.1016/j.regsciurbeco.2017.03.007},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi, Lee - 2018 - A spatial panel data model with time varying endogenous weights matrices and common factors.pdf:pdf},
	issn = {18792308},
	journal = {Regional Science and Urban Economics},
	keywords = {Endogenous spatial weighting matrix,Multiplicative individual and time effects,QMLE,Reverse mortgages,Spatial panel data},
	mendeley-groups = {Panel Data/Spatial Panels},
	number = {March},
	pages = {6--34},
	publisher = {Elsevier},
	title = {{A spatial panel data model with time varying endogenous weights matrices and common factors}},
	url = {http://dx.doi.org/10.1016/j.regsciurbeco.2017.03.007},
	volume = {72},
	year = {2018}
}
@article{Yuan2018,
	abstract = {Economic theory establishes that pension privatization weakens the link between old and young and so reduces the incentive to invest in public education in an economy with lower return rate of capital than growth rate of wage. However, empirical studies of the link change are few. In this paper, we investigate the effects of pension privatization and the central government's subsidy to individual accounts on public education spending in a three-period overlapping generation model. And then, we take contemporary pension reforms in a number of Chinese provinces as offering natural experiment conditions. Using a difference-in-difference framework and 282 municipal districts panel data over years 1998–2009, we test the pension-education theoretical link change. Both our theoretical and empirical results confirm that pension privatization is adversely associated with local public spending on education in China. Private pension subsidies, moreover, magnify this effect. Our study supports the theoretical assertion and selective empirical findings of a negative intergenerational effect of pension privatization.},
	author = {Yuan, Cheng and Li, Chengjian and Johnston, Lauren A.},
	doi = {10.1007/s00148-018-0690-3},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan, Li, Johnston - 2018 - The intergenerational education spillovers of pension reform in China.pdf:pdf},
	issn = {09331433},
	journal = {Journal of Population Economics},
	keywords = {Fully funding individual accounts,Local public finance,Pension system,Public education spending},
	mendeley-groups = {My Research/Pensions},
	number = {3},
	pages = {671--701},
	publisher = {Journal of Population Economics},
	title = {{The intergenerational education spillovers of pension reform in China}},
	volume = {31},
	year = {2018}
}
@article{Andrews2019,
	abstract = {When instruments are weakly correlated with endogenous regressors, conventional methods for instrumental variables (IV) estimation and inference become unreliable. A large literature in econometrics has developed procedures for detecting weak instruments and constructing robust confidence sets, but many of the results in this literature are limited to settings with independent and homoskedastic data, while data encountered in practice frequently violate these assumptions. We review the literature on weak instruments in linear IV regression with an emphasis on results for nonhomoskedastic (heteroskedastic, serially correlated, or clustered) data. To assess the practical importance of weak instruments, we also report tabulations and simulations based on a survey of papers published in the American Economic Review from 2014 to 2018 that use IV. These results suggest that weak instruments remain an important issue for empirical practice, and that there are simple steps that researchers can take to better handle weak instruments in applications.},
	author = {Andrews, Isaiah and Stock, James H. and Sun, Liyang},
	doi = {10.1146/annurev-economics-080218-025643},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Weak IV/Andrews, Stock, Sun (2018) Weak Instruments in IV Regression - Theory and Practice.pdf:pdf},
	issn = {1941-1383},
	journal = {Annual Review of Economics},
	keywords = {f-statistic,heteroskedasticity,weak instruments},
	mendeley-groups = {Weak IV},
	number = {1},
	pages = {727--753},
	title = {{Weak Instruments in Instrumental Variables Regression: Theory and Practice}},
	volume = {11},
	year = {2019}
}
@unpublished{Beutner2019,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1710.00643v2},
	author = {Beutner, Eric and Heinemann, Alexander and Smeekes, Stephan},
	eprint = {arXiv:1710.00643v2},
	file = {:C$\backslash$:/Users/moren/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beutner, Heinemann, Smeekes - 2019 - A Justification of Conditional Confidence Intervals.pdf:pdf},
	keywords = {conditional confidence intervals,parameter uncertainty,sample-splitting},
	mendeley-groups = {Confidence Sets},
	title = {{A Justification of Conditional Confidence Intervals}},
	year = {2019}
}
@incollection{Huang2019,
	author = {Huang, Bai and Lee, Tae-Hwy and Ullah, Aman},
	booktitle = {Topics in Identification, Limited Dependent Variables, Partial Observability, Experimentation, and Flexible Modeling: Part A},
	doi = {10.1108/s0731-90532019000040a011},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Huang, Lee, Ullah (2018) Stein-Like Shrinkage Estimation of Panel Data Models with CCE.pdf:pdf},
	keywords = {aging,and an anonymous referee,at the aie40 conference,at uci,c13,c33,c52,common correlated effects,endogeneity,fixed effect,for many valuable,hausman test,jel classification,local asymptotics,model aver-,panel data,shrinkage,the co-editor,we thank the participants},
	mendeley-groups = {Model Selection and Averaging/Panel},
	number = {September},
	pages = {249--274},
	title = {{Stein-like Shrinkage Estimation of Panel Data Models with Common Correlated Effects}},
	year = {2019}
}
@article{Nordhaus2019,
	author = {Nordhaus, William},
	doi = {10.1257/aer.109.6.1991},
	file = {:D$\backslash$:/Academic Things/Energy and Climate/Articles/Climate Change/Nordhaus (2019) Climate Change, The Ultimate Challenge for Economics.pdf:pdf},
	issn = {19447981},
	journal = {American Economic Review},
	mendeley-groups = {Climate Enviro and Energyu/Climate Change},
	number = {6},
	pages = {1991--2014},
	title = {{Climate change: The ultimate challenge for economics}},
	volume = {109},
	year = {2019}
}
@unpublished{Nibbering2019,
	author = {Nibbering, Didier and Paap, Richard},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Nibbering, Paap (2019) Panel Forecasting with Asymmetric Grouping.pdf:pdf},
	keywords = {australia,business statistics,c23,c51,c53,clayton vic 3800,correspondence to,department of econometrics,didier,e-mail,edu,forecasting,jel classification,monash,monash uni-,nibbering,panel data,parameter heterogeneity,versity},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	title = {{Panel Forecasting with Asymmetric Grouping}},
	year = {2019}
}
@article{Okui2019,
	abstract = {This paper proposes nonparametric kernel-smoothing estimation for panel data to examine the degree of heterogeneity across cross-sectional units. We first estimate the sample mean, autocovariances, and autocorrelations for each unit and then apply kernel smoothing to compute their density functions. The dependence of the kernel estimator on bandwidth makes asymptotic bias of very high order affect the required condition on the relative magnitudes of the cross-sectional sample size (N) and the time-series length (T). In particular, it makes the condition on N and T stronger and more complicated than those typically observed in the long-panel literature without kernel smoothing. We also consider a split-panel jackknife method to correct bias and construction of confidence intervals. An empirical application illustrates our procedure.},
	archivePrefix = {arXiv},
	arxivId = {1802.08825},
	author = {Okui, Ryo and Yanagi, Takahide},
	doi = {10.1093/ectj/utz019},
	eprint = {1802.08825},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Okui, Yanagi (2019) Kernel Estimation for Panel Data with Heterogeneous Dynamics.pdf:pdf},
	issn = {1937240X},
	journal = {The Econometrics Journal},
	keywords = {Autocorrelation,density estimation,heterogeneity,incidental parameter,jackknife,kernel smoothing},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {156--175},
	title = {{Kernel estimation for panel data with heterogeneous dynamics}},
	volume = {23},
	year = {2019}
}
@article{Okui2019b,
	author = {Okui, Ryo and Yanagi, Takahide},
	doi = {10.1016/j.jeconom.2019.04.036},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Okui, Yanagi (2019) Panels with Heterogeneous Dynamics.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {2},
	pages = {451--475},
	publisher = {Elsevier B.V.},
	title = {{Panel Data Analysis with Heterogeneous Dynamics}},
	url = {10.1016/j.jeconom.2019.04.036},
	volume = {212},
	year = {2019}
}

@article{Zhang2019,
	author = {Zhang, Yingying and Wang, Huixia Judy and Zhu, Zhongyi},
	doi = {10.1016/j.jeconom.2019.04.005},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Zhang, Wang, Zhu (2019). Quantile-regression-based clustering for panel data.pdf:pdf},
	issn = {0304-4076},
	journal = {Journal of Econometrics},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {1},
	pages = {54--67},
	publisher = {Elsevier B.V.},
	title = {{Quantile-regression-based clustering for panel data}},
	url = {https://doi.org/10.1016/j.jeconom.2019.04.005},
	volume = {213},
	year = {2019}
}
@article{Hsiao2019,
	author = {Hsiao, Cheng and Li, Qi and Liang, Zhongwen and Xie, Wei},
	doi = {10.3390/econometrics7010007},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Hsiao et al. (2019) Panel Data Estimation for Correlated Random Coefficients Models.pdf:pdf},
	journal = {Econometrics},
	keywords = {c13,c33,correlated random coefficients,efficiency bound,jel classification,panel data},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {7},
	pages = {1--18},
	title = {{Panel Data Estimation for Correlated Random Coefficients Models}},
	volume = {7},
	year = {2019}
}
@article{Jochmans2019,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1803.04991v4},
	author = {Jochmans, Koen and Weidner, Martin},
	doi = {10.1017/S0266466622000378},
	eprint = {arXiv:1803.04991v4},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Jochmans, Weidner (2019) Inference on a Distribution from Noisy Draws.pdf:pdf},
	journal = {Econometric Theory},
	mendeley-groups = {Panel Data/Heterogeneous Panels,Panel Data/Heterogeneous Panels/Distributions,Random Coefficients/Panel},
	number = {1},
	pages = {60--97},
	title = {{Inference on a Distribution from Noisy Draws}},
	volume = {40},
	year = {2024}
}
@article{Ding2016,
	archivePrefix = {arXiv},
	arxivId = {1412.5000},
	author = {Ding, Peng and Feller, Avi and Miratrix, Luke},
	doi = {10.1111/rssb.12124},
	eprint = {1412.5000},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Treatment Effects/Permutation Inference/Ding, Feller, Miratrix (2016) Randomization inference for treatment effect variation.pdf:pdf},
	issn = {14679868},
	journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	mendeley-groups = {Treatment Effects/Randomization Inference},
	number = {3},
	pages = {655--671},
	title = {{Randomization Inference for Treatment Effect Variation}},
	volume = {78},
	year = {2016}
}


@unpublished{Laage2019,
	abstract = {This paper studies a class of linear panel models with random coefficients. We do not restrict the joint distribution of the time-invariant unobserved heterogeneity and the covariates. We investigate identification of the average partial effect (APE) when fixed-effect techniques cannot be used to control for the correlation between the regressors and the time-varying disturbances. Relying on control variables, we develop a constructive two-step identification argument. The first step identifies nonparametrically the conditional expectation of the disturbances given the regressors and the control variables, and the second step uses "between-group" variations, correcting for endogeneity, to identify the APE. We propose a natural semiparametric estimator of the APE, show its c n asymptotic normality and compute its asymptotic variance. The estimator is computationally easy to implement, and Monte Carlo simulations show favorable finite sample properties. Control variables arise in various economic and econometric models, and we provide variations of our argument to obtain identification in some applications. As an empirical illustration, we estimate the average elasticity of intertemporal substitution in a labor supply model with random coefficients.},
	archivePrefix = {arXiv},
	arxivId = {2003.09367},
	author = {Laage, Louise},
	eprint = {2003.09367},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Laage (2019) A Correlated Random Coefficient Panel Model with Time-Varying Endogeneity.pdf:pdf},
	keywords = {Economics - Econometrics},
	mendeley-groups = {Panel Data,Panel Data/Partial Effects,Panel Data/Heterogeneous Panels,Control Function Estimation,Control Function Estimation/Theory,Random Coefficients/Panel},
	title = {{A Correlated Random Coefficient Panel Model with Time-Varying Endogeneity}},
	year = {2022}
}


@article{Wang2019,
	abstract = {This paper considers estimating the slope parameters and forecasting in potentially heterogeneous panel data regressions with a long time dimension. We propose a novel optimal pooling averaging estimator that makes an explicit trade-off between efficiency gains from pooling and bias due to heterogeneity. By theoretically and numerically comparing various estimators, we find that a uniformly best estimator does not exist and that our new estimator is superior in nonextreme cases and robust in extreme cases. Our results provide practical guidance for the best estimator and forecast depending on features of data and models. We apply our method to examine the determinants of sovereign credit default swap spreads and forecast future spreads.},
	author = {Wang, Wendun and Zhang, Xinyu and Paap, Richard},
	doi = {10.1002/jae.2696},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Wang, Zhang, Paap (2019) To pool or not to pool - What is a good strategy for parameter estimation and forecasting in panel regressions.pdf:pdf},
	issn = {10991255},
	journal = {Journal of Applied Econometrics},
	keywords = {academy of mathematics and,address for correspondence,c23,c52,chinese academy,credit default swap spreads,forecasting,g15,heterogeneous panel,jel classification,model screening,panel data,pooling averaging,systems science,x,zhang},
	mendeley-groups = {Panel Data/Heterogeneous Panels},
	number = {5},
	pages = {724--745},
	title = {{To pool or not to pool: What is a good strategy for parameter estimation and forecasting in panel regressions?}},
	volume = {34},
	year = {2019}
}
 
@article{Lee2019,
	abstract = {We show that control function estimators (CFEs) of the firm production function, such as Olley–Pakes, may be biased when productivity evolves with a firm-specific intercept, in which case the correctly specified control function will contain a firm-specific term, omitted in the standard CFEs. We develop an estimator that is free from this bias by introducing firm fixed effects in the control function. Applying our estimator to the data, we find that it outperforms the existing CFEs in terms of capturing persistent unobserved heterogeneity in firm productivity. Our estimator involves minimal modification to the standard CFE procedures and can be easily implemented using common statistical software.},
	author = {Lee, Yoonseok and Stoyanov, Andrey and Zubanov, Nikolay},
	doi = {10.1111/obes.12259},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Applied/Production Function Estimation/Lee, Stoyanov, Zubanov (2018).Olley and Pakes-style Production Function Estimators with Firm Fixed Effects.pdf:pdf},
	issn = {14680084},
	journal = {Oxford Bulletin of Economics and Statistics},
	mendeley-groups = {Applied/Production Function Estimation},
	number = {1},
	pages = {79--97},
	title = {{Olley and Pakes-style Production Function Estimators with Firm Fixed Effects}},
	volume = {81},
	year = {2019}
}
@article{Zhu2019,
	author = {Zhu, Ke},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Time Series/HAC/Zhu (2019) - Statistical Inference for Autoregressive Models under Heteroscedasticity of unknown form (2019).pdf:pdf},
	journal = {The Annals of Statistics},
	keywords = {62F03,62F12,62F35,62M10,Adaptive estimator,autoregressive model,conditional heteroscedasticity,heteroscedasticity,weighted least absolute deviations estimator,wild bootstrap},
	mendeley-groups = {Time Series/HAC Robust Inference},
	number = {6},
	pages = {3185--3215},
	title = {{Statistical Inference for Autoregressive Models Under Heteroskedasticity of Unknown Form}},
	volume = {47},
	year = {2019}
}
@article{Zhang2019a,
	abstract = {This paper considers the problem of inference for nested least squares averaging estimators. We study the asymptotic behavior of the Mallows model averaging estimator (MMA; Hansen, 2007) and the jackknife model averaging estimator (JMA; Hansen and Racine, 2012) under the standard asymptotics with fixed parameters setup. We find that both MMA and JMA estimators asymptotically assign zero weight to the under-fitted models, and MMA and JMA weights of just-fitted and over-fitted models are asymptotically random. Building on the asymptotic behavior of model weights, we derive the asymptotic distributions of MMA and JMA estimators and propose a simulation-based confidence interval for the least squares averaging estimator. Monte Carlo simulations show that the coverage probabilities of proposed confidence intervals achieve the nominal level.},
	author = {Zhang, Xinyu and Liu, Chu-An},
	doi = {10.1017/S0266466618000269},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Zhang, Liu (2017) Inference After Model Averagin in Linear Regression Modls.pdf:pdf},
	journal = {Econometric Theory},
	keywords = {and peter phillips for,c51,c52,confidence intervals,eraging,inference post-model-averaging,jackknife model av-,jel classification,mallows model averaging,many constructive comments and,sug-,three referees,we thank the co-editor},
	mendeley-groups = {Model Selection and Averaging},
	number = {4},
	pages = {816--841},
	title = {{Inference after Model Averaging in Linear Regression Models}},
	volume = {35},
	year = {2019}
}
@article{Yin2019,
	abstract = {This paper considers model selection and model averaging in panel data models with a multifactor error structure. We investigate the limiting distribution of the common correlated effects estimator (Pesaran, 2006) in a local asymptotic framework and show that the trade-off between bias and variance remains in the asymptotic theory. We then propose a focused information criterion and a plug-in averaging estimator for large heterogeneous panels and examine their theoretical properties. The novel feature of the proposed method is that it aims to minimize the sample analog of the asymptotic mean squared error and can be applied to cases irrespective of whether the rank condition holds or not. Monte Carlo simulations show that both proposed selection and averaging methods generally achieve lower expected squared error than other methods. The proposed methods are applied to analyze the consumer response to gasoline taxes.},
	author = {Yin, Shou-Yung and Liu, Chu-An and Lin, Chang-Ching},
	doi = {10.1080/07350015.2019.1623044},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/Panel/Yin, Liu, Lin (2019) Focused Information Criterion and Model Averaging for Large Panels With a Multifactor Error Structure.pdf:pdf},
	journal = {Journal of Business and Economic Statistics},
	keywords = {common correlated effects,cross-sectional dependence,model selection,plug-},
	mendeley-groups = {Model Selection and Averaging/Panel},
	number = {1},
	pages = {54--68},
	title = {{Focused Information Criterion and Model Averaging for Large Panels with a Multifactor Error Structure}},
	volume = {39},
	year = {2021}
}

@article{Liu2020,
	abstract = {This paper considers the problem of forecasting a collection of short time series using cross sectional information in panel data. We construct point predictors using Tweedie's formula for the posterior mean of heterogeneous coeffcients under a correlated random effects distribution. This formula utilizes cross-sectional information to transform the unit-speciffc (quasi) maximum likelihood estimator into an approximation of the posterior mean under a prior distribution that equals the population distribution of the random coeffcients. We show that the risk of a predictor based on a non-parametric estimate of the Tweedie correction is asymptotically equivalent to the risk of a predictor that treats the correlated-random-effects distribution as known (ratio-optimality). Our empirical Bayes predictor performs well compared to various competitors in a Monte Carlo study. In an empirical application we use the predictor to forecast revenues for a large panel of bank holding companies and compare forecasts that condition on actual and severely adverse macroeconomic conditions.},
	archivePrefix = {arXiv},
	arxivId = {1709.10193},
	author = {Liu, Laura and Moon, Hyungsik Roger and Schorfheide, Frank},
	doi = {10.2139/ssrn.2908529},
	eprint = {1709.10193},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Dynamic Panels/Liu, Moon, Schorfheide (2017) Forecasting WIth Dynamic Panel Data Models.pdf:pdf},
	journal = {Econometrica},
	mendeley-groups = {Panel Data/Dynamic Panels},
	number = {1},
	pages = {171--201},
	title = {{Forecasting with Dynamic Pane Data Models}},
	volume = {88},
	year = {2020}
}
@article{Miao2020,
	abstract = {In this paper, we consider the least squares estimation of a panel structure threshold regression (PSTR) model where both the slope coefficients and threshold parameters may exhibit latent group structures. We study the asymptotic properties of the estimators of the latent group structure and the slope and threshold coefficients. We show that we can estimate the latent group structure correctly with probability approaching 1 and the estimators of the slope and threshold coefficients are asymptotically equivalent to the infeasible estimators that are obtained as if the true group structures were known. We study likelihood-ratio-based inferences on the group-specific threshold parameters under the shrinking-threshold-effect framework. We also propose two specification tests: one tests whether the threshold parameters are homogeneous across groups, and the other tests whether the threshold effects are present. When the number of latent groups is unknown, we propose a BIC-type information criterion to determine the number of groups in the data. Simulations demonstrate that our estimators and tests perform reasonably well in finite samples. We apply our model to revisit the relationship between capital market imperfection and the investment behavior of firms and to examine the impact of bank deregulation on income inequality. We document a large degree of heterogeneous effects in both applications that cannot be captured by conventional panel threshold regressions.},
	author = {Miao, Ke and Su, Liangjun and Wang, Wendun},
	doi = {10.1016/j.jeconom.2019.07.006},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Panel Data/Heterogeneous Panels/Miao, Su,, Wang, (2019) Panel threshold regressions with latent group structures..pdf:pdf},
	issn = {18726895},
	journal = {Journal of Econometrics},
	keywords = {Classification,Dynamic panel,Latent group structures,Panel structure model,Panel threshold regression},
	mendeley-groups = {Panel Data/Heterogeneous Panels,Threshold Models},
	number = {2},
	pages = {451--481},
	publisher = {Elsevier B.V.},
	title = {{Panel threshold regressions with latent group structures}},
	url = {https://doi.org/10.1016/j.jeconom.2019.07.006},
	volume = {214},
	year = {2020}
}
@article{Steel2020,
	abstract = {The method of model averaging has become an important tool to deal with model uncertainty, for example in situations where a large amount of different theories exist, as are common in economics. Model averaging is a natural and formal response to model uncertainty in a Bayesian framework, and most of the paper deals with Bayesian model averaging. The important role of the prior assumptions in these Bayesian procedures is highlighted. In addition, frequentist model averaging methods are also discussed. Numerical methods to implement these methods are explained, and I point the reader to some freely available computational resources. The main focus is on uncertainty regarding the choice of covariates in normal linear regression models, but the paper also covers other, more challenging, settings, with particular emphasis on sampling models commonly used in economics. Applications of model averaging in economics are reviewed and discussed in a wide range of areas, among which growth economics, production modelling, finance and forecasting macroeconomic quantities.},
	archivePrefix = {arXiv},
	arxivId = {1709.08221},
	author = {Steel, Mark F. J.},
	doi = {10.1257/jel.20191385},
	eprint = {1709.08221},
	file = {:D$\backslash$:/Academic Things/Econometrics/Articles/Model Selection/General/Steel (2020) Model Averaging in Economics.pdf:pdf},
	journal = {Journal of Economic Literature},
	mendeley-groups = {Model Selection and Averaging},
	number = {3},
	pages = {644--719},
	title = {{Model Averaging and its Use in Economics}},
	volume = {58},
	year = {2020}
}
